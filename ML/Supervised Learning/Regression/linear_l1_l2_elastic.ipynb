{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Regression Models â€” Step-by-Step Math\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Linear Regression (OLS)\n",
    "\n",
    "**Loss:**\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n}\\|y - Xw\\|^2\n",
    "$$\n",
    "\n",
    "Expand:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n}(y - Xw)^T(y - Xw)\n",
    "= \\frac{1}{2n}\\big(y^Ty - 2w^TX^Ty + w^TX^TXw\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Gradient wrt \\(w\\):**\n",
    "\n",
    "- Derivative of \\(y^Ty\\) = 0\n",
    "- Derivative of \\(-2w^TX^Ty\\) = \\(-2X^Ty\\)\n",
    "- Derivative of \\(w^TX^TXw\\) = \\(2X^TXw\\)\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = \\frac{1}{2n}(-2X^Ty + 2X^TXw)\n",
    "= -\\frac{1}{n}X^Ty + \\frac{1}{n}X^TXw\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = -\\frac{1}{n}X^T(y - Xw)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Closed-form solution:**\n",
    "\n",
    "Set gradient = 0:\n",
    "\n",
    "$$\n",
    "X^TXw = X^Ty\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^* = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Ridge Regression (L2)\n",
    "\n",
    "**Loss:**\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n}\\|y - Xw\\|^2 + \\frac{\\lambda}{2}\\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "Expand:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n}(y^Ty - 2w^TX^Ty + w^TX^TXw) + \\frac{\\lambda}{2}w^Tw\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Gradient wrt \\(w\\):**\n",
    "\n",
    "- First part: same as OLS â†’ \\(-\\frac{1}{n}X^Ty + \\frac{1}{n}X^TXw\\)\n",
    "- Second part: derivative of \\(\\tfrac{\\lambda}{2}w^Tw\\) = \\(\\lambda w\\)\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = -\\frac{1}{n}X^Ty + \\frac{1}{n}X^TXw + \\lambda w\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{n}X^T(y - Xw) + \\lambda w\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Closed-form solution:**\n",
    "\n",
    "Set gradient = 0:\n",
    "\n",
    "$$\n",
    "(X^TX + n\\lambda I)w = X^Ty\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^* = (X^TX + n\\lambda I)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Lasso Regression (L1)\n",
    "\n",
    "**Loss:**\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n}\\|y - Xw\\|^2 + \\lambda\\|w\\|_1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Gradient (subgradient):**\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = -\\frac{1}{n}X^T(y - Xw) + \\lambda \\,\\text{sign}(w)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\text{sign}(w_j) =\n",
    "\\begin{cases}\n",
    "+1 & w_j > 0 \\\\\n",
    "-1 & w_j < 0 \\\\\n",
    "[-1,1] & w_j = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Coordinate Descent Update\n",
    "\n",
    "- Define:\n",
    "\n",
    "$$\n",
    "a_j = \\frac{1}{n}\\|x_j\\|^2, \\quad b_j = \\frac{1}{n}x_j^T r^{(j)}, \\quad r^{(j)} = y - \\sum_{k\\neq j} x_k w_k\n",
    "$$\n",
    "\n",
    "Then objective for \\(w_j\\) becomes:\n",
    "\n",
    "$$\n",
    "J(w_j) = \\frac{1}{2} a_j w_j^2 - b_j w_j + \\lambda |w_j| + \\text{const}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Case 1: \\(w_j > 0\\)**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = a_j w_j - b_j + \\lambda = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_j = \\frac{b_j - \\lambda}{a_j}\n",
    "$$\n",
    "\n",
    "Valid if \\(b_j > \\lambda\\).\n",
    "\n",
    "---\n",
    "\n",
    "**Case 2: \\(w_j < 0\\)**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = a_j w_j - b_j - \\lambda = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_j = \\frac{b_j + \\lambda}{a_j}\n",
    "$$\n",
    "\n",
    "Valid if \\(b_j < -\\lambda\\).\n",
    "\n",
    "---\n",
    "\n",
    "**Case 3: \\(w_j = 0\\)**\n",
    "\n",
    "At zero, the subgradient is:\n",
    "\n",
    "$$\n",
    "-b_j \\in [-\\lambda, +\\lambda]\n",
    "$$\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$$\n",
    "|b_j| \\leq \\lambda\n",
    "$$\n",
    "\n",
    "So if feature correlation \\(b_j\\) is small, the optimal \\(w_j = 0\\).\n",
    "\n",
    "---\n",
    "\n",
    "**Final Soft-thresholding Update:**\n",
    "\n",
    "$$\n",
    "w_j =\n",
    "\\begin{cases}\n",
    "\\frac{b_j - \\lambda}{a_j}, & b_j > \\lambda \\\\\n",
    "0, & |b_j| \\leq \\lambda \\\\\n",
    "\\frac{b_j + \\lambda}{a_j}, & b_j < -\\lambda\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Compact form:\n",
    "\n",
    "$$\n",
    "w_j = \\frac{1}{a_j}S(b_j,\\lambda)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "S(b_j,\\lambda) = \\text{sign}(b_j)\\max(|b_j| - \\lambda, 0)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Elastic Net (L1 + L2)\n",
    "\n",
    "**Loss:**\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n}\\|y - Xw\\|^2 + \\alpha\\lambda\\|w\\|_1 + \\frac{(1-\\alpha)\\lambda}{2}\\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Gradient (subgradient):**\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = -\\frac{1}{n}X^T(y - Xw) + \\alpha\\lambda\\,\\text{sign}(w) + (1-\\alpha)\\lambda w\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Coordinate Descent Update\n",
    "\n",
    "- Define:\n",
    "\n",
    "$$\n",
    "a_j = \\frac{1}{n}\\|x_j\\|^2 + (1-\\alpha)\\lambda, \\quad b_j = \\frac{1}{n}x_j^T r^{(j)}\n",
    "$$\n",
    "\n",
    "Update:\n",
    "\n",
    "$$\n",
    "w_j = \\frac{1}{a_j}S(b_j,\\alpha\\lambda)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Summary\n",
    "\n",
    "- **Linear (OLS):**\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = -\\frac{1}{n}X^T(y - Xw), \\quad\n",
    "w^* = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "- **Ridge (L2):**\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = -\\frac{1}{n}X^T(y - Xw) + \\lambda w, \\quad\n",
    "w^* = (X^TX + n\\lambda I)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "- **Lasso (L1):**\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = -\\frac{1}{n}X^T(y - Xw) + \\lambda\\,\\text{sign}(w), \\quad\n",
    "w_j = \\tfrac{1}{a_j}S(b_j,\\lambda)\n",
    "$$\n",
    "\n",
    "- **Elastic Net:**\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = -\\frac{1}{n}X^T(y - Xw) + \\alpha\\lambda\\,\\text{sign}(w) + (1-\\alpha)\\lambda w, \\quad\n",
    "w_j = \\tfrac{1}{a_j}S(b_j,\\alpha\\lambda)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ž Note on Proximal Gradient\n",
    "\n",
    "There is also a **proximal gradient descent method** for Lasso, where the update is:\n",
    "\n",
    "$$\n",
    "w^{t+1} = S\\Big(w^t - \\eta \\nabla f(w^t), \\eta\\lambda\\Big)\n",
    "$$\n",
    "\n",
    "But coordinate descent is the standard approach.\n",
    "\n"
   ],
   "id": "10a9630774e87492"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ“˜ Regression Beyond Linear Models: SVR and GBDT\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Support Vector Regression (SVR)\n",
    "\n",
    "### Idea\n",
    "- Instead of separating classes, SVR finds a flat function:\n",
    "  $$\n",
    "  f(x) = w^Tx + b\n",
    "  $$\n",
    "- Goal: predictions should lie within an **Îµ-insensitive tube** around $y$.\n",
    "- Only residuals larger than $\\varepsilon$ matter â†’ updates happen *only when errors are big*.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective (Îµ-SVR)\n",
    "\n",
    "$$\n",
    "\\min_{w,b,\\xi,\\xi^*} \\; \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^n (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i - (w^Tx_i + b) \\leq \\varepsilon + \\xi_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "(w^Tx_i + b) - y_i \\leq \\varepsilon + \\xi_i^*\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\xi_i, \\xi_i^* \\geq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Dual solution (after optimization)\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) K(x_i, x) + b\n",
    "$$\n",
    "\n",
    "- $\\alpha_i, \\alpha_i^*$ are dual coefficients.\n",
    "- **Update rule:** coefficients are adjusted only when $|y_i - f(x_i)| > \\varepsilon$.\n",
    "- Points inside the $\\varepsilon$-tube have no effect (their coefficients = 0).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gradient Boosted Decision Trees (GBDT for Regression)\n",
    "\n",
    "### Idea\n",
    "- Build trees *sequentially* to correct residuals of the current model.\n",
    "- Equivalent to gradient descent in **function space**.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "Given a loss $L(y, \\hat{y})$, e.g. squared error:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Start with:\n",
    "\n",
    "$$\n",
    "F_0(x) = \\arg\\min_c \\sum_i L(y_i, c) = \\text{mean}(y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iterative Updates\n",
    "\n",
    "At step $m$:\n",
    "\n",
    "1. **Compute pseudo-residuals (negative gradient):**\n",
    "\n",
    "$$\n",
    "r_{im} = - \\left[\\frac{\\partial L(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}\\right]_{\\hat{y}_i = F_{m-1}(x_i)}\n",
    "$$\n",
    "\n",
    "- For squared error:\n",
    "  $r_{im} = y_i - F_{m-1}(x_i)$\n",
    "\n",
    "---\n",
    "\n",
    "2. **Fit a regression tree** $h_m(x)$ to residuals $\\{r_{im}\\}$.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Compute optimal step size:**\n",
    "\n",
    "$$\n",
    "\\gamma_m = \\arg\\min_\\gamma \\sum_i L\\big(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\big)\n",
    "$$\n",
    "\n",
    "- For squared error: $\\gamma_m = 1$\n",
    "\n",
    "---\n",
    "\n",
    "4. **Update model:**\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) + \\eta \\gamma_m h_m(x)\n",
    "$$\n",
    "\n",
    "where $\\eta$ = learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### Updates in words\n",
    "\n",
    "- Each tree corrects what the last model got wrong.\n",
    "- Large residuals â†’ stronger influence on new splits.\n",
    "- Parameters are updated additively:\n",
    "  $$\n",
    "  F(x) = \\sum_{m=0}^M \\eta \\gamma_m h_m(x)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Summary\n",
    "\n",
    "- **SVR:**\n",
    "  - Updates only when residuals exceed $\\varepsilon$.\n",
    "  - Parameters = dual coefficients $(\\alpha_i, \\alpha_i^*)$.\n",
    "  - Prediction uses only *support vectors*.\n",
    "\n",
    "- **GBDT:**\n",
    "  - Updates via gradient descent in function space.\n",
    "  - Each new tree fits residuals = negative gradient of loss.\n",
    "  - Final model is the sum of trees with shrinkage.\n"
   ],
   "id": "e6333d8a65953eb1"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
