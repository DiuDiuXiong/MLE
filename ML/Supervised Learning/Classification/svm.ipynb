{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Linear SVM — Notes\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Setup\n",
    "\n",
    "- Data:\n",
    "  $$X \\in \\mathbb{R}^{m \\times d},\\quad x_i^\\top \\in \\mathbb{R}^d$$\n",
    "  → shape: \\(m\\) samples × \\(d\\) features.\n",
    "\n",
    "- Labels:\n",
    "  $$y \\in \\{-1,+1\\}^m$$\n",
    "  → vector of length \\(m\\).\n",
    "\n",
    "- Parameters:\n",
    "  $$w \\in \\mathbb{R}^d,\\quad b \\in \\mathbb{R}$$\n",
    "  → weights and bias.\n",
    "\n",
    "- Model:\n",
    "  $$\n",
    "  f(x) = w^\\top x + b\n",
    "  $$\n",
    "\n",
    "- Decision boundary (hyperplane):\n",
    "  $$\n",
    "  \\mathcal{H} = \\{z \\in \\mathbb{R}^d : w^\\top z + b = 0\\}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Idea: max-margin separator\n",
    "\n",
    "Correct classification condition:\n",
    "\n",
    "$$\n",
    "y_i(w^\\top x_i+b) > 0.\n",
    "$$\n",
    "\n",
    "Geometric margin:\n",
    "\n",
    "$$\n",
    "\\gamma(w,b) = \\min_i \\frac{y_i\\,(w^\\top x_i+b)}{\\|w\\|_2}.\n",
    "$$\n",
    "\n",
    "Canonical scaling (closest points have functional margin 1):\n",
    "\n",
    "$$\n",
    "y_i(w^\\top x_i+b)=1 \\quad\\Rightarrow\\quad \\gamma=\\frac{1}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "**Hard-margin SVM (separable):**\n",
    "\n",
    "$$\n",
    "\\min_{w,b}\\ \\tfrac12\\|w\\|_2^2\n",
    "\\quad\\text{s.t.}\\quad y_i(w^\\top x_i+b)\\ge 1,\\ \\forall i.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Distance function\n",
    "\n",
    "Perpendicular Euclidean distance:\n",
    "\n",
    "$$\n",
    "\\operatorname{dist}(x,\\mathcal H)=\\frac{|w^\\top x+b|}{\\|w\\|_2}.\n",
    "$$\n",
    "\n",
    "Signed distance for a labeled point:\n",
    "\n",
    "$$\n",
    "\\frac{y_i(w^\\top x_i+b)}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "Margin band width:\n",
    "\n",
    "$$\n",
    "\\text{width} = \\frac{2}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Soft-margin with slacks\n",
    "\n",
    "Introduce \\(\\xi_i\\ge 0\\):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,b,\\ \\xi\\ge 0}\\quad & \\tfrac12\\|w\\|_2^2 + C\\sum_{i=1}^m \\xi_i \\\\\n",
    "\\text{s.t.}\\quad & y_i\\,(w^\\top x_i+b)\\ \\ge\\ 1-\\xi_i,\\quad i=1,\\dots,m.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "At optimum:\n",
    "\n",
    "$$\n",
    "\\xi_i^\\*=\\max(0,\\ 1-y_i(w^\\top x_i+b)).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Hinge-loss ERM\n",
    "\n",
    "Scores and margins:\n",
    "\n",
    "$$\n",
    "s=Xw+b\\mathbf{1},\\quad r=1-y\\odot s.\n",
    "$$\n",
    "\n",
    "**L1-hinge:**\n",
    "\n",
    "$$\n",
    "J(w,b)=\\frac{\\lambda}{2}\\|w\\|_2^2+\\frac{1}{m}\\sum_{i=1}^m \\max(0,\\ 1-y_i(w^\\top x_i+b)).\n",
    "$$\n",
    "\n",
    "**L2-hinge:**\n",
    "\n",
    "$$\n",
    "J_{\\text{sq}}(w,b)=\\frac{\\lambda}{2}\\|w\\|_2^2+\\frac{1}{m}\\sum_{i=1}^m \\big[\\max(0,\\ 1-y_i(w^\\top x_i+b))\\big]^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Gradients / subgradients\n",
    "\n",
    "Let \\(M=\\mathbf{1}_{(r>0)}\\), \\(r_+=\\max(r,0)\\).\n",
    "\n",
    "**L1-hinge (full batch):**\n",
    "\n",
    "$$\n",
    "\\nabla_w J = \\lambda w - \\tfrac{1}{m}X^\\top(y\\odot M), \\qquad\n",
    "\\nabla_b J = -\\tfrac{1}{m}\\mathbf{1}^\\top(y\\odot M).\n",
    "$$\n",
    "\n",
    "**L1-hinge (mini-batch of size \\(B\\)):**\n",
    "\n",
    "$$\n",
    "\\nabla_w J_B = \\lambda w - \\tfrac{1}{B}X_B^\\top(y_B\\odot M_B), \\qquad\n",
    "\\nabla_b J_B = -\\tfrac{1}{B}\\mathbf{1}^\\top(y_B\\odot M_B).\n",
    "$$\n",
    "\n",
    "**L2-hinge (full batch):**\n",
    "\n",
    "$$\n",
    "\\nabla_w J_{\\text{sq}} = \\lambda w - \\tfrac{2}{m}X^\\top(y\\odot r_+), \\qquad\n",
    "\\nabla_b J_{\\text{sq}} = -\\tfrac{2}{m}\\mathbf{1}^\\top(y\\odot r_+).\n",
    "$$\n",
    "\n",
    "**L2-hinge (mini-batch of size \\(B\\)):**\n",
    "\n",
    "$$\n",
    "\\nabla_w J_{B,\\text{sq}} = \\lambda w - \\tfrac{2}{B}X_B^\\top(y_B\\odot \\max(r_B,0)), \\qquad\n",
    "\\nabla_b J_{B,\\text{sq}} = -\\tfrac{2}{B}\\mathbf{1}^\\top(y_B\\odot \\max(r_B,0)).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Update rule\n",
    "\n",
    "With step size \\(\\eta_t\\):\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta_t\\,\\nabla_w,\\qquad\n",
    "b \\leftarrow b - \\eta_t\\,\\nabla_b.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Norm reminder\n",
    "\n",
    "For \\(w=(w_1,\\dots,w_d)\\):\n",
    "\n",
    "$$\n",
    "\\|w\\|_2=\\sqrt{\\sum_{j=1}^d w_j^2},\\qquad \\|w\\|_2^2=\\sum_{j=1}^d w_j^2.\n",
    "$$\n",
    "\n",
    "Smaller \\(\\|w\\|\\) ⇒ larger margin \\(1/\\|w\\|\\) under canonical scaling.\n"
   ],
   "id": "6dea6afccfe87d0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T14:19:57.649889Z",
     "start_time": "2025-09-27T14:19:57.647685Z"
    }
   },
   "cell_type": "code",
   "source": "# for how kernel method work, see: https://0809zheng.github.io/2021/07/23/kernel.html",
   "id": "b7c3d7b395077b48",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Soft-margin SVM → kernelized gradients (step-by-step)\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Setup & notation\n",
    "\n",
    "We have data \\(x_i \\in \\mathbb{R}^d\\), labels \\(y_i \\in \\{-1,+1\\}\\), \\(i=1,\\dots,m\\).\n",
    "\n",
    "For any weight \\(w\\) and bias \\(b\\), define scores:\n",
    "\n",
    "$$f_i = w^\\top x_i + b.$$\n",
    "\n",
    "The regularized empirical risk:\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{\\lambda}{2}\\,\\|w\\|^2 \\;+\\; \\frac{1}{m}\\sum_{i=1}^m \\ell(y_i, f_i).\n",
    "$$\n",
    "\n",
    "Residuals, indicators, and positive part:\n",
    "\n",
    "$$\n",
    "r_i \\equiv 1 - y_i f_i,\\quad\n",
    "M_i \\equiv \\mathbf{1}_{(r_i>0)},\\quad\n",
    "r_i^+ \\equiv \\max(r_i,0).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Non-kernel soft-margin gradients (w, b)\n",
    "\n",
    "### (a) L1-hinge loss\n",
    "\n",
    "$$\\ell(y,f) = \\max(0,\\,1-yf)$$\n",
    "\n",
    "Gradients:\n",
    "\n",
    "$$\n",
    "\\nabla_w J = \\lambda\\,w \\;-\\; \\frac{1}{m}\\sum_{i=1}^m M_i y_i x_i,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b J = -\\frac{1}{m}\\sum_{i=1}^m M_i y_i.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### (b) L2-hinge loss\n",
    "\n",
    "$$\\ell(y,f) = \\big(\\max(0,\\,1-yf)\\big)^2$$\n",
    "\n",
    "Gradients:\n",
    "\n",
    "$$\n",
    "\\nabla_w J_{\\text{sq}} = \\lambda\\,w \\;-\\; \\frac{2}{m}\\sum_{i=1}^m r_i^+ y_i x_i,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b J_{\\text{sq}} = -\\frac{2}{m}\\sum_{i=1}^m r_i^+ y_i.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Why \\(w\\) can be written as a sum of training points\n",
    "\n",
    "By the **representer theorem** (or via dual/KKT derivation):\n",
    "\n",
    "$$\n",
    "w = \\sum_{j=1}^m \\beta_j\\,y_j\\,\\phi(x_j),\n",
    "$$\n",
    "\n",
    "where \\(\\phi(x)\\) is the feature map (for linear case, \\(\\phi(x)=x\\)).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Substitute into gradients → kernel form\n",
    "\n",
    "### 3.1 Scores in kernel form\n",
    "\n",
    "$$\n",
    "f_i = \\sum_{j=1}^m \\beta_j y_j K(x_j,x_i) + b,\n",
    "$$\n",
    "\n",
    "where \\(K(x,z) = \\langle \\phi(x), \\phi(z)\\rangle\\).\n",
    "\n",
    "Vector form:\n",
    "\n",
    "$$\n",
    "s = K(\\beta \\odot y) + b\\,\\mathbf{1}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 From \\(\\nabla_w J\\) to \\(\\nabla_\\beta J\\)\n",
    "\n",
    "By chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_k}\n",
    "= y_k \\,\\langle \\nabla_w J, \\phi(x_k) \\rangle.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### L1-hinge\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta J = \\lambda\\,YKY\\,\\beta \\;-\\; \\frac{1}{m}\\,Y K (y\\odot M),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b J = -\\frac{1}{m}\\,\\mathbf{1}^\\top (y\\odot M).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### L2-hinge\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta J_{\\text{sq}} = \\lambda\\,YKY\\,\\beta \\;-\\; \\frac{2}{m}\\,Y K (y\\odot r_+),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b J_{\\text{sq}} = -\\frac{2}{m}\\,\\mathbf{1}^\\top (y\\odot r_+).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Shapes & identity\n",
    "\n",
    "- \\(K \\in \\mathbb{R}^{m\\times m}, \\;\\beta,y,M,r_+ \\in \\mathbb{R}^m,\\; Y = \\mathrm{diag}(y)\\).\n",
    "\n",
    "Useful identity:\n",
    "\n",
    "$$\n",
    "YK(Y\\beta) = y \\odot \\big(K(\\beta \\odot y)\\big).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Mini-batch (size \\(B\\))\n",
    "\n",
    "For batch \\(\\mathcal{B}\\):\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta J_B = \\lambda\\,YKY\\,\\beta \\;-\\; \\frac{1}{B}\\,Y K_B (y_B \\odot M_B),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b J_B = -\\frac{1}{B}\\,\\mathbf{1}^\\top (y_B \\odot M_B).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta J_{B,\\text{sq}} = \\lambda\\,YKY\\,\\beta \\;-\\; \\frac{2}{B}\\,Y K_B (y_B \\odot r_{B,+}),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b J_{B,\\text{sq}} = -\\frac{2}{B}\\,\\mathbf{1}^\\top (y_B \\odot r_{B,+}).\n",
    "$$\n"
   ],
   "id": "693e2dfab138a8cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T15:55:19.207756Z",
     "start_time": "2025-09-27T15:55:19.204694Z"
    }
   },
   "cell_type": "code",
   "source": "# https://www.deep-ml.com/problems/21",
   "id": "9880c357f17bbe3c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Understanding the Pegasos Kernel SVM implementation\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Kernel functions\n",
    "\n",
    "- **Linear kernel**: computes the dot product\n",
    "  $$K(x,y) = x^\\top y.$$\n",
    "\n",
    "- **RBF kernel**: computes similarity\n",
    "  $$K(x,y) = \\exp\\left(-\\frac{\\|x-y\\|^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "These kernels allow the SVM to work in either the original feature space (linear) or in an implicit infinite-dimensional space (RBF).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Initialization\n",
    "\n",
    "- The algorithm starts with:\n",
    "  - `alphas` = vector of coefficients, one per training point, all zeros.\n",
    "  - `b` = bias term, set to zero.\n",
    "\n",
    "- Input parameters:\n",
    "  - `lambda_val`: regularization parameter.\n",
    "  - `iterations`: number of training passes.\n",
    "  - `sigma`: width parameter for the RBF kernel.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Training loop\n",
    "\n",
    "- The outer loop runs over iterations \\(t = 1, \\dots, T\\).\n",
    "- The inner loop runs through every training point \\(i\\).\n",
    "- At each step, the learning rate is:\n",
    "  $$\\eta_t = \\frac{1}{\\lambda t}.$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Decision function\n",
    "\n",
    "For each training point \\(x_i\\), compute the margin score:\n",
    "\n",
    "$$\n",
    "f(x_i) = \\sum_{j=1}^n \\alpha_j y_j K(x_j, x_i) + b\n",
    "$$\n",
    "\n",
    "This is the standard kernel SVM decision function, using all current coefficients \\(\\alpha_j\\).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Update rule\n",
    "\n",
    "If the margin constraint is violated:\n",
    "\n",
    "$$\n",
    "y_i f(x_i) < 1,\n",
    "$$\n",
    "\n",
    "then update:\n",
    "\n",
    "- The coefficient for the current sample:\n",
    "  $$\n",
    "  \\alpha_i \\;\\leftarrow\\; \\alpha_i + \\eta \\big(y_i - \\lambda \\alpha_i\\big)\n",
    "  $$\n",
    "- The bias term:\n",
    "  $$\n",
    "  b \\;\\leftarrow\\; b + \\eta y_i\n",
    "  $$\n",
    "\n",
    "Otherwise, no update is made.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Return\n",
    "\n",
    "At the end of training, the function returns:\n",
    "\n",
    "- The coefficients \\(\\alpha\\) (rounded for readability).\n",
    "- The bias \\(b\\) (also rounded).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Summary\n",
    "\n",
    "This function implements a **simplified kernelized Pegasos SVM**:\n",
    "\n",
    "- Maintains per-sample coefficients \\(\\alpha_i\\).\n",
    "- Uses either a linear or RBF kernel for similarity.\n",
    "- Iteratively updates coefficients and bias when the margin is violated.\n",
    "- Returns the final decision function:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n",
    "$$\n",
    "\n",
    "**Notes:**\n",
    "- The original Pegasos algorithm is stochastic (randomly samples points).\n",
    "- Here, the version is deterministic: it loops through all points sequentially.\n",
    "- It does not include projection steps, so it is best seen as an educational simplification.\n"
   ],
   "id": "48d52a016d1f8a60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "40f8c8a386809b1b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
