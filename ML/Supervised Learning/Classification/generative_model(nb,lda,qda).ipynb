{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìö Summary: Generative Classifiers (Naive Bayes + Discriminant Analysis)\n",
    "\n",
    "All these models are *generative*: they model the class-conditional distributions\n",
    "$P(x \\mid y=c)$, then apply Bayes‚Äô rule with priors $P(y=c)$ to predict:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_c \\; P(y=c) \\, P(x \\mid y=c)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üî∏ Naive Bayes family\n",
    "\n",
    "**Shared idea:** Assume features are conditionally independent given the class.\n",
    "So joint likelihood factorizes as:\n",
    "\n",
    "$$\n",
    "P(x \\mid y=c) = \\prod_j P(x_j \\mid y=c)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1. GaussianNB\n",
    "- **Assumption:** Each feature is Gaussian:\n",
    "  $$\n",
    "  x_j \\mid y=c \\sim \\mathcal{N}(\\mu_{jc}, \\sigma_{jc}^2)\n",
    "  $$\n",
    "- **Parameters:** mean $\\mu_{jc}$ and variance $\\sigma_{jc}^2$ for each feature $j$ in class $c$.\n",
    "- **Best for:** continuous, real-valued features (e.g. sensor data, normally distributed features).\n",
    "- **Limitation:** ignores correlations between features (covariance assumed diagonal).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. MultinomialNB\n",
    "- **Assumption:** Features are counts/frequencies (e.g. word counts).\n",
    "- **Likelihood:**\n",
    "  $$\n",
    "  P(x \\mid y=c) \\propto \\prod_j \\theta_{jc}^{\\,x_j}\n",
    "  $$\n",
    "- **Parameter estimation:**\n",
    "  $$\n",
    "  \\theta_{jc} = \\frac{N_{jc} + \\alpha}{\\sum_k (N_{kc} + \\alpha)}\n",
    "  $$\n",
    "  where $N_{jc}$ is the total count of feature $j$ in class $c$, and $\\alpha$ is smoothing.\n",
    "- **Best for:** text classification (bag-of-words, TF-IDF).\n",
    "- **Limitation:** not suitable for continuous values; treats fractional counts oddly.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. BernoulliNB\n",
    "- **Assumption:** Features are binary (present/absent).\n",
    "- **Likelihood:**\n",
    "  $$\n",
    "  P(x \\mid y=c) = \\prod_j \\theta_{jc}^{\\,x_j} (1-\\theta_{jc})^{1-x_j}\n",
    "  $$\n",
    "- **Best for:** binary indicators (e.g. \"does this email contain word X?\").\n",
    "- **Limitation:** discards frequency information.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. CategoricalNB\n",
    "- **Assumption:** Features are categorical, each taking values from a finite set.\n",
    "- **Likelihood:**\n",
    "  $$\n",
    "  P(x \\mid y=c) = \\prod_j P(x_j = v \\mid y=c)\n",
    "  $$\n",
    "- **Parameter estimation:**\n",
    "  $$\n",
    "  \\theta_{j,v,c} = \\frac{N_{jvc} + \\alpha}{N_{jc} + \\alpha K_j}\n",
    "  $$\n",
    "  where $N_{jvc}$ = number of samples in class $c$ with feature $j=v$,\n",
    "  $N_{jc}$ = total samples in class $c$, $K_j$ = number of categories for feature $j$.\n",
    "- **Best for:** categorical/tabular data (e.g. color ‚àà {red, green, blue}).\n",
    "- ‚ö†Ô∏è If used on raw continuous floats, each unique value is treated as its own category ‚Üí memorization.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∏ Discriminant Analysis family\n",
    "\n",
    "**Shared idea:** Assume class-conditional distributions are *multivariate Gaussian*:\n",
    "\n",
    "$$\n",
    "P(x \\mid y=c) = \\mathcal{N}(x; \\mu_c, \\Sigma_c)\n",
    "$$\n",
    "\n",
    "Differences come from assumptions about the covariance $\\Sigma_c$.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Linear Discriminant Analysis (LDA)\n",
    "- **Assumption:** Each class is Gaussian with the **same covariance** $\\Sigma$:\n",
    "  $$\n",
    "  P(x \\mid y=c) \\sim \\mathcal{N}(\\mu_c, \\Sigma)\n",
    "  $$\n",
    "- **Discriminant function:**\n",
    "  $$\n",
    "  \\delta_c(x) = x^\\top \\Sigma^{-1}\\mu_c \\;-\\; \\tfrac{1}{2}\\mu_c^\\top \\Sigma^{-1}\\mu_c \\;+\\; \\log P(y=c)\n",
    "  $$\n",
    "  Prediction = class with largest $\\delta_c(x)$.\n",
    "  Boundaries are **linear** hyperplanes.\n",
    "\n",
    "- **Fisher‚Äôs dimension reduction view:**\n",
    "  Define *within-class scatter*:\n",
    "  $$\n",
    "  S_W = \\sum_{c=1}^K \\sum_{i: y_i=c} (x_i - \\mu_c)(x_i - \\mu_c)^\\top\n",
    "  $$\n",
    "  Define *between-class scatter*:\n",
    "  $$\n",
    "  S_B = \\sum_{c=1}^K n_c (\\mu_c - \\mu)(\\mu_c - \\mu)^\\top\n",
    "  $$\n",
    "  Optimize:\n",
    "  $$\n",
    "  J(w) = \\frac{w^\\top S_B w}{w^\\top S_W w}\n",
    "  $$\n",
    "  This leads to the generalized eigenproblem:\n",
    "  $$\n",
    "  S_B w = \\lambda S_W w\n",
    "  $$\n",
    "  Keep the top $K-1$ eigenvectors (since $\\mathrm{rank}(S_B) \\le K-1$).\n",
    "  Project data to at most $K-1$ dimensions where classes are maximally separated.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Quadratic Discriminant Analysis (QDA)\n",
    "- **Assumption:** Each class is Gaussian with its **own covariance** $\\Sigma_c$:\n",
    "  $$\n",
    "  P(x \\mid y=c) \\sim \\mathcal{N}(\\mu_c, \\Sigma_c)\n",
    "  $$\n",
    "- **Discriminant function:**\n",
    "  $$\n",
    "  \\delta_c(x) = -\\tfrac{1}{2}\\log|\\Sigma_c| - \\tfrac{1}{2}(x-\\mu_c)^\\top \\Sigma_c^{-1}(x-\\mu_c) + \\log P(y=c)\n",
    "  $$\n",
    "  Boundaries are **quadratic curves/surfaces**.\n",
    "- **More flexible** than LDA, but needs more data to estimate each $\\Sigma_c$ reliably.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Big Picture\n",
    "\n",
    "- **Naive Bayes:** assumes conditional independence.\n",
    "  - GaussianNB ‚Üí continuous, independent features.\n",
    "  - MultinomialNB ‚Üí count/frequency features.\n",
    "  - BernoulliNB ‚Üí binary features.\n",
    "  - CategoricalNB ‚Üí categorical features.\n",
    "\n",
    "- **Discriminant Analysis:** assumes multivariate Gaussian.\n",
    "  - LDA ‚Üí shared covariance ‚Üí linear boundaries, dimension reduction.\n",
    "  - QDA ‚Üí separate covariance ‚Üí quadratic boundaries, more flexible.\n",
    "\n",
    "All follow the same recipe:\n",
    "1. Estimate $P(x \\mid y)$ under some assumption.\n",
    "2. Multiply by prior $P(y)$.\n",
    "3. Pick the class with max posterior.\n"
   ],
   "id": "e679582bb49964f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d591abc36070a13a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
