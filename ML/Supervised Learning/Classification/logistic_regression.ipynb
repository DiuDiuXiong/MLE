{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "s# Logistic Regression — Full Notes\n",
    "\n",
    "## 1. What it is\n",
    "- **Linear classifier** that models conditional probability directly:\n",
    "\n",
    "  $$\n",
    "  P(y=1 \\mid x) = \\sigma(w^\\top x + b)\n",
    "  $$\n",
    "\n",
    "- **Discriminative model** (models $p(y|x)$ directly), unlike Naïve Bayes or LDA (generative).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Sigmoid function\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "- Maps $(-\\infty, \\infty) \\to (0,1)$.\n",
    "- Nice derivative property:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma(z)}{dz} = \\sigma(z)\\big(1-\\sigma(z)\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why not Linear Regression?\n",
    "- Linear regression: $\\hat{y} = w^\\top x + b$. Threshold at 0.5 → linear boundary.\n",
    "- Problems:\n",
    "  - Predictions not constrained to $[0,1]$.\n",
    "  - Squared error loss does **not** match Bernoulli distribution assumption.\n",
    "- Logistic regression:\n",
    "  - Proper **probabilities**.\n",
    "  - Uses **log loss**, aligned with classification.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Loss function derivation\n",
    "\n",
    "For one sample $(x,y)$, model probability is:\n",
    "\n",
    "$$\n",
    "P(y|x) = \\sigma(z)^y \\big(1-\\sigma(z)\\big)^{1-y}, \\quad z = w^\\top x + b\n",
    "$$\n",
    "\n",
    "### Likelihood for dataset\n",
    "$$\n",
    "L(w,b) = \\prod_{i=1}^n \\sigma(z_i)^{y_i} (1-\\sigma(z_i))^{1-y_i}\n",
    "$$\n",
    "\n",
    "### Log-likelihood\n",
    "$$\n",
    "\\ell(w,b) = \\sum_{i=1}^n \\Big[ y_i \\log \\sigma(z_i) + (1-y_i)\\log (1-\\sigma(z_i)) \\Big]\n",
    "$$\n",
    "\n",
    "### Negative log-likelihood (loss)\n",
    "$$\n",
    "J(w,b) = -\\ell(w,b)\n",
    "$$\n",
    "\n",
    "If averaged:\n",
    "$$\n",
    "J(w,b) = -\\frac{1}{n}\\sum_{i=1}^n \\Big[ y_i \\log \\sigma(z_i) + (1-y_i)\\log (1-\\sigma(z_i)) \\Big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Gradient derivation (no skipped steps)\n",
    "\n",
    "### For one sample\n",
    "$$\n",
    "J = - \\Big[ y \\log \\sigma(z) + (1-y)\\log(1-\\sigma(z)) \\Big]\n",
    "$$\n",
    "\n",
    "Differentiate wrt $w$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w}\n",
    "= - \\Bigg[ y \\cdot \\frac{1}{\\sigma(z)} \\cdot \\frac{\\partial \\sigma(z)}{\\partial w}\n",
    "+ (1-y) \\cdot \\frac{1}{1-\\sigma(z)} \\cdot \\frac{\\partial (1-\\sigma(z))}{\\partial w} \\Bigg]\n",
    "$$\n",
    "\n",
    "Now, compute derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma(z)}{\\partial w} = \\sigma(z)(1-\\sigma(z))x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (1-\\sigma(z))}{\\partial w} = -\\sigma(z)(1-\\sigma(z))x\n",
    "$$\n",
    "\n",
    "Plug back:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w}\n",
    "= - \\Big[ y \\cdot (1-\\sigma(z))x - (1-y)\\sigma(z)x \\Big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (\\sigma(z) - y)x\n",
    "$$\n",
    "\n",
    "Similarly for bias:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\sigma(z) - y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Vectorized (all samples)\n",
    "Let:\n",
    "- $X \\in \\mathbb{R}^{n \\times d}$ (design matrix),\n",
    "- $p = \\sigma(Xw + b) \\in \\mathbb{R}^n$,\n",
    "- $y \\in \\mathbb{R}^n$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\nabla_w J = X^\\top (p-y), \\qquad\n",
    "\\frac{\\partial J}{\\partial b} = \\mathbf{1}^\\top (p-y)\n",
    "$$\n",
    "\n",
    "If using mean loss, divide by $n$.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Log-odds interpretation\n",
    "The model assumes:\n",
    "\n",
    "$$\n",
    "\\log \\frac{P(y=1|x)}{P(y=0|x)} = w^\\top x + b\n",
    "$$\n",
    "\n",
    "So logistic regression is a **linear model on the log-odds**.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Regularization\n",
    "\n",
    "### L2 penalty\n",
    "$$\n",
    "J = -\\ell(w,b) + \\frac{\\lambda}{2}\\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "$$\n",
    "\\nabla_w J = X^\\top (p-y) + \\lambda w\n",
    "$$\n",
    "\n",
    "Effect: shrinks weights, keeps all features.\n",
    "\n",
    "---\n",
    "\n",
    "### L1 penalty\n",
    "$$\n",
    "J = -\\ell(w,b) + \\lambda \\|w\\|_1\n",
    "$$\n",
    "\n",
    "Gradient/subgradient:\n",
    "$$\n",
    "\\nabla_w J = X^\\top (p-y) + \\lambda \\,\\text{sign}(w)\n",
    "$$\n",
    "\n",
    "Effect: drives some weights exactly to 0 → **feature selection**.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Closed-form solution?\n",
    "- **No closed form** for logistic regression.\n",
    "- Need iterative optimization (gradient descent, Newton’s method, LBFGS, SAGA, etc).\n",
    "- With L2, Newton’s method = Iteratively Reweighted Least Squares (IRLS).\n",
    "- With L1, need subgradient or coordinate descent.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Key takeaways\n",
    "- Same *type* of linear decision boundary as linear regression, but with:\n",
    "  - Correct **probabilistic interpretation**,\n",
    "  - Proper **log-loss objective**,\n",
    "  - Better robustness and generalization.\n",
    "- Gradient = **(prediction – truth) × input** — simple, elegant.\n",
    "- Regularization changes gradient directly and affects sparsity/smoothness.\n"
   ],
   "id": "5ba43bf01f894f7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T06:44:25.809110Z",
     "start_time": "2025-09-26T06:44:25.806158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ML craft: https://www.deep-ml.com/problems/104, https://www.deep-ml.com/problems/106\n",
    "import numpy as np\n",
    "\n",
    "def predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> np.ndarray:\n",
    "\t\"\"\"\n",
    "\tImplements binary classification prediction using Logistic Regression.\n",
    "\n",
    "\tArgs:\n",
    "\t\tX: Input feature matrix (shape: N x D)\n",
    "\t\tweights: Model weights (shape: D)\n",
    "\t\tbias: Model bias\n",
    "\n",
    "\tReturns:\n",
    "\t\tBinary predictions (0 or 1)\n",
    "\t\"\"\"\n",
    "\tz = X @ weights + bias\n",
    "\treturn (z >= 0).astype(int)\n",
    "\n",
    "assert (predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == np.array([1, 1, 0, 0])).all()"
   ],
   "id": "8b5df2f1b23dc858",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T08:17:35.436813Z",
     "start_time": "2025-09-26T08:17:35.430541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n",
    "    \"\"\"\n",
    "    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n",
    "    \"\"\"\n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    m, n = X.shape\n",
    "    X = np.hstack((np.ones((m, 1)), X))\n",
    "    w = np.zeros((n+1, 1))\n",
    "    y = y.reshape(-1, 1)\n",
    "    losses = []\n",
    "    for i in range(iterations):\n",
    "        y_pred = sigmoid(X @ w)\n",
    "        log_loss = -(y*np.log(y_pred)+(1-y)*np.log(1-y_pred)).sum()\n",
    "        losses.append(np.round(log_loss, decimals=4))\n",
    "        gradient = X.T @ (y_pred - y)\n",
    "        w = w - learning_rate * gradient\n",
    "    return w, losses\n",
    "\n",
    "train_logreg(np.array([[1.0, 0.5], [-0.5, -1.5], [2.0, 1.5], [-2.0, -1.0]]), np.array([1, 0, 1, 0]), 0.01, 20)"
   ],
   "id": "6fa54e68d4aa64b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-3.10665949e-04],\n",
       "        [ 4.03831112e-01],\n",
       "        [ 3.37881902e-01]]),\n",
       " [2.7726,\n",
       "  2.6485,\n",
       "  2.533,\n",
       "  2.4254,\n",
       "  2.325,\n",
       "  2.2314,\n",
       "  2.1441,\n",
       "  2.0625,\n",
       "  1.9862,\n",
       "  1.9148,\n",
       "  1.8479,\n",
       "  1.7852,\n",
       "  1.7263,\n",
       "  1.6708,\n",
       "  1.6187,\n",
       "  1.5696,\n",
       "  1.5232,\n",
       "  1.4794,\n",
       "  1.438,\n",
       "  1.3988])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Multinomial (Softmax) Logistic Regression — Full Notes\n",
    "\n",
    "## 1) Setup & Notation\n",
    "- Classes: $K \\ge 2$.\n",
    "- Features: $x \\in \\mathbb{R}^d$.\n",
    "- Parameters: weight matrix $W \\in \\mathbb{R}^{d \\times K}$, bias $b \\in \\mathbb{R}^K$.\n",
    "- Logits:\n",
    "$$\n",
    "z = W^\\top x + b \\in \\mathbb{R}^K, \\quad z_k = w_k^\\top x + b_k\n",
    "$$\n",
    "(where $w_k$ is the $k$-th column of $W$).\n",
    "\n",
    "## 2) Softmax & Class Probabilities\n",
    "$$\n",
    "\\text{softmax}(z)_k = p_k = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- $p_k \\in (0,1)$ and $\\sum_k p_k = 1$.\n",
    "- Numerical stability uses: $\\text{softmax}(z)_k = \\frac{e^{z_k - \\max_j z_j}}{\\sum_j e^{z_j - \\max_j z_j}}$.\n",
    "\n",
    "## 3) Categorical (Multinomial) Likelihood\n",
    "For one sample with one-hot label $y \\in \\{0,1\\}^K$:\n",
    "$$\n",
    "P(y\\mid x) = \\prod_{k=1}^K p_k^{\\,y_k}\n",
    "$$\n",
    "\n",
    "Log-likelihood for one sample:\n",
    "$$\n",
    "\\ell = \\sum_{k=1}^K y_k \\log p_k\n",
    "$$\n",
    "\n",
    "Negative log-likelihood (cross-entropy) for one sample:\n",
    "$$\n",
    "J = -\\ell = -\\sum_{k=1}^K y_k \\log p_k\n",
    "$$\n",
    "\n",
    "For a dataset $\\{(x_i, y_i)\\}_{i=1}^n$ (with $Y$ one-hot):\n",
    "- **Sum loss**:\n",
    "$$\n",
    "J(W,b) = -\\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\log p_{ik}\n",
    "$$\n",
    "- **Mean loss** (common in libraries):\n",
    "$$\n",
    "J(W,b) = -\\frac{1}{n}\\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\log p_{ik}\n",
    "$$\n",
    "\n",
    "## 4) Gradients (No Skipped Steps)\n",
    "\n",
    "### Key derivatives\n",
    "- Softmax Jacobian (for one sample):\n",
    "$$\n",
    "\\frac{\\partial p_k}{\\partial z_j} = p_k(\\delta_{kj} - p_j)\n",
    "$$\n",
    "\n",
    "- By chain rule, for one sample’s loss $J = -\\sum_k y_k \\log p_k$:\n",
    "  1) $\\frac{\\partial J}{\\partial p_k} = -\\frac{y_k}{p_k}$\n",
    "  2) $\\frac{\\partial p_k}{\\partial z_j} = p_k(\\delta_{kj}-p_j)$\n",
    "  3) So\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial z_j}\n",
    "= \\sum_{k=1}^K \\frac{\\partial J}{\\partial p_k}\\frac{\\partial p_k}{\\partial z_j}\n",
    "= \\sum_{k=1}^K \\Big(-\\frac{y_k}{p_k}\\Big) p_k(\\delta_{kj}-p_j)\n",
    "= \\sum_{k=1}^K \\big(-y_k\\delta_{kj} + y_k p_j\\big)\n",
    "= -y_j + p_j \\sum_{k=1}^K y_k\n",
    "= p_j - y_j\n",
    "$$\n",
    "(using $\\sum_k y_k = 1$ for one-hot).\n",
    "\n",
    "Thus, for one sample:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial z} = p - y\n",
    "$$\n",
    "\n",
    "And since $z = W^\\top x + b$:\n",
    "- For $W$ (each class $k$ column):\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_k} = (p_k - y_k)\\,x\n",
    "$$\n",
    "Stacking for all $k$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W} = x\\, (p - y)^\\top \\in \\mathbb{R}^{d \\times K}\n",
    "$$\n",
    "\n",
    "- For $b$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = p - y \\in \\mathbb{R}^K\n",
    "$$\n",
    "\n",
    "### Vectorized over $n$ samples\n",
    "Let $X \\in \\mathbb{R}^{n \\times d}$, $P \\in \\mathbb{R}^{n \\times K}$, $Y \\in \\mathbb{R}^{n \\times K}$:\n",
    "- **Sum loss gradients**:\n",
    "$$\n",
    "\\nabla_W J = X^\\top (P - Y), \\qquad \\nabla_b J = \\mathbf{1}^\\top (P - Y)\n",
    "$$\n",
    "- **Mean loss gradients** (common):\n",
    "$$\n",
    "\\nabla_W J = \\frac{1}{n} X^\\top (P - Y), \\qquad \\nabla_b J = \\frac{1}{n}\\,\\mathbf{1}^\\top (P - Y)\n",
    "$$\n",
    "\n",
    "## 5) Regularization\n",
    "\n",
    "### L2 (weight decay)\n",
    "Add $\\frac{\\lambda}{2}\\|W\\|_F^2$ (no penalty on $b$ typically):\n",
    "$$\n",
    "J_\\text{reg} = J + \\frac{\\lambda}{2}\\|W\\|_F^2\n",
    "$$\n",
    "Gradient update:\n",
    "$$\n",
    "\\nabla_W J_\\text{reg} = \\nabla_W J + \\lambda W, \\qquad \\nabla_b J_\\text{reg} = \\nabla_b J\n",
    "$$\n",
    "\n",
    "### L1\n",
    "Add $\\lambda \\|W\\|_1$:\n",
    "$$\n",
    "\\nabla_W J_\\text{reg} \\in \\nabla_W J + \\lambda\\,\\text{sign}(W) \\quad (\\text{subgradient})\n",
    "$$\n",
    "Effect: sparsity across weights.\n",
    "\n",
    "## 6) OvR vs Multinomial\n",
    "- **OvR (One-vs-Rest)**: train $K$ independent binary logistic regressions; pick class with largest score/prob.\n",
    "- **Multinomial (softmax)**: single model with coupled probabilities via softmax.\n",
    "- In scikit-learn: `LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\"|\"saga\")` trains the true softmax; `multi_class=\"ovr\"` does OvR.\n",
    "\n",
    "## 7) Numerical Stability Tips\n",
    "- Use **log-sum-exp** trick for loss:\n",
    "$$\n",
    "\\log \\sum_j e^{z_j} = a + \\log \\sum_j e^{z_j - a}, \\quad a=\\max_j z_j\n",
    "$$\n",
    "- Stabilized softmax by subtracting $\\max(z)$ before `exp`.\n",
    "\n",
    "## 8) Shapes Recap\n",
    "- $X:(n\\times d),\\; W:(d\\times K),\\; b:(K,),\\; Z= XW + \\mathbf{1}b^\\top:(n\\times K)$\n",
    "- $P=\\text{softmax}(Z):(n\\times K),\\; Y$ one-hot $(n\\times K)$\n",
    "- $\\nabla_W J: (d\\times K),\\; \\nabla_b J:(K,)$\n"
   ],
   "id": "74186fc301384615"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T11:57:09.924721Z",
     "start_time": "2025-09-26T11:57:09.921534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# https://www.deep-ml.com/problems/23\n",
    "# Write a Python function that computes the softmax activation for a given list of scores. The function should return the softmax values as a list, each rounded to four decimal places.\n",
    "import math\n",
    "\n",
    "def softmax(scores: list[float]) -> list[float]:\n",
    "\t# Your code here\n",
    "    exp_lst = [math.exp(s) for s in scores]\n",
    "    return [round(s/sum(exp_lst), 4) for s in exp_lst]\n",
    "\n",
    "scores = [1, 2, 3]\n",
    "output = [0.0900, 0.2447, 0.6652]"
   ],
   "id": "d6dcdaf308207211",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T11:57:10.259574Z",
     "start_time": "2025-09-26T11:57:10.256495Z"
    }
   },
   "cell_type": "code",
   "source": "softmax(scores)",
   "id": "e1ccc103c8bad901",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09, 0.2447, 0.6652]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T12:20:35.223642Z",
     "start_time": "2025-09-26T12:20:35.220538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# https://www.deep-ml.com/problems/39\n",
    "# In machine learning and statistics, the softmax function is a generalization of the logistic function that converts a vector of scores into probabilities. The log-softmax function is the logarithm of the softmax function, and it is often used for numerical stability when computing the softmax of large numbers.\n",
    "# Given a 1D numpy array of scores, implement a Python function to compute the log-softmax of the array.\n",
    "\"\"\"\n",
    "A = np.array([1, 2, 3])\n",
    "print(log_softmax(A))\n",
    "array([-2.4076, -1.4076, -0.4076])\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def log_softmax(scores: list) -> np.ndarray:\n",
    "    exp_lst = np.exp(scores)\n",
    "    sft_mx = exp_lst/sum(exp_lst)\n",
    "    lg_sft_mx = np.round(np.log(sft_mx),4)\n",
    "    return lg_sft_mx\n",
    "A = np.array([1, 2, 3])\n",
    "print(log_softmax(A))"
   ],
   "id": "b3c3ffc60f88072a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4076 -1.4076 -0.4076]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T15:28:38.750757Z",
     "start_time": "2025-09-26T15:28:38.741877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# https://www.deep-ml.com/problems/105\n",
    "# Implement a gradient descent-based training algorithm for Softmax regression. Your task is to compute model parameters using Cross Entropy loss and return the optimized coefficients along with collected loss values over iterations. Make sure to round your solution to 4 decimal places\n",
    "def train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n",
    "    \"\"\"\n",
    "\tGradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n",
    "\t\"\"\"\n",
    "\n",
    "    def one_hot_encoding(y: np.ndarray) -> np.ndarray:\n",
    "        num_classes = len(np.unique(y))\n",
    "        sorted_classes = np.sort(np.unique(y)).tolist()\n",
    "        one_hot_encodings = np.zeros((len(y), num_classes))\n",
    "        for i in range(len(y)):\n",
    "            one_hot_encodings[i][sorted_classes.index(y[i])] = 1.0\n",
    "        return one_hot_encodings\n",
    "\n",
    "    # Step 1 Add bias term to x\n",
    "    m, n = X.shape\n",
    "    X = np.hstack((np.ones((m, 1)), X))\n",
    "    y = one_hot_encoding(y)\n",
    "    y_m, y_n = y.shape\n",
    "    w = np.zeros((n+1, y_n))\n",
    "    losses = []\n",
    "\n",
    "    def softmax_rows(Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Row-wise softmax for a 2D array Z (n×K).\"\"\"\n",
    "        assert Z.ndim == 2, f\"Expected 2D, got shape {Z.shape}\"\n",
    "        Zmax = Z.max(axis=1, keepdims=True)          # (n,1)\n",
    "        expZ = np.exp(Z - Zmax)                      # stability\n",
    "        return expZ / expZ.sum(axis=1, keepdims=True)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        p = softmax_rows(X @ w)\n",
    "        log_loss = np.round((-np.log(p)*y).sum(), decimals=4)\n",
    "        losses.append(log_loss)\n",
    "\n",
    "        gradient = X.T @ (p - y)\n",
    "        w = w - learning_rate * gradient\n",
    "\n",
    "    return np.round(w.T, 4).tolist(), losses\n",
    "\n",
    "train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)"
   ],
   "id": "118ec898ebbcdc26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[-0.0011, 0.0145, -0.0921],\n",
       "  [0.002, -0.0598, 0.1263],\n",
       "  [-0.0009, 0.0453, -0.0342]],\n",
       " [3.2958,\n",
       "  3.2611,\n",
       "  3.2272,\n",
       "  3.1941,\n",
       "  3.1618,\n",
       "  3.1302,\n",
       "  3.0993,\n",
       "  3.0692,\n",
       "  3.0398,\n",
       "  3.011])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
