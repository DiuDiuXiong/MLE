{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:24:39.352610Z",
     "start_time": "2025-09-30T07:24:39.350251Z"
    }
   },
   "cell_type": "code",
   "source": "# Decision Tree (CART, ID3, C4.5, etc.)",
   "id": "92850458f970515b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:24:47.543408Z",
     "start_time": "2025-09-30T07:24:47.541327Z"
    }
   },
   "cell_type": "code",
   "source": "# Random Forest (RF, 4.5 “bridge” between DT and boosting)",
   "id": "f8131a213ee72b24",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:25:02.755733Z",
     "start_time": "2025-09-30T07:25:02.753757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradient Boosted Decision Trees (GBDT)\n",
    "# Good link: https://www.showmeai.tech/article-detail/193"
   ],
   "id": "ebe80766561b679c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:25:04.645697Z",
     "start_time": "2025-09-30T07:25:04.643950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# XGBoost / LightGBM / CatBoost\n",
    "# https://zhuanlan.zhihu.com/p/142413825"
   ],
   "id": "979e6c86f0d534d4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:25:12.028945Z",
     "start_time": "2025-09-30T07:25:12.027138Z"
    }
   },
   "cell_type": "code",
   "source": "# Other Notable Tree Variants",
   "id": "700c3039143657a3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🌳 Decision Tree Family: ID3, C4.5, C5.0, CART\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Entropy & Information Gain (core math first)\n",
    "\n",
    "Let dataset $\\mathcal{D}$, target $Y$, candidate feature $X$.\n",
    "\n",
    "**Entropy of target:**\n",
    "$$\n",
    "H(Y) = - \\sum_{c} p(c) \\log p(c)\n",
    "$$\n",
    "\n",
    "**Conditional entropy (split on $X$):**\n",
    "- If $X$ is categorical:\n",
    "$$\n",
    "H(Y|X) = \\sum_{v \\in \\mathcal{V}} p(X=v) \\; H(Y | X=v)\n",
    "$$\n",
    "\n",
    "- If $X$ is continuous with threshold $t$:\n",
    "$$\n",
    "H(Y|X \\leq t) = \\frac{|\\mathcal{D}_{\\leq t}|}{|\\mathcal{D}|} H(Y | X \\leq t) + \\frac{|\\mathcal{D}_{>t}|}{|\\mathcal{D}|} H(Y | X > t)\n",
    "$$\n",
    "\n",
    "**Information Gain:**\n",
    "$$\n",
    "IG(Y,X) = H(Y) - H(Y|X)\n",
    "$$\n",
    "\n",
    "**Split Information (intrinsic info):**\n",
    "$$\n",
    "SI(X) = - \\sum_{v \\in \\mathcal{V}} p(X=v) \\log p(X=v)\n",
    "$$\n",
    "\n",
    "**Gain Ratio (used by C4.5):**\n",
    "$$\n",
    "GR(Y,X) = \\frac{IG(Y,X)}{SI(X)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 📜 ID3 (Quinlan, 1980s)\n",
    "\n",
    "- **Split criterion:** Information Gain.\n",
    "- **Features:** Categorical only (no native continuous).\n",
    "- **Splits:** Multiway (branch for each category).\n",
    "- **Pruning:** None/minimal → tends to overfit.\n",
    "- **Missing values:** Not handled.\n",
    "\n",
    "---\n",
    "\n",
    "## 📜 C4.5 (1993, Quinlan)\n",
    "\n",
    "- **Split criterion:** Gain Ratio (fixes ID3’s bias toward high-cardinality features).\n",
    "- **Continuous variables:** Handled natively (binary threshold splits).\n",
    "- **Missing values:** Handled via fractional instance weighting.\n",
    "- **Splits:** Multiway (categorical), binary (continuous).\n",
    "- **Pruning:** Error-based pruning (see below).\n",
    "\n",
    "### 🔍 Error-based pruning explained\n",
    "1. Grow the full tree (overfitted).\n",
    "2. For each subtree, estimate **expected error** using a binomial confidence interval:\n",
    "   - If replacing the subtree with a single leaf has **similar expected error**, prune it.\n",
    "3. Intuition: keep only splits that reduce error *confidently*.\n",
    "\n",
    "This avoids overfitting while not being as aggressive as CART’s pruning.\n",
    "\n",
    "---\n",
    "\n",
    "## 📜 C5.0 (Commercial successor)\n",
    "\n",
    "At first glance it looks like “just a faster C4.5”, but it includes real upgrades:\n",
    "\n",
    "- **Memory/computation:**\n",
    "  - C4.5: builds full tree in memory.\n",
    "  - C5.0: compressed data structures (bitmaps, caching) → much lower memory, faster splits.\n",
    "\n",
    "- **Boosting:** Built-in AdaBoost-like option.\n",
    "- **Winnowing:** Drops irrelevant features automatically.\n",
    "- **Cost-sensitive:** Allows weighting of misclassification types.\n",
    "- **Rule sets:** Can convert trees into compact, interpretable rules.\n",
    "- **Overall:** Same logical core as C4.5, but industrial-strength for large, noisy datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 📜 CART (Breiman et al., 1984)\n",
    "\n",
    "- **Split criteria:**\n",
    "  - Classification: Gini impurity (or entropy).\n",
    "    $$\n",
    "    Gini(Y) = 1 - \\sum_c p(c)^2\n",
    "    $$\n",
    "  - Regression: MSE (variance reduction).\n",
    "- **Splits:** Always **binary**, even for categorical features.\n",
    "- **Regression support:** Native (variance).\n",
    "- **Missing values:** Handled via surrogate splits.\n",
    "- **Pruning:** Cost–complexity pruning (details below).\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 CART for Regression\n",
    "\n",
    "CART isn’t limited to classification — it was designed to also support **regression tasks**.\n",
    "This makes it more general-purpose than ID3/C4.5, which are classification-only.\n",
    "\n",
    "### 🔹 Impurity Measure for Regression\n",
    "Instead of entropy or Gini, CART uses **variance** (equivalently MSE).\n",
    "\n",
    "At a node $t$ with target values $\\{y_i\\}_{i=1}^N$:\n",
    "\n",
    "$$\n",
    "Var(t) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\bar{y})^2, \\quad \\bar{y} = \\frac{1}{N}\\sum_{i=1}^N y_i\n",
    "$$\n",
    "\n",
    "### 🔹 Split Selection\n",
    "When splitting a node into left/right children:\n",
    "\n",
    "$$\n",
    "\\Delta Var = Var(parent) - \\Big( \\frac{N_L}{N} Var(L) + \\frac{N_R}{N} Var(R) \\Big)\n",
    "$$\n",
    "\n",
    "- $N_L$, $N_R$: number of samples in left/right child.\n",
    "- Choose the split that **maximizes variance reduction** (just like maximizing Gini reduction in classification).\n",
    "\n",
    "### 🔹 Predictions at Leaves\n",
    "- **Classification:** majority class in leaf.\n",
    "- **Regression:** average of the target values in leaf:\n",
    "  $$\n",
    "  \\hat{y}_{leaf} = \\frac{1}{N_{leaf}} \\sum_{i \\in leaf} y_i\n",
    "  $$\n",
    "\n",
    "### 🔹 Why This Matters\n",
    "- ID3 / C4.5 use entropy-based criteria, which only apply to discrete class distributions.\n",
    "- CART defined splitting more generally:\n",
    "  - Classification → Gini/entropy.\n",
    "  - Regression → Variance/MSE.\n",
    "- This dual design made CART the **foundation for Random Forests and Gradient Boosting**, which often need regression trees internally (e.g., GBDT fits residuals with regression splits).\n",
    "\n",
    "---\n",
    "\n",
    "## ✂️ CART’s Cost–Complexity Pruning\n",
    "\n",
    "Define cost:\n",
    "$$\n",
    "R_\\alpha(T) = R(T) + \\alpha |T|\n",
    "$$\n",
    "- $R(T)$: empirical error (misclass or MSE).\n",
    "- $|T|$: number of leaves.\n",
    "- $\\alpha$: complexity penalty.\n",
    "\n",
    "### Weakest link pruning (back-merging)\n",
    "1. Start with the full tree.\n",
    "2. For each internal node $t$, compute:\n",
    "   $$\n",
    "   \\alpha(t) = \\frac{R(t) - R(T_t)}{|T_t|-1}\n",
    "   $$\n",
    "   where $T_t$ is the subtree rooted at $t$.\n",
    "   - Intuition: error increase per leaf removed.\n",
    "3. Find the node with smallest $\\alpha(t)$ (“weakest link”).\n",
    "4. Collapse its subtree into a leaf.\n",
    "5. Repeat until only the root remains.\n",
    "\n",
    "This produces a **sequence of nested trees**:\n",
    "$$\n",
    "T_0 \\supset T_1 \\supset T_2 \\supset \\dots \\supset T_m\n",
    "$$\n",
    "\n",
    "Finally, choose the optimal subtree via **cross-validation**.\n",
    "\n",
    "---\n",
    "\n",
    "## ❓ Clarification Points (Q&A merged into notes)\n",
    "\n",
    "### Q1: Difference between C4.5 and C5.0?\n",
    "- C5.0 isn’t just “faster.” It adds boosting, feature winnowing, cost-sensitive learning, memory compression, and rule extraction. Much more production-ready.\n",
    "\n",
    "### Q2: C4.5 pruning — what does \"error-based pruning after full tree\" mean?\n",
    "- It means grow first, then prune back subtrees where error reduction isn’t statistically significant, using confidence intervals.\n",
    "\n",
    "### Q3: Details for CART pruning?\n",
    "- CART uses cost–complexity pruning with weakest link merging. See formula for $\\alpha(t)$ above. Produces full pruning path down to a single leaf.\n",
    "\n",
    "### Q4: Why CART is better for modern ensembles?\n",
    "- Ensembles (Random Forest, GBDT, XGBoost) require:\n",
    "  - **Binary splits** (CART enforces this).\n",
    "  - **Regression support** (variance reduction\n"
   ],
   "id": "85130909461b0a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T13:21:01.720015Z",
     "start_time": "2025-09-30T13:21:01.709079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "def learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n",
    "  # Your code here\n",
    "  # Step 0 If there's only one feature, do majority count\n",
    "  if len(attributes) == 1:\n",
    "    a = attributes[0]\n",
    "    attr_to_target = {}\n",
    "    for e in examples:\n",
    "        if e[a] not in attr_to_target:\n",
    "            attr_to_target[e[a]] = []\n",
    "            attr_to_target[e[a]].append(e[target_attr])\n",
    "    return {a: {k: majority(v) for k,v in attr_to_target.items()}}\n",
    "  # Step 1 Build y_vals, {attribute: {attribute_val: [y_vals]}}\n",
    "  y_vals = []\n",
    "  attr_to_y_vals = {}\n",
    "  for e in examples:\n",
    "      y_vals.append(e[target_attr])\n",
    "      for a in attributes:\n",
    "          if a not in attr_to_y_vals:\n",
    "              attr_to_y_vals[a] = {}\n",
    "          if e[a] not in attr_to_y_vals[a]: attr_to_y_vals[a][e[a]] = []\n",
    "          attr_to_y_vals[a][e[a]].append(e[target_attr])\n",
    "  # Step 2 Check IG # Since we only want to compare IG, and they will all be h(y) - h(y|x), so we jsut pick the one with smallest h(y|x)\n",
    "  attr_to_ig = {}\n",
    "  for a in attributes:\n",
    "      attr_to_ig[a] = h_x(attr_to_y_vals[a])\n",
    "\n",
    "  # Step 3 Choose the best attributes\n",
    "  lowest_key = min(attr_to_ig.items(), key=lambda kv: (kv[1], kv[0]))[0]\n",
    "  new_attributes = attributes.copy()\n",
    "  new_attributes.remove(lowest_key)\n",
    "  dt = {lowest_key: {}}\n",
    "  for k, v in attr_to_y_vals[lowest_key].items():\n",
    "      if len(set(v)) == 1:\n",
    "          dt[lowest_key][k] = v[0]\n",
    "      else:\n",
    "          dt[lowest_key][k] = learn_decision_tree([e for e in examples if e[lowest_key] == k], new_attributes, target_attr)\n",
    "  # a. if y pure -> val # b. if y not pure -> recurssion\n",
    "  return dt\n",
    "def majority(y_vals: list[str]):\n",
    "    counts = Counter(y_vals)\n",
    "    majority_label, _ = counts.most_common(1)[0]\n",
    "    return majority_label\n",
    "\n",
    "def h(feature: list[str]):\n",
    "    count = {}\n",
    "    for f in feature:\n",
    "      if f in count:\n",
    "          count[f] += 1\n",
    "      else:\n",
    "          count[f] = 1\n",
    "    for k in count.keys():\n",
    "        count[k] /= len(feature)\n",
    "    h_val = 0\n",
    "    for v in count.values():\n",
    "        h_val -= v*math.log(v)\n",
    "    return h_val\n",
    "\n",
    "def h_x(feature: dict[str, list[str]]):\n",
    "    h_sum = 0\n",
    "    items = 0\n",
    "    for x, y_vals in feature.items():\n",
    "        h_sum += h(y_vals)*len(y_vals)\n",
    "        items += len(y_vals)\n",
    "    return h_sum/items\n",
    "\n",
    "print(learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis'))"
   ],
   "id": "13f2130ca4bb565a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Overcast': 'Yes', 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}}}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T15:05:11.142296Z",
     "start_time": "2025-09-30T15:05:11.140170Z"
    }
   },
   "cell_type": "code",
   "source": "# https://www.deep-ml.com/problems/138\n",
   "id": "71a44e4c98fe96b0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gradient Boosted Decision Trees (GBDT) — practical, intuition-first notes\n",
    "\n",
    "> These are copy-paste friendly notebook notes (markdown cells). Math is kept light and only where it sharpens intuition.\n",
    "\n",
    "---\n",
    "\n",
    "## What GBDT is (in one breath)\n",
    "\n",
    "- We build a **prediction function** \\(F(x)\\) as a **sum of small trees**:\n",
    "  $$\n",
    "  F_M(x) = F_0(x) + \\nu \\sum_{m=1}^{M} \\gamma_m\\, T_m(x)\n",
    "  $$\n",
    "  where each small regression tree \\(T_m\\) nudges \\(F\\) a little, \\(\\nu\\) is a **learning rate**, and \\(\\gamma_m\\) is a step size.\n",
    "\n",
    "- For **binary classification**, we usually let \\(F(x)\\) represent **log-odds** and then map it to probability with the sigmoid:\n",
    "  $$\n",
    "  p(x) = \\sigma(F(x)) = \\frac{1}{1+e^{-F(x)}}\n",
    "  $$\n",
    "\n",
    "- Training is **stage-wise**: at each round we add one more tree that reduces the chosen loss on the current model’s mistakes.\n",
    "\n",
    "---\n",
    "\n",
    "## Why “boost the gradient” (and not the residual) — using logistic loss\n",
    "\n",
    "### The short answer\n",
    "- “Residuals” are only the right error notion for **squared error**. For other losses, “residual” doesn’t align with the loss surface.\n",
    "- The **gradient** of the loss tells you the direction of steepest improvement **for the loss you actually care about**. So we fit a weak learner to the **negative gradient** and add it to the model.\n",
    "\n",
    "### The binary logistic case (intuition, not derivation)\n",
    "- Logistic loss for a label \\(y\\in\\{0,1\\}\\) and predicted probability \\(p=\\sigma(F)\\) is:\n",
    "  $$\n",
    "  \\ell(y,p) = -\\big[y\\log p + (1-y)\\log(1-p)\\big]\n",
    "  $$\n",
    "- If you ask “what small change \\(\\delta\\) in the **score** \\(F\\) would most reduce \\(\\ell\\) right now?”, calculus answers: **move in the negative gradient** of \\(\\ell\\) w.r.t. \\(F\\).\n",
    "- For logistic loss, that negative gradient at each sample becomes:\n",
    "  $$\n",
    "  -\\frac{\\partial \\ell}{\\partial F} = y - p\n",
    "  $$\n",
    "  which you can read as **“desired probability minus current probability.”**\n",
    "\n",
    "- Key intuition: in classification, your target is **not** to match labels directly with the raw score \\(F\\); it’s to make **probabilities** correct after the **link** \\(p=\\sigma(F)\\). The gradient **bakes in the link** and the loss shape automatically.\n",
    "\n",
    "- Using “residual = label − prediction” only makes sense when the prediction is on the same scale as the target **and** the loss is squared error. With logistic, the right “residual-like” signal is exactly the **negative gradient** \\(y-p\\) in **score space**.\n",
    "\n",
    "### Takeaways\n",
    "- Boosting = **functional gradient descent**: each tree approximates the negative gradient field over \\(x\\).\n",
    "- This generalizes to **any differentiable loss** (ranking, Poisson, quantile, etc.). Residuals don’t.\n",
    "\n",
    "---\n",
    "\n",
    "## Why use CART regression trees as weak learners?\n",
    "\n",
    "### Practical reasons\n",
    "- **Piecewise-constant tweaks**: regression trees predict a constant per leaf. Perfect for **local adjustments** to \\(F(x)\\) in regions where the gradient says “push up” or “push down.”\n",
    "- **Captures interactions**: Even shallow trees model **feature interactions**. Stacking many gives strong non-linearity.\n",
    "- **Scale-free, low prep**: Minimal preprocessing needed, robust to feature scaling and missingness.\n",
    "- **Fast fitting**: Many small trees are computationally efficient with histogram/approximate split finding.\n",
    "- **Simple regularization knobs**: Depth, min samples per leaf, L2 on leaf values, subsampling.\n",
    "\n",
    "### Conceptual fit\n",
    "- At each round, we have a target “signal” (the negative gradient per sample). A regression tree is a **universal, cheap function approximator** that fits that signal piecewise.\n",
    "- With second-order updates (XGBoost/LightGBM), leaf values are set by Newton updates (using gradients + Hessians), and CART leaves (constants) make that closed-form.\n",
    "\n",
    "---\n",
    "\n",
    "## Anatomy of one boosting round (binary logistic)\n",
    "\n",
    "1. Current model: \\(F_{m-1}(x)\\) → probabilities \\(p=\\sigma(F_{m-1}(x))\\).\n",
    "2. Compute pseudo-residuals: \\(r_i = y_i - p_i\\).\n",
    "3. Fit a small regression tree \\(T_m(x)\\) to \\(\\{(x_i, r_i)\\}\\).\n",
    "4. Compute step sizes per leaf (line search or Newton step).\n",
    "5. Update model:\n",
    "   $$\n",
    "   F_m(x) = F_{m-1}(x) + \\nu \\cdot \\gamma_m T_m(x)\n",
    "   $$\n",
    "\n",
    "> Each tree is a local “probability correction map” saying: *in this region, we’re under-predicting positives by ~0.03; bump the score a bit.*\n",
    "\n",
    "---\n",
    "\n",
    "## The role of learning rate \\(\\nu\\)\n",
    "\n",
    "- Small \\(\\nu\\) = cautious steps, many trees → smoother path, better generalization.\n",
    "- Large \\(\\nu\\) = fewer trees, but high risk of overfit.\n",
    "- Typical: \\(\\nu \\in [0.01, 0.1]\\) with hundreds–thousands of trees.\n",
    "\n",
    "---\n",
    "\n",
    "## Regularization that matters\n",
    "\n",
    "- **Tree depth**: shallow trees = simpler corrections = less variance.\n",
    "- **Min samples/weight per leaf**: avoids fitting noise.\n",
    "- **L2 on leaf values**: shrinks extreme corrections.\n",
    "- **Row/col subsampling**: adds diversity, stabilizes trees.\n",
    "- **Early stopping**: prevents fitting noise.\n",
    "\n",
    "---\n",
    "\n",
    "## Initialization & class imbalance\n",
    "\n",
    "- Start with \\(F_0\\) = log-odds of base rate. Avoids wasting trees rediscovering prior.\n",
    "- For imbalance:\n",
    "  - Use **class weights** or **scale_pos_weight**.\n",
    "  - Tune threshold post-training for your metric.\n",
    "  - Validate with PR-AUC / recall when imbalance is severe.\n",
    "\n",
    "---\n",
    "\n",
    "## GBDT vs linear models\n",
    "\n",
    "- **GBDT wins**: strong feature interactions, thresholds, minimal preprocessing.\n",
    "- **Linear wins**: very high-dim sparse data, smooth extrapolation, calibrated probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## CART vs alternatives\n",
    "\n",
    "- **CART regression trees**: workhorse, piecewise constant.\n",
    "- **Oblivious/symmetric trees (CatBoost)**: same split at each depth, efficient with categoricals.\n",
    "- **Linear leaves**: add local linear trends, costlier.\n",
    "\n",
    "---\n",
    "\n",
    "## First-order vs second-order\n",
    "\n",
    "- First-order: gradients only, stable.\n",
    "- Second-order: gradients + Hessians, faster convergence. Hessian naturally downweights extreme/saturated points.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical tuning recipe\n",
    "\n",
    "1. Shallow trees (depth 3–6).\n",
    "2. \\(\\nu \\sim 0.05–0.1\\).\n",
    "3. Subsample rows/columns.\n",
    "4. Use early stopping.\n",
    "5. If overfitting → smaller \\(\\nu\\), shallower trees, stronger L2, bigger min leaf, more subsampling.\n",
    "6. Calibrate probabilities if needed.\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretability\n",
    "\n",
    "- **Feature importance**: what features matter.\n",
    "- **Partial dependence / ICE**: how features shift score.\n",
    "- **SHAP**: per-feature contributions.\n",
    "\n",
    "---\n",
    "\n",
    "## Missing values & categoricals\n",
    "\n",
    "- Trees can branch on NaNs → no imputation needed.\n",
    "- CatBoost/LightGBM handle categoricals natively; otherwise use careful encoding.\n",
    "\n",
    "---\n",
    "\n",
    "## Pitfalls\n",
    "\n",
    "- Too deep + high LR = overfit.\n",
    "- No early stopping = wasted trees.\n",
    "- Ignoring imbalance = misleading metrics.\n",
    "- Target leakage with encodings = catastrophic overfit.\n",
    "- Tiny validation = unstable early stopping.\n",
    "\n",
    "---\n",
    "\n",
    "## Boosting vs bagging\n",
    "\n",
    "- **Bagging (Random Forests)**: deep independent trees, reduce variance by averaging.\n",
    "- **Boosting (GBDT)**: shallow sequential trees, reduce bias by correcting gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## Checklist for future self\n",
    "\n",
    "- ✅ Work in **score space**.\n",
    "- ✅ Use negative gradients, not residuals.\n",
    "- ✅ Prefer small trees + shrinkage.\n",
    "- ✅ Always early stop.\n",
    "- ✅ Handle imbalance explicitly.\n",
    "- ✅ Choose library wisely (XGB, LGBM, CatBoost).\n",
    "- ✅ Interpret with SHAP.\n",
    "\n",
    "---\n",
    "\n",
    "## Minimal recap\n",
    "\n",
    "- Predict score \\(F(x)\\).\n",
    "- Probability = \\(\\sigma(F)\\).\n",
    "- Negative gradient = error signal.\n",
    "- Fit tree to it, add it in.\n",
    "- Use shrinkage + regularization.\n",
    "- That’s GBDT.\n"
   ],
   "id": "ef54949092b2d96f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GBDT — Interview Q&A Cheatsheet\n",
    "\n",
    "---\n",
    "\n",
    "## Q: What is boosting?\n",
    "- **Answer:** Boosting is an ensemble technique where we build models sequentially, each new model focusing on correcting the mistakes of the previous ones.\n",
    "- In GBDT, the correction is guided by the **negative gradient of the loss**, so each tree learns where the current model is wrong and nudges predictions in the right direction.\n",
    "- Compared to bagging (like Random Forests), boosting reduces **bias** more than variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Q: How is GBDT different from Random Forests?\n",
    "- **Random Forests (bagging):**\n",
    "  - Train many **deep trees independently** on bootstrapped samples.\n",
    "  - Use **Gini/entropy** as split criteria (classification).\n",
    "  - Reduce **variance** via averaging.\n",
    "- **GBDT (boosting):**\n",
    "  - Train many **shallow regression trees sequentially**.\n",
    "  - Each tree fits the **gradient of the loss**.\n",
    "  - Reduce **bias** by stage-wise functional gradient descent.\n",
    "- **Key difference:** RF = “majority vote of independent trees,” GBDT = “sum of sequential corrective trees.”\n",
    "\n",
    "---\n",
    "\n",
    "## Q: Why do we fit gradients instead of residuals?\n",
    "- Residuals = correct signal only for squared error (MSE).\n",
    "- For other losses (e.g. logistic), residuals don’t align with the true loss surface.\n",
    "- The **gradient of the loss w.r.t. prediction** is the right direction to reduce *any* differentiable loss.\n",
    "- Example (logistic loss): gradient = \\(y - p\\), i.e. desired probability minus predicted probability.\n",
    "- So fitting gradients = aligning each tree with the objective you care about.\n",
    "\n",
    "---\n",
    "\n",
    "## Q: How does logistic loss work with GBDT?\n",
    "- For binary classification:\n",
    "  - Model outputs a **score** \\(F(x)\\) (log-odds).\n",
    "  - Probability = \\(\\sigma(F(x)) = 1 / (1 + e^{-F(x)})\\).\n",
    "  - Logistic loss: \\(-[y\\log p + (1-y)\\log(1-p)]\\).\n",
    "- The gradient is \\(g = p - y\\).\n",
    "- The Hessian is \\(h = p(1-p)\\).\n",
    "- Each tree is fit to these gradients/Hessians, providing score corrections that shift probabilities closer to labels.\n",
    "- Interpretation: trees are not predicting classes directly, they’re **probability corrections** in log-odds space.\n",
    "\n",
    "---\n",
    "\n",
    "## Q: How do we prevent overfitting in GBDT?\n",
    "- **Learning rate (shrinkage):** smaller steps, more trees → smoother fit.\n",
    "- **Tree depth / num leaves:** shallow trees capture only simple corrections.\n",
    "- **Min samples/weight per leaf:** avoids splits on noise.\n",
    "- **Regularization (λ, γ):** penalize large leaf values or unnecessary splits.\n",
    "- **Subsampling (rows/columns):** adds randomness, reduces correlation between trees.\n",
    "- **Early stopping:** stop adding trees once validation loss stops improving.\n",
    "- **Feature engineering & leakage control:** ensure features don’t encode target info.\n",
    "\n",
    "---\n",
    "\n",
    "# Quick Tips for Interview\n",
    "- Emphasize **functional gradient descent** as the key principle of GBDT.\n",
    "- Always contrast with **Random Forests** (independent bagging vs sequential boosting).\n",
    "- Show awareness of **Hessians**: “XGBoost/LightGBM use second-order updates for stability.”\n",
    "- Overfitting controls = learning rate, shallow trees, early stopping.\n"
   ],
   "id": "29743b7c5f471393"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:09:04.518538Z",
     "start_time": "2025-10-01T11:09:04.512727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paste into a Jupyter *code* cell and run to render the notes as Markdown\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "md = r\"\"\"\n",
    "# GBDT/XGBoost Split Scoring via Taylor Expansion — One-Cell Notes\n",
    "\n",
    "## Why Taylor expansion shows up\n",
    "We add a new tree \\(f_t\\) to the current ensemble score \\(F^{(t-1)}(x)\\). The round-\\(t\\) objective is\n",
    "$$\n",
    "\\mathcal L^{(t)}=\\sum_{i=1}^n \\ell\\!\\big(y_i,\\;F^{(t-1)}(x_i)+f_t(x_i)\\big)+\\Omega(f_t).\n",
    "$$\n",
    "Approximate the change in loss with a **second-order Taylor expansion** around \\(F^{(t-1)}(x_i)\\):\n",
    "$$\n",
    "\\ell\\!\\big(y_i,\\,F^{(t-1)}(x_i)+f_t(x_i)\\big)\\;\\approx\\;\\ell\\!\\big(y_i,\\,\\hat y_i^{(t-1)}\\big)\\;+\\;g_i\\,f_t(x_i)\\;+\\;\\tfrac12\\,h_i\\,f_t(x_i)^2,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "g_i=\\frac{\\partial \\ell}{\\partial F}\\Big|_{F=F^{(t-1)}(x_i)},\\qquad\n",
    "h_i=\\frac{\\partial^2 \\ell}{\\partial F^2}\\Big|_{F=F^{(t-1)}(x_i)}.\n",
    "$$\n",
    "\n",
    "A tree predicts a **constant** \\(w_j\\) per leaf \\(j\\). If leaf \\(j\\) receives samples \\(I_j\\), its quadratic objective with L2 regularization \\(\\lambda\\) is\n",
    "$$\n",
    "\\sum_{i\\in I_j}\\!\\Big(g_i w_j+\\tfrac12 h_i w_j^2\\Big)\\;+\\;\\tfrac{\\lambda}{2}w_j^2,\n",
    "$$\n",
    "minimized by the **closed-form leaf value**\n",
    "$$\n",
    "w_j^\\*=-\\frac{\\sum_{i\\in I_j} g_i}{\\sum_{i\\in I_j} h_i+\\lambda}=-\\frac{G_j}{H_j+\\lambda}.\n",
    "$$\n",
    "\n",
    "**Key idea:** The Taylor surrogate gives (1) closed-form leaf predictions and (2) a principled **split gain** used to pick the best (feature, threshold).\n",
    "\n",
    "---\n",
    "\n",
    "## How the split (feature + threshold) is chosen\n",
    "At a node with aggregates \\(G=\\sum g_i,\\;H=\\sum h_i\\), consider splitting into left/right children with \\((G_L,H_L)\\) and \\((G_R,H_R)\\). The **gain** (loss reduction) is\n",
    "$$\n",
    "\\text{Gain}\\;=\\;\\frac{1}{2}\\!\\left(\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{G^2}{H+\\lambda}\\right)-\\gamma.\n",
    "$$\n",
    "- Evaluate this **for each candidate threshold of each feature** (exact or histogram/quantile binned).\n",
    "- Pick the split with **largest positive** Gain; if the best Gain \\(\\le 0\\), **don’t split** (pruned by \\(\\gamma\\)).\n",
    "\n",
    "This is why modern GBDT libraries optimize **loss reduction via gradients/Hessians**, not Gini/entropy (those are for standalone classification trees/Random Forests).\n",
    "\n",
    "---\n",
    "\n",
    "## Symbol glossary (render-safe math)\n",
    "\n",
    "**Feature vector**\n",
    "$$x_i \\in \\mathbb{R}^d$$\n",
    "\n",
    "**Target**\n",
    "$$y_i \\in \\{0,1\\}\\ \\text{or}\\ \\mathbb{R}$$\n",
    "\n",
    "**Ensemble score after \\(t\\) rounds**\n",
    "$$F^{(t)}(x) \\in \\mathbb{R}$$\n",
    "\n",
    "**Prediction (e.g., logistic)**\n",
    "$$\\hat y_i^{(t)}=\\sigma\\!\\big(F^{(t)}(x_i)\\big),\\quad \\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "**Per-sample loss**\n",
    "$$\\ell(y_i,\\hat y_i)$$\n",
    "\n",
    "**Gradient (at current score \\(F^{(t-1)}(x_i)\\))**\n",
    "$$g_i=\\frac{\\partial \\ell(y_i,\\hat y_i)}{\\partial F}\\Big|_{F=F^{(t-1)}(x_i)}$$\n",
    "\n",
    "**Hessian (second derivative at current score)**\n",
    "$$h_i=\\frac{\\partial^2 \\ell(y_i,\\hat y_i)}{\\partial F^2}\\Big|_{F=F^{(t-1)}(x_i)}$$\n",
    "\n",
    "**New tree at round \\(t\\)**\n",
    "$$f_t(x)$$\n",
    "\n",
    "**Leaf index set**\n",
    "$$I_j=\\{\\,i:\\ x_i\\ \\text{falls into leaf } j\\,\\}$$\n",
    "\n",
    "**Leaf value (score the tree outputs)**\n",
    "$$w_j \\in \\mathbb{R}$$\n",
    "\n",
    "**Aggregated gradient/Hessian in leaf \\(j\\)**\n",
    "$$G_j=\\sum_{i\\in I_j} g_i,\\qquad H_j=\\sum_{i\\in I_j} h_i$$\n",
    "\n",
    "**Optimal leaf value (with L2)**\n",
    "$$w_j^\\*=-\\frac{G_j}{H_j+\\lambda}$$\n",
    "\n",
    "**Regularization (XGBoost-style complexity)**\n",
    "$$\\Omega(f)=\\gamma T+\\frac{\\lambda}{2}\\sum_{j=1}^T w_j^2\\quad\\text{(optionally }+\\ \\alpha\\,\\lVert w\\rVert_{1}\\text{)}$$\n",
    "\n",
    "**Split gain (parent \\(G,H\\) into left/right \\(G_L,H_L\\) and \\(G_R,H_R\\))**\n",
    "$$\n",
    "\\text{Gain}\n",
    "=\n",
    "\\frac{1}{2}\\!\\left(\n",
    "\\frac{G_L^2}{H_L+\\lambda}\n",
    "+\n",
    "\\frac{G_R^2}{H_R+\\lambda}\n",
    "-\n",
    "\\frac{G^2}{H+\\lambda}\n",
    "\\right)-\\gamma\n",
    "$$\n",
    "\n",
    "**Learning rate / ensemble update**\n",
    "$$F^{(t)}(x)=F^{(t-1)}(x)+\\eta\\, f_t(x),\\qquad \\eta\\in(0,1]$$\n",
    "\n",
    "**Binary logistic specific derivatives**\n",
    "$$g_i=\\hat y_i^{(t-1)}-y_i,\\qquad h_i=\\hat y_i^{(t-1)}\\big(1-\\hat y_i^{(t-1)}\\big)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Logistic loss example (binary classification)\n",
    "Let \\(F\\) be the score and \\(\\hat y=\\sigma(F)=1/(1+e^{-F})\\). With\n",
    "$$\n",
    "\\ell(y,\\hat y)=-\\big(y\\log \\hat y+(1-y)\\log(1-\\hat y)\\big),\n",
    "$$\n",
    "we have the derivatives\n",
    "$$\n",
    "g=\\hat y-y,\\qquad h=\\hat y(1-\\hat y).\n",
    "$$\n",
    "Plug these \\(g,h\\) into \\(w^\\*=-G/(H+\\lambda)\\) and the **Gain** formula above.\n",
    "**Why keep \\(h\\)?** It yields a **Newton step in function space**, improving step size and split scoring vs. first-order residual fitting—especially important for logistic loss where gradients can saturate.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical notes & gotchas\n",
    "- **Not Gini/entropy:** Boosted trees pick splits by **loss reduction** from the Taylor surrogate, not impurity.\n",
    "- **Thresholds:** exact (all unique values) vs. **histogram/quantile** binned (default in XGBoost/LightGBM/sklearn-hist).\n",
    "- **Regularization interacts with splitting:** larger \\(\\lambda\\), \\(\\gamma\\), and `min_child_weight` (minimum child Hessian sum) suppress weak/noisy splits.\n",
    "- **Missing values (NaN):** per split, learn a **default direction** (left/right) that maximizes Gain—no explicit imputation needed.\n",
    "- **Early stopping + shrinkage:** low \\(\\eta\\) with many trees and validation-based early stopping is a strong, reliable regularizer.\n",
    "\n",
    "---\n",
    "\n",
    "## Interview-style recap (one-liners)\n",
    "- **Why Taylor expansion?** Quadratic surrogate \\(\\Rightarrow\\) closed-form leaf values + principled split gains.\n",
    "- **Why Hessians?** Curvature → Newton-like updates; faster, stabler convergence than residual-only fits.\n",
    "- **How is the split chosen?** Evaluate Gain over (feature, threshold); take the **largest positive**; else stop splitting.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(md))\n"
   ],
   "id": "bf3b16dea9303778",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n# GBDT/XGBoost Split Scoring via Taylor Expansion — One-Cell Notes\n\n## Why Taylor expansion shows up\nWe add a new tree \\(f_t\\) to the current ensemble score \\(F^{(t-1)}(x)\\). The round-\\(t\\) objective is\n$$\n\\mathcal L^{(t)}=\\sum_{i=1}^n \\ell\\!\\big(y_i,\\;F^{(t-1)}(x_i)+f_t(x_i)\\big)+\\Omega(f_t).\n$$\nApproximate the change in loss with a **second-order Taylor expansion** around \\(F^{(t-1)}(x_i)\\):\n$$\n\\ell\\!\\big(y_i,\\,F^{(t-1)}(x_i)+f_t(x_i)\\big)\\;\\approx\\;\\ell\\!\\big(y_i,\\,\\hat y_i^{(t-1)}\\big)\\;+\\;g_i\\,f_t(x_i)\\;+\\;\\tfrac12\\,h_i\\,f_t(x_i)^2,\n$$\nwhere\n$$\ng_i=\\frac{\\partial \\ell}{\\partial F}\\Big|_{F=F^{(t-1)}(x_i)},\\qquad\nh_i=\\frac{\\partial^2 \\ell}{\\partial F^2}\\Big|_{F=F^{(t-1)}(x_i)}.\n$$\n\nA tree predicts a **constant** \\(w_j\\) per leaf \\(j\\). If leaf \\(j\\) receives samples \\(I_j\\), its quadratic objective with L2 regularization \\(\\lambda\\) is\n$$\n\\sum_{i\\in I_j}\\!\\Big(g_i w_j+\\tfrac12 h_i w_j^2\\Big)\\;+\\;\\tfrac{\\lambda}{2}w_j^2,\n$$\nminimized by the **closed-form leaf value**\n$$\nw_j^\\*=-\\frac{\\sum_{i\\in I_j} g_i}{\\sum_{i\\in I_j} h_i+\\lambda}=-\\frac{G_j}{H_j+\\lambda}.\n$$\n\n**Key idea:** The Taylor surrogate gives (1) closed-form leaf predictions and (2) a principled **split gain** used to pick the best (feature, threshold).\n\n---\n\n## How the split (feature + threshold) is chosen\nAt a node with aggregates \\(G=\\sum g_i,\\;H=\\sum h_i\\), consider splitting into left/right children with \\((G_L,H_L)\\) and \\((G_R,H_R)\\). The **gain** (loss reduction) is\n$$\n\\text{Gain}\\;=\\;\\frac{1}{2}\\!\\left(\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{G^2}{H+\\lambda}\\right)-\\gamma.\n$$\n- Evaluate this **for each candidate threshold of each feature** (exact or histogram/quantile binned).\n- Pick the split with **largest positive** Gain; if the best Gain \\(\\le 0\\), **don’t split** (pruned by \\(\\gamma\\)).\n\nThis is why modern GBDT libraries optimize **loss reduction via gradients/Hessians**, not Gini/entropy (those are for standalone classification trees/Random Forests).\n\n---\n\n## Symbol glossary (render-safe math)\n\n**Feature vector**\n$$x_i \\in \\mathbb{R}^d$$\n\n**Target**\n$$y_i \\in \\{0,1\\}\\ \\text{or}\\ \\mathbb{R}$$\n\n**Ensemble score after \\(t\\) rounds**\n$$F^{(t)}(x) \\in \\mathbb{R}$$\n\n**Prediction (e.g., logistic)**\n$$\\hat y_i^{(t)}=\\sigma\\!\\big(F^{(t)}(x_i)\\big),\\quad \\sigma(z)=\\frac{1}{1+e^{-z}}$$\n\n**Per-sample loss**\n$$\\ell(y_i,\\hat y_i)$$\n\n**Gradient (at current score \\(F^{(t-1)}(x_i)\\))**\n$$g_i=\\frac{\\partial \\ell(y_i,\\hat y_i)}{\\partial F}\\Big|_{F=F^{(t-1)}(x_i)}$$\n\n**Hessian (second derivative at current score)**\n$$h_i=\\frac{\\partial^2 \\ell(y_i,\\hat y_i)}{\\partial F^2}\\Big|_{F=F^{(t-1)}(x_i)}$$\n\n**New tree at round \\(t\\)**\n$$f_t(x)$$\n\n**Leaf index set**\n$$I_j=\\{\\,i:\\ x_i\\ \\text{falls into leaf } j\\,\\}$$\n\n**Leaf value (score the tree outputs)**\n$$w_j \\in \\mathbb{R}$$\n\n**Aggregated gradient/Hessian in leaf \\(j\\)**\n$$G_j=\\sum_{i\\in I_j} g_i,\\qquad H_j=\\sum_{i\\in I_j} h_i$$\n\n**Optimal leaf value (with L2)**\n$$w_j^\\*=-\\frac{G_j}{H_j+\\lambda}$$\n\n**Regularization (XGBoost-style complexity)**\n$$\\Omega(f)=\\gamma T+\\frac{\\lambda}{2}\\sum_{j=1}^T w_j^2\\quad\\text{(optionally }+\\ \\alpha\\,\\lVert w\\rVert_{1}\\text{)}$$\n\n**Split gain (parent \\(G,H\\) into left/right \\(G_L,H_L\\) and \\(G_R,H_R\\))**\n$$\n\\text{Gain}\n=\n\\frac{1}{2}\\!\\left(\n\\frac{G_L^2}{H_L+\\lambda}\n+\n\\frac{G_R^2}{H_R+\\lambda}\n-\n\\frac{G^2}{H+\\lambda}\n\\right)-\\gamma\n$$\n\n**Learning rate / ensemble update**\n$$F^{(t)}(x)=F^{(t-1)}(x)+\\eta\\, f_t(x),\\qquad \\eta\\in(0,1]$$\n\n**Binary logistic specific derivatives**\n$$g_i=\\hat y_i^{(t-1)}-y_i,\\qquad h_i=\\hat y_i^{(t-1)}\\big(1-\\hat y_i^{(t-1)}\\big)$$\n\n---\n\n## Logistic loss example (binary classification)\nLet \\(F\\) be the score and \\(\\hat y=\\sigma(F)=1/(1+e^{-F})\\). With\n$$\n\\ell(y,\\hat y)=-\\big(y\\log \\hat y+(1-y)\\log(1-\\hat y)\\big),\n$$\nwe have the derivatives\n$$\ng=\\hat y-y,\\qquad h=\\hat y(1-\\hat y).\n$$\nPlug these \\(g,h\\) into \\(w^\\*=-G/(H+\\lambda)\\) and the **Gain** formula above.\n**Why keep \\(h\\)?** It yields a **Newton step in function space**, improving step size and split scoring vs. first-order residual fitting—especially important for logistic loss where gradients can saturate.\n\n---\n\n## Practical notes & gotchas\n- **Not Gini/entropy:** Boosted trees pick splits by **loss reduction** from the Taylor surrogate, not impurity.\n- **Thresholds:** exact (all unique values) vs. **histogram/quantile** binned (default in XGBoost/LightGBM/sklearn-hist).\n- **Regularization interacts with splitting:** larger \\(\\lambda\\), \\(\\gamma\\), and `min_child_weight` (minimum child Hessian sum) suppress weak/noisy splits.\n- **Missing values (NaN):** per split, learn a **default direction** (left/right) that maximizes Gain—no explicit imputation needed.\n- **Early stopping + shrinkage:** low \\(\\eta\\) with many trees and validation-based early stopping is a strong, reliable regularizer.\n\n---\n\n## Interview-style recap (one-liners)\n- **Why Taylor expansion?** Quadratic surrogate \\(\\Rightarrow\\) closed-form leaf values + principled split gains.\n- **Why Hessians?** Curvature → Newton-like updates; faster, stabler convergence than residual-only fits.\n- **How is the split chosen?** Evaluate Gain over (feature, threshold); take the **largest positive**; else stop splitting.\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:16:20.713838Z",
     "start_time": "2025-10-01T11:16:20.709374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paste into a Jupyter *code* cell and run to render the notes as Markdown.\n",
    "# (This version avoids LaTeX inside tables, which can break MathJax in some setups.)\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "md = r\"\"\"\n",
    "# XGBoost vs. (Vanilla) GBDT — One-Cell Cheatsheet\n",
    "\n",
    "## TL;DR\n",
    "- **Both** are additive ensembles of CART trees trained stage-wise to minimize a loss.\n",
    "- **Vanilla GBDT** typically uses **first-order** boosting (fit pseudo-residuals) with lighter explicit regularization.\n",
    "- **XGBoost** uses a **second-order (Taylor)** approximation (gradients *and* Hessians), a **regularized objective**, and strong **systems optimizations**.\n",
    "\n",
    "---\n",
    "\n",
    "## Where the math differs (core formulas)\n",
    "\n",
    "**Taylor (second-order) surrogate (per sample)**\n",
    "\\[\n",
    "\\ell(y, F+f) \\approx \\ell(y,F) + g\\,f + \\tfrac12 h\\,f^2,\n",
    "\\quad\n",
    "g=\\frac{\\partial \\ell}{\\partial F},\\;\n",
    "h=\\frac{\\partial^2 \\ell}{\\partial F^2}.\n",
    "\\]\n",
    "\n",
    "**Leaf value in XGBoost (with L2 \\(\\lambda\\))**\n",
    "\\[\n",
    "w^\\* \\;=\\; -\\frac{G}{H+\\lambda},\n",
    "\\quad\n",
    "G=\\sum g_i,\\; H=\\sum h_i \\text{ over the leaf.}\n",
    "\\]\n",
    "\n",
    "**Split gain (parent \\(G,H\\) → children \\(G_L,H_L\\), \\(G_R,H_R\\))**\n",
    "\\[\n",
    "\\text{Gain}\n",
    "=\n",
    "\\frac{1}{2}\\!\\left(\n",
    "\\frac{G_L^2}{H_L+\\lambda}\n",
    "+\n",
    "\\frac{G_R^2}{H_R+\\lambda}\n",
    "-\n",
    "\\frac{G^2}{H+\\lambda}\n",
    "\\right) - \\gamma .\n",
    "\\]\n",
    "\n",
    "**Binary logistic derivatives (score \\(F\\), \\(\\hat y=\\sigma(F)\\))**\n",
    "\\[\n",
    "g=\\hat y-y,\n",
    "\\qquad\n",
    "h=\\hat y(1-\\hat y),\n",
    "\\qquad\n",
    "\\hat y=\\frac{1}{1+e^{-F}}.\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Conceptual comparison (bullets instead of a table)\n",
    "\n",
    "**Update type**\n",
    "- *GBDT*: First-order (fit residuals) or gradient-only.\n",
    "- *XGBoost*: Second-order (uses \\(g\\) and \\(h\\)) → Newton-like step.\n",
    "\n",
    "**Objective**\n",
    "- *GBDT*: Empirical loss (regularization mostly via depth, min samples, shrinkage).\n",
    "- *XGBoost*: Loss **+** complexity\n",
    "  \\[\n",
    "  \\Omega(f)=\\gamma T+\\frac{\\lambda}{2}\\sum_j w_j^2\n",
    "  \\quad (\\text{optionally } + \\alpha \\|w\\|_1).\n",
    "  \\]\n",
    "\n",
    "**Split criterion**\n",
    "- *GBDT*: Loss reduction (often residual SSE for regression).\n",
    "- *XGBoost*: **Regularized Gain** (formula above), pruned if \\(\\le 0\\).\n",
    "\n",
    "**Missing values**\n",
    "- *GBDT*: Usually impute/ignore.\n",
    "- *XGBoost*: Learns a default direction (left/right) per split to maximize Gain.\n",
    "\n",
    "**Performance**\n",
    "- *GBDT*: Library-dependent; exact or histogram splits.\n",
    "- *XGBoost*: Histogram/quantile sketch, column/row subsampling, parallelism, out-of-core.\n",
    "\n",
    "**Regularization knobs**\n",
    "- *GBDT*: `learning_rate`, `max_depth`, `min_samples_leaf`, early stopping.\n",
    "- *XGBoost*: Above **plus** `reg_lambda` (λ), `reg_alpha` (α), `gamma` (γ), `min_child_weight` (min Hessian mass).\n",
    "\n",
    "---\n",
    "\n",
    "## Practical implications\n",
    "\n",
    "- **Stability & convergence**: Second-order updates give better step sizes and split scoring (especially for logistic loss).\n",
    "- **Overfitting control**: Shrinkage + early stopping in both; XGBoost’s λ/α/γ/min_child_weight directly appear in the split math.\n",
    "- **Sparsity/missingness**: XGBoost’s learned default direction often beats naïve imputation.\n",
    "- **Speed/scale**: XGBoost’s hist + subsampling + threading is very fast on large/sparse data.\n",
    "\n",
    "---\n",
    "\n",
    "## When to use which (rules of thumb)\n",
    "\n",
    "- Choose **XGBoost** for large/sparse/imbalanced data, logistic loss, or when you want stronger defaults and native missing handling.\n",
    "- A **simple GBDT** can be fine for small/moderate data and quick prototypes.\n",
    "\n",
    "---\n",
    "\n",
    "## Minimal set to memorize\n",
    "\n",
    "\\[\n",
    "w^\\* = -\\frac{G}{H+\\lambda}\n",
    "\\qquad\n",
    "\\text{Gain} = \\tfrac12\\!\\left(\\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda}\\right) - \\gamma\n",
    "\\qquad\n",
    "g=\\hat y-y,\\; h=\\hat y(1-\\hat y)\n",
    "\\]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(md))\n"
   ],
   "id": "235732521a5bf5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n# XGBoost vs. (Vanilla) GBDT — One-Cell Cheatsheet\n\n## TL;DR\n- **Both** are additive ensembles of CART trees trained stage-wise to minimize a loss.\n- **Vanilla GBDT** typically uses **first-order** boosting (fit pseudo-residuals) with lighter explicit regularization.\n- **XGBoost** uses a **second-order (Taylor)** approximation (gradients *and* Hessians), a **regularized objective**, and strong **systems optimizations**.\n\n---\n\n## Where the math differs (core formulas)\n\n**Taylor (second-order) surrogate (per sample)**\n\\[\n\\ell(y, F+f) \\approx \\ell(y,F) + g\\,f + \\tfrac12 h\\,f^2,\n\\quad\ng=\\frac{\\partial \\ell}{\\partial F},\\;\nh=\\frac{\\partial^2 \\ell}{\\partial F^2}.\n\\]\n\n**Leaf value in XGBoost (with L2 \\(\\lambda\\))**\n\\[\nw^\\* \\;=\\; -\\frac{G}{H+\\lambda},\n\\quad\nG=\\sum g_i,\\; H=\\sum h_i \\text{ over the leaf.}\n\\]\n\n**Split gain (parent \\(G,H\\) → children \\(G_L,H_L\\), \\(G_R,H_R\\))**\n\\[\n\\text{Gain}\n=\n\\frac{1}{2}\\!\\left(\n\\frac{G_L^2}{H_L+\\lambda}\n+\n\\frac{G_R^2}{H_R+\\lambda}\n-\n\\frac{G^2}{H+\\lambda}\n\\right) - \\gamma .\n\\]\n\n**Binary logistic derivatives (score \\(F\\), \\(\\hat y=\\sigma(F)\\))**\n\\[\ng=\\hat y-y,\n\\qquad\nh=\\hat y(1-\\hat y),\n\\qquad\n\\hat y=\\frac{1}{1+e^{-F}}.\n\\]\n\n---\n\n## Conceptual comparison (bullets instead of a table)\n\n**Update type**\n- *GBDT*: First-order (fit residuals) or gradient-only.\n- *XGBoost*: Second-order (uses \\(g\\) and \\(h\\)) → Newton-like step.\n\n**Objective**\n- *GBDT*: Empirical loss (regularization mostly via depth, min samples, shrinkage).\n- *XGBoost*: Loss **+** complexity\n  \\[\n  \\Omega(f)=\\gamma T+\\frac{\\lambda}{2}\\sum_j w_j^2\n  \\quad (\\text{optionally } + \\alpha \\|w\\|_1).\n  \\]\n\n**Split criterion**\n- *GBDT*: Loss reduction (often residual SSE for regression).\n- *XGBoost*: **Regularized Gain** (formula above), pruned if \\(\\le 0\\).\n\n**Missing values**\n- *GBDT*: Usually impute/ignore.\n- *XGBoost*: Learns a default direction (left/right) per split to maximize Gain.\n\n**Performance**\n- *GBDT*: Library-dependent; exact or histogram splits.\n- *XGBoost*: Histogram/quantile sketch, column/row subsampling, parallelism, out-of-core.\n\n**Regularization knobs**\n- *GBDT*: `learning_rate`, `max_depth`, `min_samples_leaf`, early stopping.\n- *XGBoost*: Above **plus** `reg_lambda` (λ), `reg_alpha` (α), `gamma` (γ), `min_child_weight` (min Hessian mass).\n\n---\n\n## Practical implications\n\n- **Stability & convergence**: Second-order updates give better step sizes and split scoring (especially for logistic loss).\n- **Overfitting control**: Shrinkage + early stopping in both; XGBoost’s λ/α/γ/min_child_weight directly appear in the split math.\n- **Sparsity/missingness**: XGBoost’s learned default direction often beats naïve imputation.\n- **Speed/scale**: XGBoost’s hist + subsampling + threading is very fast on large/sparse data.\n\n---\n\n## When to use which (rules of thumb)\n\n- Choose **XGBoost** for large/sparse/imbalanced data, logistic loss, or when you want stronger defaults and native missing handling.\n- A **simple GBDT** can be fine for small/moderate data and quick prototypes.\n\n---\n\n## Minimal set to memorize\n\n\\[\nw^\\* = -\\frac{G}{H+\\lambda}\n\\qquad\n\\text{Gain} = \\tfrac12\\!\\left(\\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda}\\right) - \\gamma\n\\qquad\ng=\\hat y-y,\\; h=\\hat y(1-\\hat y)\n\\]\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🌲 Random Forest (RF) Notes\n",
    "\n",
    "## What is Random Forest?\n",
    "- Ensemble method built from **many decision trees**.\n",
    "- Uses **bagging (bootstrap aggregation)** + **random feature selection**.\n",
    "- Final prediction = **majority vote (classification)** or **average (regression)**.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Ideas\n",
    "1. **Bootstrap Sampling**: each tree trained on random sample of data (with replacement).\n",
    "2. **Random Feature Selection**: only a random subset of features considered at each split → decorrelates trees.\n",
    "3. **Aggregation**: combine predictions from all trees.\n",
    "\n",
    "---\n",
    "\n",
    "## Why it Works\n",
    "- Decision trees = high variance learners.\n",
    "- Bagging reduces variance by averaging many models.\n",
    "- Random features ensure diversity → less correlated errors.\n",
    "\n",
    "---\n",
    "\n",
    "## Strengths\n",
    "- Robust against overfitting compared to single trees.\n",
    "- Handles high-dimensional data well.\n",
    "- Provides **feature importance** estimates.\n",
    "\n",
    "## Weaknesses\n",
    "- Less interpretable than a single tree.\n",
    "- Can be slower with many trees.\n",
    "- May still struggle on highly imbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Typical Hyperparameters\n",
    "- **n_estimators**: number of trees (larger = less variance).\n",
    "- **max_features**: features considered at each split.\n",
    "- **max_depth**: max tree depth (controls overfitting).\n",
    "- **min_samples_split / min_samples_leaf**: smoothness of predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest vs GBDT\n",
    "- **RF**: parallel trees, bagging, variance reduction.\n",
    "- **GBDT**: sequential trees, boosting, bias reduction.\n"
   ],
   "id": "d705d433c45e5a1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📒 XGBoost Split Math & Why It’s Fast (gᵢ, hᵢ, f*, Gain)\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Setup: Boosting with a New Tree\n",
    "At boosting step \\(t\\), we add a tree \\(f\\) to current predictions \\(\\hat{y}^{(t-1)}\\):\n",
    "\n",
    "$$\n",
    "\\text{Obj}^{(t)} = \\sum_i l\\big(y_i, \\hat{y}_i^{(t-1)} + f(x_i)\\big) + \\Omega(f)\n",
    "$$\n",
    "\n",
    "We do a **2nd-order Taylor expansion** around \\(\\hat{y}_i^{(t-1)}\\):\n",
    "\n",
    "$$\n",
    "l\\big(y_i, \\hat{y}_i^{(t-1)} + f(x_i)\\big) \\approx l(y_i,\\hat{y}_i) + g_i f(x_i) + \\tfrac{1}{2} h_i f(x_i)^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "g_i=\\frac{\\partial l}{\\partial \\hat{y}}\\Big|_{\\hat{y}=\\hat{y}_i^{(t-1)}},\\quad\n",
    "h_i=\\frac{\\partial^2 l}{\\partial \\hat{y}^2}\\Big|_{\\hat{y}=\\hat{y}_i^{(t-1)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Leaf Value Is a Scalar \\(f\\) (Per Leaf)\n",
    "Each leaf outputs a constant \\(f\\). All samples in the leaf get their prediction shifted by \\(f\\).\n",
    "\n",
    "Define leaf sums:\n",
    "\n",
    "$$\n",
    "G=\\sum_{i\\in \\text{leaf}} g_i,\\quad H=\\sum_{i\\in \\text{leaf}} h_i\n",
    "$$\n",
    "\n",
    "The approximated objective for one leaf with L2 penalty \\(\\lambda\\):\n",
    "\n",
    "$$\n",
    "\\text{Obj}_\\text{leaf} = G f + \\tfrac12 (H+\\lambda) f^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Why the Optimal \\(f^*\\) Depends on G, H, λ\n",
    "Take derivative wrt \\(f\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Obj}_\\text{leaf}}{\\partial f} = G + (H+\\lambda) f\n",
    "$$\n",
    "\n",
    "Set to zero:\n",
    "\n",
    "$$\n",
    "G + (H+\\lambda) f^* = 0\n",
    "$$\n",
    "\n",
    "Solve:\n",
    "\n",
    "$$\n",
    "f^* = -\\frac{G}{H+\\lambda}\n",
    "$$\n",
    "\n",
    "**Interpretation**\n",
    "- \\(G = \\sum g_i\\): how strong the gradients push the predictions in one direction.\n",
    "- \\(H = \\sum h_i\\): curvature (confidence in that direction).\n",
    "- \\(\\lambda\\): regularization, prevents extreme weights when \\(H\\) is small.\n",
    "\n",
    "So \\(f^*\\) is essentially a **Newton step**: gradient / curvature, with regularization for stability.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Minimum Leaf Contribution\n",
    "Plugging back into the objective:\n",
    "\n",
    "$$\n",
    "\\text{Obj}_\\text{leaf}^{\\min} = -\\tfrac12 \\frac{G^2}{H+\\lambda}\n",
    "$$\n",
    "\n",
    "This value is used to compare different split structures.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Split Gain Formula (Parent → Left/Right)\n",
    "For a candidate split that partitions samples into **Left (L)** and **Right (R)**:\n",
    "\n",
    "- Parent:\n",
    "  $$ G=G_L+G_R,\\; H=H_L+H_R $$\n",
    "\n",
    "- Gain from split:\n",
    "\n",
    "$$\n",
    "\\text{Gain}=\\tfrac12\\Big(\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{G^2}{H+\\lambda}\\Big)-\\gamma\n",
    "$$\n",
    "\n",
    "- \\(\\gamma\\): penalty for adding a leaf (controls complexity).\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Why This Is Fast\n",
    "- Compute \\(g_i,h_i\\) **once per sample** per boosting round.\n",
    "- Split search uses only **aggregated sums** (\\(G,H\\)) via prefix sums.\n",
    "- Closed-form \\(f^*\\) means **no search over leaf outputs**.\n",
    "- Each split evaluation reduces to simple arithmetic → much faster than recomputing the full loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) TL;DR\n",
    "- \\(g_i,h_i\\) = per-sample gradient & curvature of the loss.\n",
    "- Optimal leaf weight:\n",
    "  $$ f^* = -\\frac{G}{H+\\lambda} $$\n",
    "- This formula comes from minimizing the quadratic approximation, making training efficient.\n",
    "- Split gain is then derived entirely from these aggregated terms, enabling fast search across thresholds.\n"
   ],
   "id": "46b4c80fb9e0c332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8cfb63ae2f860282"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
