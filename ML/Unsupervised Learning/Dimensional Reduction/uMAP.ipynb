{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🌀 Understanding UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "UMAP is a **nonlinear dimensionality reduction** technique that builds upon concepts from **manifold learning** and **fuzzy topology**.\n",
    "It’s similar in goal to *t-SNE* but is faster, preserves more global structure, and can be used as a general embedding method.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 1️⃣ Overview\n",
    "\n",
    "UMAP assumes that:\n",
    "\n",
    "> High-dimensional data lie on a manifold, and we want to find a low-dimensional projection that preserves the manifold’s *local topology*.\n",
    "\n",
    "So instead of matching probability distributions like t-SNE,\n",
    "UMAP builds a **fuzzy topological graph** in high-D and optimizes a low-D representation that preserves it.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 2️⃣ Step-by-Step Mechanics\n",
    "\n",
    "### (a) Build a weighted graph in high-D\n",
    "\n",
    "For each data point \\(i\\):\n",
    "\n",
    "1. Find its **k nearest neighbors** (based on some distance metric).\n",
    "2. Define connection strength:\n",
    "\n",
    "   $$\n",
    "   w_{ij} = \\exp\\!\\left(-\\frac{\\max(0,\\, d(i,j) - \\rho_i)}{\\sigma_i}\\right)\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - \\(d(i,j)\\): distance between \\(i\\) and \\(j\\)\n",
    "   - \\(\\rho_i\\): distance to \\(i\\)’s nearest neighbor (local connectivity floor)\n",
    "   - \\(\\sigma_i\\): smooths distances so all points have similar entropy\n",
    "\n",
    "3. Symmetrize with a fuzzy union:\n",
    "\n",
    "   $$\n",
    "   w^{(sym)}_{ij} = w_{ij} + w_{ji} - w_{ij} w_{ji}\n",
    "   $$\n",
    "\n",
    "This gives a **fuzzy simplicial set** (a graph encoding probabilistic connections).\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Define the low-D graph\n",
    "\n",
    "In embedding space, define a smooth connection curve:\n",
    "\n",
    "$$\n",
    "w'_{ij} = \\frac{1}{1 + a\\,{d'(i,j)}^{2b}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(d'(i,j)\\): distance between embedded points \\(y_i\\) and \\(y_j\\)\n",
    "- \\(a,b\\) ≈ (1.929, 0.7915): constants controlling curve shape\n",
    "\n",
    "---\n",
    "\n",
    "### (c) Optimize embeddings\n",
    "\n",
    "UMAP minimizes a **cross-entropy** loss between the high-D and low-D fuzzy graphs:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i \\ne j} \\Big[-w_{ij}\\log(w'_{ij}) - (1 - w_{ij})\\log(1 - w'_{ij})\\Big]\n",
    "$$\n",
    "\n",
    "- The first term pulls close (neighbor) points together.\n",
    "- The second term pushes unrelated (non-neighbor) points apart.\n",
    "\n",
    "Optimization is performed via **stochastic gradient descent (SGD)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 3️⃣ Intuition Behind the Components\n",
    "\n",
    "| Symbol | Meaning | Description |\n",
    "|---------|----------|-------------|\n",
    "| \\(d(i,j)\\) | High-D distance | Distance between points in original data space |\n",
    "| \\(w_{ij}\\) | High-D connection weight | Fuzzy measure of neighbor strength |\n",
    "| \\(d'(i,j)\\) | Low-D distance | Distance between embedded points |\n",
    "| \\(w'_{ij}\\) | Low-D connection weight | Learned similarity curve |\n",
    "| Loss \\(L\\) | Cross-entropy | Encourages \\(w'_{ij} \\approx w_{ij}\\) |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 4️⃣ Handling Neighbors vs Non-neighbors\n",
    "\n",
    "- The **k-NN graph** defines which pairs \\((i,j)\\) are neighbors.\n",
    "- For neighbors → compute attractive term:\n",
    "  $$-w_{ij}\\log(w'_{ij})$$\n",
    "- For non-neighbors → approximate the repulsive term via **negative sampling**:\n",
    "  $$-\\log(1 - w'_{ij^-})$$\n",
    "\n",
    "This avoids computing all \\(O(N^2)\\) pairs — only a few random negatives are sampled for each positive edge.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ 5️⃣ Why UMAP Is Fast\n",
    "\n",
    "1. **Approximate k-NN graph** via **NN-Descent** (O(N log N)):\n",
    "   - Iteratively refines neighbor lists using neighbor-of-neighbor propagation.\n",
    "2. **Sparse graph representation**:\n",
    "   - Only store k edges per node (not all pairs).\n",
    "3. **SGD optimization** with sampled edges:\n",
    "   - No full normalization constant like in t-SNE.\n",
    "\n",
    "Result: linear or near-linear scaling with dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 6️⃣ The Distances \\(d(i,j)\\) and \\(d'(i,j)\\)\n",
    "\n",
    "| Symbol | Space | Typical Metric | Used For |\n",
    "|---------|--------|----------------|-----------|\n",
    "| \\(d(i,j)\\) | High-dimensional space | Euclidean / Cosine / Hamming | Building the fuzzy k-NN graph |\n",
    "| \\(d'(i,j)\\) | Low-dimensional embedding | Euclidean | Measuring similarity between embedded points |\n",
    "\n",
    "### High-D weights:\n",
    "$$\n",
    "w_{ij} = \\exp\\!\\left(-\\frac{\\max(0,\\,d(i,j)-\\rho_i)}{\\sigma_i}\\right)\n",
    "$$\n",
    "\n",
    "### Low-D weights:\n",
    "$$\n",
    "w'_{ij} = \\frac{1}{1 + a\\,{d'(i,j)}^{2b}}\n",
    "$$\n",
    "\n",
    "UMAP optimizes embeddings so that these match: \\(w'_{ij} \\approx w_{ij}\\).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 7️⃣ Relation to t-SNE\n",
    "\n",
    "| Aspect | **t-SNE** | **UMAP** |\n",
    "|--------|------------|-----------|\n",
    "| Core idea | Match pairwise probabilities | Preserve local fuzzy topology |\n",
    "| High-D similarity | Gaussian | Exponential with adaptive scaling |\n",
    "| Low-D similarity | Student-t | Smooth curve \\(1/(1+a d^{2b})\\) |\n",
    "| Loss | KL divergence | Cross-entropy |\n",
    "| Optimization | Gradient descent | SGD with sampled negatives |\n",
    "| Structure preserved | Local only | Local + moderate global |\n",
    "| Complexity | O(N²) | O(N log N) |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 8️⃣ Geometric Intuition\n",
    "\n",
    "Think of each point as having a *local connectivity radius* \\( \\rho_i \\):\n",
    "- Inside that radius → fully connected (\\(w_{ij} = 1\\))\n",
    "- Outside → exponentially decaying connection strength\n",
    "\n",
    "In low-D space, points are moved so that their connection curve \\(w'_{ij}\\) matches \\(w_{ij}\\).\n",
    "This yields an embedding that preserves local clusters and overall manifold shape.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 9️⃣ Summary\n",
    "\n",
    "- \\(d(i,j)\\): high-dimensional distance\n",
    "- \\(d'(i,j)\\): embedding-space distance\n",
    "- \\(w_{ij}\\): fuzzy high-D connectivity\n",
    "- \\(w'_{ij}\\): learned connectivity in low-D\n",
    "- Optimization: SGD to minimize cross-entropy\n",
    "- Non-neighbors: approximated via random negative sampling\n",
    "- Efficiency: NN-Descent + sparse graph → O(N log N) scaling\n",
    "\n",
    "> **t-SNE** matches pairwise probabilities.\n",
    "> **UMAP** matches fuzzy graph connectivity.\n",
    "\n",
    "Result: faster, more scalable, and often better at preserving global structure.\n",
    "\n",
    "---\n",
    "\n",
    "*References:*\n",
    "- McInnes, Healy, & Melville (2018). *UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.*\n",
    "- Official implementation: [https://umap-learn.readthedocs.io](https://umap-learn.readthedocs.io)\n"
   ],
   "id": "fdf5f8a980ca34ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔍 Why UMAP Preserves Global Structure (vs t-SNE)\n",
    "\n",
    "## 1️⃣ Pairwise penalties\n",
    "\n",
    "### t-SNE loss\n",
    "$$\n",
    "D_{KL}(P\\|Q)=\\sum_{ij} p_{ij}\\log\\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "- **Missed neighbor**: \\(p\\) large, \\(q\\) small → big term\n",
    "  e.g. \\(p=0.1, q=0.001 \\Rightarrow 0.46\\)\n",
    "- **False neighbor**: \\(p\\) tiny, \\(q\\) large → small term\n",
    "  e.g. \\(p=10^{-4}, q=0.2 \\Rightarrow 7.6\\times10^{-4}\\)\n",
    "\n",
    "➡ KL heavily penalizes *missed* neighbors but barely cares about *false* ones → strong local clustering, weak global layout.\n",
    "\n",
    "---\n",
    "\n",
    "### UMAP loss\n",
    "$$\n",
    "L_{ij}=-[w_{ij}\\log w'_{ij}+(1-w_{ij})\\log(1-w'_{ij})]\n",
    "$$\n",
    "- **Missed neighbor**: \\(w\\!\\approx\\!1,\\ w'\\!\\to\\!0 \\Rightarrow\\) large\n",
    "- **False neighbor**: \\(w\\!\\approx\\!0,\\ w'\\!\\to\\!1 \\Rightarrow\\) large\n",
    "\n",
    "Both directions penalized → symmetric pressure.\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Gradients (qualitative)\n",
    "\n",
    "| Case | t-SNE | UMAP |\n",
    "|------|--------|-------|\n",
    "| Missed neighbor | Strong pull (big \\(p-q\\)) | Strong pull |\n",
    "| False neighbor | Weak push (small \\(p\\)) | Strong push (via sampled negatives) |\n",
    "\n",
    "KL’s asymmetry ⇒ points repel weakly across clusters, so clusters drift apart.\n",
    "UMAP’s balanced cross-entropy ⇒ consistent pull-push, preserving manifold continuity.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "| Property | **t-SNE** | **UMAP** |\n",
    "|-----------|------------|-----------|\n",
    "| Divergence | Asymmetric KL | Symmetric cross-entropy |\n",
    "| Penalizes | Missed neighbors only (strongly) | Both missed & false |\n",
    "| Effect | Tight, separated blobs | Continuous global manifold |\n",
    "\n",
    "> **t-SNE:** “protect local neighbors.”\n",
    "> **UMAP:** “preserve neighborhoods *and* their global connections.”\n"
   ],
   "id": "e96292a308eda57e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📘 Parameters and Intuitions: t-SNE vs UMAP\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Overview\n",
    "\n",
    "Both **t-SNE** and **UMAP** aim to reduce dimensionality while preserving local structure.\n",
    "They differ mainly in **how they model similarity** and **what their key parameters control**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 t-SNE Parameters\n",
    "\n",
    "### **1. Perplexity**\n",
    "\n",
    "**Definition:**\n",
    "Perplexity controls the *effective number of neighbors* each point considers when defining its local probability distribution.\n",
    "\n",
    "Formally, for each point \\(x_i\\):\n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n",
    "$$\n",
    "\n",
    "where \\( \\sigma_i \\) is chosen such that:\n",
    "\n",
    "$$\n",
    "2^{H(P_i)} = \\text{perplexity}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i}\n",
    "$$\n",
    "\n",
    "So **perplexity ≈ effective number of neighbors**.\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "- Small perplexity → very local view → captures fine clusters, may ignore global layout\n",
    "- Large perplexity → wider neighborhood → captures more global structure but may blur clusters\n",
    "\n",
    "Typical range: **5 – 50**\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Learning Rate, Early Exaggeration (brief)**\n",
    "\n",
    "- **Learning rate** affects how fast embeddings move — too small slows convergence, too large causes collapse.\n",
    "- **Early exaggeration** temporarily increases attractive forces between similar points early on, helping clusters separate before refinement.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 UMAP Parameters\n",
    "\n",
    "UMAP builds a **K-NN graph** in high-dim space and optimizes a low-dim embedding that preserves those relationships.\n",
    "\n",
    "### **1. n_neighbors**\n",
    "\n",
    "Controls the *size of the local neighborhood* considered when building the graph.\n",
    "\n",
    "- Small → focus on local structure\n",
    "- Large → capture more global relationships\n",
    "\n",
    "This is conceptually similar to t-SNE’s perplexity, but deterministic (KNN-based rather than entropy-based).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. min_dist**\n",
    "\n",
    "Controls *how tightly points are allowed to pack together* in the low-dim embedding.\n",
    "\n",
    "During optimization, the low-dim connection strength between points \\(i,j\\) is:\n",
    "\n",
    "$$\n",
    "w'_{ij} = \\frac{1}{1 + a\\|y_i - y_j\\|^{2b}}\n",
    "$$\n",
    "\n",
    "where \\(a,b\\) are derived from `min_dist`.\n",
    "\n",
    "- Smaller `min_dist` → points can get very close → compact clusters\n",
    "- Larger `min_dist` → points repel earlier → smoother, more spread-out structure\n",
    "\n",
    "| min_dist | Effect |\n",
    "|-----------|---------|\n",
    "| 0.0 | Very tight clusters |\n",
    "| 0.1 (default) | Balanced local/global |\n",
    "| 0.5 | Broader structure |\n",
    "| >0.8 | Global, blurry clusters |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Interplay between n_neighbors and min_dist**\n",
    "\n",
    "| n_neighbors | min_dist | Result |\n",
    "|--------------|-----------|---------|\n",
    "| Small (5–15) | Small (0.0–0.1) | Very local, dense clusters |\n",
    "| Large (30–100) | Large (0.3–0.8) | Global continuity, smooth manifold |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Conceptual Analogy\n",
    "\n",
    "| Concept | t-SNE | UMAP |\n",
    "|----------|--------|-------|\n",
    "| Locality parameter | *Perplexity* → \"How many neighbors each point considers\" | *n_neighbors* → \"How many neighbors are linked in the KNN graph\" |\n",
    "| Cluster compactness | Controlled indirectly | *min_dist* → \"How close points can sit together\" |\n",
    "| Mechanism | Probabilistic similarity matching | Graph + topological optimization |\n",
    "| Effect | Emphasizes local clusters | Balances local + global structure |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Summary Table\n",
    "\n",
    "| Aspect | t-SNE | UMAP |\n",
    "|--------|--------|-------|\n",
    "| Core parameters | `perplexity`, `learning_rate`, `early_exaggeration` | `n_neighbors`, `min_dist` |\n",
    "| Defines locality by | Entropy-based Gaussian width | KNN graph |\n",
    "| Controls cluster size via | Indirectly (learning rate, exaggeration) | Directly (min_dist) |\n",
    "| Local vs Global balance | Tuned by perplexity | Tuned by n_neighbors |\n",
    "| Typical use | Visualization | Visualization or general reduction |\n",
    "| Speed / scale | Slower | Much faster |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Quick Takeaway\n",
    "\n",
    "- **t-SNE:** adjusts *how many neighbors* each point “feels” → via **perplexity**.\n",
    "- **UMAP:** fixes *how many neighbors* via **n_neighbors**, then adjusts *how tightly they’re packed* via **min_dist**.\n",
    "\n",
    "In short:\n",
    "\n",
    "> *t-SNE learns how wide to look; UMAP learns how close to hug.*\n"
   ],
   "id": "7055656d3301921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "352cd4cbdf045b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
