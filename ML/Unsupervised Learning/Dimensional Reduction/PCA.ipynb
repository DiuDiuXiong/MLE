{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß© PCA (Original, Intuitive Version)\n",
    "\n",
    "So PCA is basically trying to find many **directions** (let‚Äôs call them \\( v_1, v_2, \\dots \\)) such that\n",
    "each \\( v \\) projects the original data \\( X \\) onto a **line**,\n",
    "and that line (projection) captures **as much variance** as possible.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Step-by-step intuition\n",
    "\n",
    "1Ô∏è‚É£ **Assume there exists some direction \\( v \\)**\n",
    "   such that projecting \\( X \\) onto it gives the **maximum variance** possible.\n",
    "   This projection is just:\n",
    "   $$\n",
    "   y = Xv\n",
    "   $$\n",
    "   where \\( X \\in \\mathbb{R}^{n \\times d} \\) (n samples, d features),\n",
    "   and \\( v \\in \\mathbb{R}^d \\) is the direction vector we‚Äôre testing.\n",
    "\n",
    "---\n",
    "\n",
    "2Ô∏è‚É£ **Compute the variance along that direction**\n",
    "\n",
    "   The variance of those projected values is:\n",
    "   $$\n",
    "   \\text{Var}(Xv) = v^\\top \\Sigma v\n",
    "   $$\n",
    "   where\n",
    "   $$\n",
    "   \\Sigma = \\frac{1}{n-1} X_c^\\top X_c\n",
    "   $$\n",
    "   is the covariance matrix of the centered data \\( X_c \\).\n",
    "\n",
    "   Intuitively, \\( v^\\top \\Sigma v \\) tells us **how spread out the data looks** when we view it from direction \\( v \\).\n",
    "\n",
    "---\n",
    "\n",
    "3Ô∏è‚É£ **Add a constraint so we‚Äôre only comparing directions, not magnitudes**\n",
    "\n",
    "   We only care about **direction**, not scale,\n",
    "   so we force the vector to have **unit length**:\n",
    "   $$\n",
    "   \\|v\\| = 1\n",
    "   $$\n",
    "\n",
    "   Then PCA becomes an optimization problem:\n",
    "   $$\n",
    "   \\max_{\\|v\\| = 1} v^\\top \\Sigma v\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "4Ô∏è‚É£ **Solve that optimization ‚Üí eigen decomposition**\n",
    "\n",
    "   This problem‚Äôs solution is given by the **eigenvectors** and **eigenvalues** of \\( \\Sigma \\):\n",
    "\n",
    "   $$\n",
    "   \\Sigma v = \\lambda v\n",
    "   $$\n",
    "\n",
    "   - \\( v \\): direction (principal component)\n",
    "   - \\( \\lambda \\): how much variance data has along that direction\n",
    "\n",
    "   The direction with the **largest eigenvalue** gives the **largest variance** when projecting data onto it.\n",
    "\n",
    "---\n",
    "\n",
    "5Ô∏è‚É£ **Get multiple principal directions**\n",
    "\n",
    "   After the first one (\\( v_1 \\)),\n",
    "   we find more directions (\\( v_2, v_3, \\dots \\)) that:\n",
    "   - are **orthogonal** to the earlier ones, and\n",
    "   - capture the next highest variances.\n",
    "\n",
    "   So overall:\n",
    "   - \\( v_1 \\): 1st principal component (max variance)\n",
    "   - \\( v_2 \\): 2nd principal component (next max variance)\n",
    "   - \\( \\dots \\)\n",
    "\n",
    "---\n",
    "\n",
    "6Ô∏è‚É£ **Projection and reconstruction**\n",
    "\n",
    "   Once we have the top \\( k \\) eigenvectors \\( V_k = [v_1, v_2, \\dots, v_k] \\),\n",
    "   we can project the data into this lower-dimensional space:\n",
    "   $$\n",
    "   X_{\\text{proj}} = X_c V_k\n",
    "   $$\n",
    "   Each column of \\( X_{\\text{proj}} \\) is how the data looks along one principal axis.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç TL;DR\n",
    "\n",
    "So in short:\n",
    "\n",
    "- We‚Äôre finding directions \\( v \\) that make the projection \\( Xv \\) have **maximum variance**.\n",
    "- That variance is measured by \\( v^\\top \\Sigma v \\).\n",
    "- With \\( \\|v\\| = 1 \\), maximizing it leads to the eigenvalue problem \\( \\Sigma v = \\lambda v \\).\n",
    "- The top eigenvalues correspond to directions with the **largest data spread**.\n",
    "- Those directions (eigenvectors) form the **principal components** of the data.\n"
   ],
   "id": "a0d57889dc638cc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If X isn‚Äôt centered, the covariance formula includes both spread and mean position.\n",
    "That means large values of the mean will dominate ‚Äî PCA would think there‚Äôs a ‚Äúbig variance‚Äù just because the data is far from zero, even if it‚Äôs tightly clustered."
   ],
   "id": "1980c6d68b4bc01e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üé≤ Probabilistic PCA\n",
    "\n",
    "### üß† Idea\n",
    "\n",
    "Probabilistic PCA (PPCA) views the data as being generated from a **Gaussian model** instead of just geometric projection.\n",
    "\n",
    "Each data point \\( x_i \\in \\mathbb{R}^d \\) is modeled as:\n",
    "\n",
    "$$\n",
    "x_i \\sim \\mathcal{N}(\\mu,\\; C)\n",
    "\\quad \\text{where} \\quad\n",
    "C = W W^\\top + \\sigma^2 I\n",
    "$$\n",
    "\n",
    "- \\( W \\): principal component loading matrix\n",
    "- \\( \\sigma^2 I \\): isotropic Gaussian noise\n",
    "- \\( \\mu \\): mean of the data\n",
    "\n",
    "So data is assumed to come from a **low-dimensional subspace** (spanned by \\( W \\))\n",
    "plus small Gaussian noise in all directions.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Log-likelihood form\n",
    "\n",
    "For each sample \\( x_i \\):\n",
    "\n",
    "$$\n",
    "\\log p(x_i) = -\\frac{1}{2}\n",
    "\\Big[\n",
    "(x_i-\\mu)^\\top C^{-1} (x_i-\\mu)\n",
    "+ \\log |C|\n",
    "+ d \\log (2\\pi)\n",
    "\\Big]\n",
    "$$\n",
    "\n",
    "This measures **how well** the sample fits into the PCA subspace.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è In scikit-learn\n",
    "\n",
    "`PCA.score(X)` computes this **average log-likelihood** (up to a constant).\n",
    "You can use it to compare models with different numbers of components in **cross-validation** ‚Äî\n",
    "higher score ‚Üí the PCA subspace explains data variance better. (we need a train test split here)\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Intuition\n",
    "\n",
    "- Regular PCA: *‚ÄúWhich directions explain variance best?‚Äù*\n",
    "- Probabilistic PCA: *‚ÄúGiven those directions, how likely is this data point under the Gaussian model of that subspace?‚Äù*\n",
    "- Each **row** (data point) is treated as one multivariate Gaussian sample ‚Äî\n",
    "  features are **jointly** modeled via the covariance \\( C \\),\n",
    "  not multiplied per column.\n"
   ],
   "id": "840b2d6414d6a1a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üîó Connection between PCA and SVD\n",
    "\n",
    "### üß© 1Ô∏è‚É£ Recall what PCA computes\n",
    "PCA works on the **covariance matrix** of centered data \\( X_c \\in \\mathbb{R}^{n \\times d} \\):\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n-1} X_c^\\top X_c\n",
    "$$\n",
    "\n",
    "and finds its eigenvectors and eigenvalues by solving:\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è 2Ô∏è‚É£ Now, decompose \\( X_c \\) using SVD\n",
    "\n",
    "We can factorize the same centered data as:\n",
    "\n",
    "$$\n",
    "X_c = U S V^\\top\n",
    "$$\n",
    "\n",
    "where\n",
    "- \\( U \\): left singular vectors (\\( n \\times n \\))\n",
    "- \\( S \\): diagonal matrix of singular values \\( s_1, s_2, \\dots \\)\n",
    "- \\( V \\): right singular vectors (\\( d \\times d \\))\n",
    "\n",
    "---\n",
    "\n",
    "### üîó 3Ô∏è‚É£ Substitute into the covariance\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n-1} X_c^\\top X_c\n",
    "       = \\frac{1}{n-1} (V S^\\top U^\\top)(U S V^\\top)\n",
    "       = \\frac{1}{n-1} V S^2 V^\\top\n",
    "$$\n",
    "\n",
    "From this, we see:\n",
    "- **Eigenvectors of \\( \\Sigma \\)** = columns of \\( V \\)\n",
    "- **Eigenvalues of \\( \\Sigma \\)** = \\( s_i^2 / (n-1) \\)\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 4Ô∏è‚É£ Interpretation\n",
    "\n",
    "So PCA can be done directly via SVD:\n",
    "- The **right singular vectors** \\( V \\) give the **principal directions** (PCA axes).\n",
    "- The **singular values** \\( s_i \\) encode how much variance each component explains.\n",
    "\n",
    "This is why modern implementations of PCA (e.g., in scikit-learn) actually use **SVD** under the hood ‚Äî\n",
    "it‚Äôs faster, numerically more stable, and avoids explicitly forming \\( X_c^\\top X_c \\).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è 5Ô∏è‚É£ Summary\n",
    "\n",
    "| Concept | PCA term | SVD term |\n",
    "|:--|:--|:--|\n",
    "| Directions (principal components) | Eigenvectors of \\( \\Sigma \\) | Right singular vectors \\( V \\) |\n",
    "| Variance explained | Eigenvalues \\( \\lambda_i \\) | \\( s_i^2 / (n-1) \\) |\n",
    "| Projected data | \\( X_c V_k \\) | \\( U_k S_k \\) |\n",
    "\n",
    "In short:\n",
    "> **PCA = SVD**, just seen through the lens of variance instead of matrix geometry.\n"
   ],
   "id": "a52f86ac20ec0355"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ‚öôÔ∏è Incremental PCA (IPCA)\n",
    "\n",
    "### üß© 1Ô∏è‚É£ Motivation\n",
    "\n",
    "Regular PCA requires building the full covariance matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n-1} X^\\top X\n",
    "$$\n",
    "\n",
    "which needs *all data in memory*.\n",
    "**Incremental PCA (IPCA)** instead updates PCA **batch by batch**, reusing the existing subspace instead of recomputing from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 2Ô∏è‚É£ Key insight\n",
    "\n",
    "PCA learns a set of **orthonormal directions** \\(V_{\\text{old}}\\) that capture variance.\n",
    "Because they are orthonormal:\n",
    "\n",
    "$$\n",
    "V_{\\text{old}}^{-1} = V_{\\text{old}}^\\top\n",
    "$$\n",
    "\n",
    "That means projecting into the PCA space and reconstructing back are *inverse operations*:\n",
    "\n",
    "- **Projection (compress)**: \\(Y = X V_{\\text{old}}\\)\n",
    "- **Reconstruction (expand)**: \\(\\hat{X} = Y V_{\\text{old}}^\\top = Y V_{\\text{old}}^{-1}\\)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è 3Ô∏è‚É£ Step-by-step update\n",
    "\n",
    "1Ô∏è‚É£ **Center the new batch**\n",
    "\n",
    "$$\n",
    "\\tilde{X}_{\\text{new}} = X_{\\text{new}} - \\mu_{\\text{old}}\n",
    "$$\n",
    "\n",
    "Update mean:\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{new}} = \\frac{n_{\\text{old}}\\mu_{\\text{old}} + m\\,\\bar{X}_{\\text{new}}}{n_{\\text{old}} + m}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "2Ô∏è‚É£ **Project into old PCA space**\n",
    "\n",
    "Compute how much of the new data is explained by the existing PCA subspace:\n",
    "\n",
    "$$\n",
    "Y = \\tilde{X}_{\\text{new}} V_{\\text{old}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "3Ô∏è‚É£ **Reconstruct explained part & find residuals**\n",
    "\n",
    "Reconstruction (what old PCA can explain):\n",
    "\n",
    "$$\n",
    "\\hat{X}_{\\text{new}} = Y V_{\\text{old}}^\\top = Y V_{\\text{old}}^{-1}\n",
    "$$\n",
    "\n",
    "Residual (what‚Äôs new):\n",
    "\n",
    "$$\n",
    "R = \\tilde{X}_{\\text{new}} - \\hat{X}_{\\text{new}}\n",
    "$$\n",
    "\n",
    "So \\(R\\) captures **new variance directions** that weren‚Äôt in the previous PCA basis.\n",
    "\n",
    "---\n",
    "\n",
    "4Ô∏è‚É£ **Combine old information and new variance**\n",
    "\n",
    "We now build a compact matrix \\(M\\) that merges:\n",
    "- The **old PCA subspace**, scaled by how strong each direction was (\\(S_{\\text{old}}\\))\n",
    "- The **new residual variance** \\(R\\)\n",
    "\n",
    "$$\n",
    "M =\n",
    "\\begin{bmatrix}\n",
    "S_{\\text{old}} V_{\\text{old}}^\\top \\\\\n",
    "R\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "üí° Here‚Äôs the key intuition:\n",
    "\n",
    "> We are **combining residuals and the old subspace (with scale)**\n",
    "> into a *new metric space*,\n",
    "> and we‚Äôll find **new eigenvectors** (principal directions) within that merged space.\n",
    "\n",
    "---\n",
    "\n",
    "5Ô∏è‚É£ **Find new principal directions**\n",
    "\n",
    "Run a **small SVD** on \\(M\\):\n",
    "\n",
    "$$\n",
    "M = U' S' V'^\\top\n",
    "$$\n",
    "\n",
    "and keep top \\(k\\) components:\n",
    "\n",
    "$$\n",
    "V_{\\text{new}} = V_{\\text{old}} V'_k, \\quad S_{\\text{new}} = S'_k\n",
    "$$\n",
    "\n",
    "This step re-orthogonalizes the space ‚Äî it finds the dominant eigenvectors of the *combined variance structure* (old + new).\n",
    "\n",
    "---\n",
    "\n",
    "### üß© 4Ô∏è‚É£ Intuition summary\n",
    "\n",
    "- \\(V_{\\text{old}}\\): old orthonormal PCA basis (so \\(V^\\top = V^{-1}\\))\n",
    "- \\(S_{\\text{old}}\\): how strong each old direction was\n",
    "- \\(R\\): new directions unexplained by old PCA\n",
    "\n",
    "We merge \\([S_{\\text{old}}V_{\\text{old}}^\\top; R]\\),\n",
    "then find **new eigenvectors** in that merged space ‚Äî\n",
    "those represent the updated global directions of maximum variance.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è 5Ô∏è‚É£ Big picture\n",
    "\n",
    "| Concept | Regular PCA | Incremental PCA |\n",
    "|:--|:--|:--|\n",
    "| Data | All at once | Mini-batches |\n",
    "| Computation | Full eigen/SVD | Small local SVD updates |\n",
    "| Basis | Fixed | Evolving |\n",
    "| Main operation | \\(X^\\top X\\) eigen | Combine \\([S_{\\text{old}}V_{\\text{old}}^\\top; R]\\), re-SVD |\n",
    "\n",
    "**In short:**\n",
    "Incremental PCA continuously merges *old knowledge (scaled basis)* with *new information (residual variance)*,\n",
    "then finds **new eigenvectors in that combined space**, keeping the PCA representation up to date.\n"
   ],
   "id": "969e065a126ac5ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ‚ö° Randomized PCA (RPCA)\n",
    "\n",
    "### üß© 1Ô∏è‚É£ Motivation\n",
    "Full PCA or SVD on large, high-dimensional data (\\(X \\in \\mathbb{R}^{n \\times d}\\)) is expensive.\n",
    "**Randomized PCA** provides a *fast approximation* by using **random projections** to find the same subspace with much less computation.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 2Ô∏è‚É£ Core idea\n",
    "We generate a random matrix:\n",
    "\n",
    "$$\n",
    "\\Omega \\in \\mathbb{R}^{d \\times (k+p)}, \\quad \\Omega_{ij} \\sim \\mathcal{N}(0,1)\n",
    "$$\n",
    "\n",
    "and project the data:\n",
    "\n",
    "$$\n",
    "Y = X \\Omega\n",
    "$$\n",
    "\n",
    "Each column of \\(Y\\) is a **random linear combination** of the features in \\(X\\).\n",
    "These random combinations act like ‚Äúrandom directions‚Äù in the feature space.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° 3Ô∏è‚É£ Why it works\n",
    "- High-variance directions in \\(X\\) dominate any random projection.\n",
    "- With high probability, the subspace spanned by \\(Y\\) captures nearly the same variance as the top \\(k\\) PCA components.\n",
    "- We then orthogonalize \\(Y\\):\n",
    "  $$\n",
    "  Q = \\text{orth}(Y)\n",
    "  $$\n",
    "  and perform a small SVD on \\(B = Q^\\top X\\).\n",
    "\n",
    "This gives the approximate decomposition:\n",
    "$$\n",
    "X \\approx Q (\\tilde{U} S V^\\top)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è 4Ô∏è‚É£ Intuition\n",
    "Randomized PCA ‚âà\n",
    "> ‚ÄúLook at the data from a few **random directions**,\n",
    "> orthogonalize what you see,\n",
    "> and perform SVD only in that smaller space.‚Äù\n",
    "\n",
    "It‚Äôs **really random** ‚Äî the weights in \\(\\Omega\\) are sampled from a random distribution ‚Äî\n",
    "but randomness in high dimensions almost always overlaps with the true top variance directions.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è 5Ô∏è‚É£ Summary\n",
    "\n",
    "| Step | What happens | Why |\n",
    "|:--|:--|:--|\n",
    "| \\( \\Omega \\) | Random weights | Choose random directions |\n",
    "| \\( Y = X\\Omega \\) | Project data | Compress while preserving structure |\n",
    "| \\( Q = \\text{orth}(Y) \\) | Orthonormal basis | Approximate main subspace |\n",
    "| SVD on \\(Q^\\top X\\) | Compute PCA in small space | Fast and accurate |\n",
    "\n",
    "So **Randomized PCA** finds nearly the same principal directions as full PCA ‚Äî\n",
    "but by using *random projections* to drastically cut computation time.\n"
   ],
   "id": "286677cb3c58c205"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üåø Sparse PCA (SparsePCA & MiniBatchSparsePCA)\n",
    "\n",
    "### üß© 1Ô∏è‚É£ Motivation\n",
    "Regular PCA components are **dense** ‚Äî every feature contributes to every component.\n",
    "That captures variance well but makes interpretation hard.\n",
    "\n",
    "**Sparse PCA** adds a sparsity constraint so that each component uses only a **small subset of features**,\n",
    "making them easier to interpret.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 2Ô∏è‚É£ Core idea\n",
    "Sparse PCA modifies the PCA objective by adding an ‚Ñì‚ÇÅ penalty (like Lasso):\n",
    "\n",
    "$$\n",
    "\\max_{V} \\text{Tr}(V^\\top \\Sigma V) - \\alpha \\|V\\|_1\n",
    "$$\n",
    "\n",
    "- The first term keeps variance large (same as PCA).\n",
    "- The ‚Ñì‚ÇÅ term forces many small coefficients in \\(V\\) to **zero** ‚Üí sparse components.\n",
    "\n",
    "So each component depends only on a few important features.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è 3Ô∏è‚É£ Implementation\n",
    "It can also be seen as minimizing the reconstruction error with sparsity:\n",
    "\n",
    "$$\n",
    "\\min_{U,V} \\|X - UV^\\top\\|_F^2 + \\alpha \\|V\\|_1\n",
    "$$\n",
    "\n",
    "- \\(V\\): sparse loading vectors (principal directions)\n",
    "- \\(U\\): projections of samples onto those directions\n",
    "\n",
    "Two variants:\n",
    "- **SparsePCA** ‚Üí full batch (coordinate descent)\n",
    "- **MiniBatchSparsePCA** ‚Üí faster version using random mini-batches\n",
    "\n",
    "---\n",
    "\n",
    "### üí° 4Ô∏è‚É£ Summary\n",
    "\n",
    "| Method | Feature usage | Interpretation | Use case |\n",
    "|:--|:--|:--|:--|\n",
    "| PCA | All features | Hard | Compression, visualization |\n",
    "| Sparse PCA | Few features | Easy | Feature selection, interpretability |\n",
    "| MiniBatchSparsePCA | Few features | Easy + Fast | Large datasets |\n",
    "\n",
    "> Sparse PCA = PCA + Lasso\n",
    "> Adds sparsity for interpretability, trading off a bit of variance for clarity.\n"
   ],
   "id": "bcaf778d9fb99768"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üåà Kernel PCA ‚Äî Training & Prediction (Algebra Only)\n",
    "\n",
    "## 1) Prediction (projection in terms of kernels)\n",
    "\n",
    "Goal: express the projection of a point onto a principal direction **using only kernels**.\n",
    "\n",
    "**Start (feature space projection):**\n",
    "$$\n",
    "z_i \\;=\\; v^\\top \\phi(x_i)\n",
    "$$\n",
    "\n",
    "**Represent the principal axis as a combo of training samples:**\n",
    "$$\n",
    "v \\;=\\; \\sum_{j=1}^n \\alpha_j \\,\\phi(x_j)\n",
    "$$\n",
    "\n",
    "**Substitute and linearity:**\n",
    "$$\n",
    "z_i \\;=\\; \\Big(\\sum_{j=1}^n \\alpha_j \\phi(x_j)\\Big)^\\top \\phi(x_i)\n",
    "\\;=\\; \\sum_{j=1}^n \\alpha_j \\,\\langle \\phi(x_j), \\phi(x_i)\\rangle\n",
    "$$\n",
    "\n",
    "**Replace by kernel values:**\n",
    "$$\n",
    "\\boxed{\\,z_i \\;=\\; \\sum_{j=1}^n \\alpha_j\\, K(x_j, x_i)\\,}\n",
    "$$\n",
    "\n",
    "\n",
    "## 2) Training (derivation to \\(K\\alpha = n\\lambda \\alpha\\))\n",
    "\n",
    "We want eigenpairs of the **feature-space covariance**:\n",
    "$$\n",
    "C_\\phi \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\phi(x_i)\\phi(x_i)^\\top,\n",
    "\\qquad C_\\phi v \\;=\\; \\lambda v.\n",
    "$$\n",
    "\n",
    "**(a) Expand \\(C_\\phi v\\) explicitly:**\n",
    "$$\n",
    "\\Big(\\tfrac{1}{n}\\sum_{i=1}^n \\phi(x_i)\\phi(x_i)^\\top\\Big) v\n",
    "\\;=\\; \\tfrac{1}{n}\\sum_{i=1}^n \\phi(x_i)\\big(\\phi(x_i)^\\top v\\big)\n",
    "\\;=\\; \\lambda v.\n",
    "$$\n",
    "\n",
    "**(b) Express \\(v\\) in the span of training samples:**\n",
    "$$\n",
    "v \\;=\\; \\sum_{j=1}^n \\alpha_j\\, \\phi(x_j).\n",
    "$$\n",
    "\n",
    "**(c) Plug into (a); use inner products:**\n",
    "$$\n",
    "\\tfrac{1}{n}\\sum_{i=1}^n \\phi(x_i)\\Big(\\phi(x_i)^\\top \\sum_{j=1}^n \\alpha_j \\phi(x_j)\\Big)\n",
    "\\;=\\; \\lambda \\sum_{j=1}^n \\alpha_j \\phi(x_j).\n",
    "$$\n",
    "\n",
    "**(d) Pull the sum out and define the kernel:**\n",
    "$$\n",
    "\\tfrac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_j\\, \\phi(x_i)\\,\\langle \\phi(x_i),\\phi(x_j)\\rangle\n",
    "\\;=\\; \\lambda \\sum_{j=1}^n \\alpha_j \\phi(x_j).\n",
    "$$\n",
    "\n",
    "Let \\(K_{ij} = \\langle \\phi(x_i),\\phi(x_j)\\rangle\\) and note both sides are linear combos of \\(\\{\\phi(x_\\ell)\\}\\).\n",
    "Equate coefficients (collect terms along the basis \\(\\phi(x_\\ell)\\)):\n",
    "\n",
    "For the coefficient of \\(\\phi(x_\\ell)\\) on the LHS:\n",
    "$$\n",
    "\\tfrac{1}{n} \\sum_{j=1}^n \\alpha_j\\, K_{\\ell j}.\n",
    "$$\n",
    "\n",
    "Set LHS = RHS coefficients for each \\(\\ell\\):\n",
    "$$\n",
    "\\tfrac{1}{n} \\sum_{j=1}^n K_{\\ell j} \\alpha_j \\;=\\; \\lambda \\alpha_\\ell\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "\\frac{1}{n}(K\\alpha)_\\ell \\;=\\; \\lambda \\alpha_\\ell.\n",
    "$$\n",
    "\n",
    "**Vector form:**\n",
    "$$\n",
    "\\boxed{\\,K \\alpha \\;=\\; n \\lambda \\alpha\\,}\n",
    "$$\n",
    "\n",
    "(After solving, normalize eigenvectors appropriately; in practice one uses the **centered** kernel \\(K_c\\) before this step.)\n"
   ],
   "id": "24509893d2f517b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "580977e33a2dedba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
