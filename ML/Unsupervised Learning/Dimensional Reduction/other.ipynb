{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "x# 2.5.4 Dictionary Learning — Short Notes\n",
    "\n",
    "## Idea\n",
    "Represent each sample using **few** learned basis vectors (atoms). For data matrix \\(X \\in \\mathbb{R}^{m \\times n}\\), learn dictionary \\(D \\in \\mathbb{R}^{m \\times k}\\) and sparse codes \\(A \\in \\mathbb{R}^{k \\times n}\\) so that:\n",
    "$$\n",
    "x_i \\approx D\\alpha_i, \\quad \\text{with } \\alpha_i \\text{ sparse.}\n",
    "$$\n",
    "\n",
    "## Objective\n",
    "Minimize reconstruction error + sparsity:\n",
    "$$\n",
    "\\min_{D, A}\\ \\tfrac{1}{2}\\|X - DA\\|_F^2 + \\lambda \\|A\\|_1\n",
    "\\quad \\text{s.t. } \\|d_j\\|_2 = 1 \\ \\forall j.\n",
    "$$\n",
    "\n",
    "- \\( \\|A\\|_1 = \\sum_{i,j}|A_{ij}| \\) promotes sparsity.\n",
    "- Unit-norm atoms avoid arbitrary scaling of \\(D\\) vs \\(A\\).\n",
    "\n",
    "## Training (Alternating Minimization)\n",
    "\n",
    "### 1) Sparse Coding (fix \\(D\\), solve \\(A\\))\n",
    "For each column \\(x_i\\):\n",
    "$$\n",
    "\\alpha_i \\leftarrow \\arg\\min_\\alpha \\ \\tfrac{1}{2}\\|x_i - D\\alpha\\|_2^2 + \\lambda \\|\\alpha\\|_1.\n",
    "$$\n",
    "Solvers: coordinate descent (Lasso), ISTA/FISTA (proximal gradient), OMP (greedy, \\(\\ell_0\\)-style).\n",
    "\n",
    "**What this step does:** finds a **sparse** set of active atoms that reconstruct \\(x_i\\) well.\n",
    "\n",
    "### 2) Dictionary Update (fix \\(A\\), solve \\(D\\))\n",
    "Solve:\n",
    "$$\n",
    "D \\leftarrow \\arg\\min_D \\ \\|X - DA\\|_F^2 \\ \\ \\text{s.t. }\\|d_j\\|_2=1.\n",
    "$$\n",
    "\n",
    "Atom-wise update using residuals. Let \\(a_j^\\top\\) be row \\(j\\) of \\(A\\), and\n",
    "$$\n",
    "R_j = X - \\sum_{\\ell\\neq j} d_\\ell a_\\ell^\\top.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "d_j \\leftarrow \\frac{R_j a_j}{\\|R_j a_j\\|_2}, \\quad\n",
    "\\text{(normalize to unit norm)}.\n",
    "$$\n",
    "\n",
    "**What this step does:** refines each atom to best explain the **current** set of samples that use it.\n",
    "\n",
    "### 3) Iterate\n",
    "Alternate 1) and 2) until objective stabilizes.\n",
    "\n",
    "## Geometry & Comparison (1-liner)\n",
    "- PCA: orthogonal bases, **dense** coefficients.\n",
    "- Dictionary Learning: non-orthogonal bases, **sparse** coefficients → parts-based, selective reconstruction.\n",
    "\n",
    "## Practical Tips\n",
    "- Choose \\(k\\) (atoms) and \\(\\lambda\\) (sparsity) via validation.\n",
    "- Mini-batch variants scale to large \\(n\\).\n",
    "- Initialize \\(D\\) with normalized random columns or k-means centroids.\n",
    "\n"
   ],
   "id": "1c41e13edb984ee9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.5.5 Factor Analysis — Understanding Mean and Covariance\n",
    "\n",
    "## Model\n",
    "Each sample \\( x_i \\in \\mathbb{R}^d \\) is generated as:\n",
    "$$\n",
    "x_i = W z_i + \\mu + \\epsilon_i\n",
    "$$\n",
    "\n",
    "- \\( W \\in \\mathbb{R}^{d \\times k} \\): factor loading matrix\n",
    "- \\( z_i \\sim \\mathcal{N}(0, I_k) \\): latent factors (shared causes)\n",
    "- \\( \\epsilon_i \\sim \\mathcal{N}(0, \\Psi) \\): feature-specific noise (diagonal covariance)\n",
    "- \\( \\mu \\in \\mathbb{R}^d \\): mean vector\n",
    "\n",
    "The marginal distribution is:\n",
    "$$\n",
    "p(x_i) = \\mathcal{N}(x_i \\mid \\mu,\\, W W^\\top + \\Psi)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Mean vs Covariance\n",
    "\n",
    "| Term | Controls | Meaning |\n",
    "|------|-----------|----------|\n",
    "| **\\(\\mu\\)** | Location | Center of the Gaussian cloud (one mean per feature) |\n",
    "| **\\(W W^\\top\\)** | Shared structure | Correlation between features via common latent factors |\n",
    "| **\\(\\Psi\\)** | Independent noise | Feature-specific variance (no correlation) |\n",
    "\n",
    "So the **mean vector** sets the baseline (average value for each feature),\n",
    "while \\( W W^\\top + \\Psi \\) defines the **shape** and **orientation** of variations *around that mean*.\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Estimating the Mean\n",
    "\n",
    "\\(\\mu\\) is not learned through \\(W\\); it’s computed directly as the data’s empirical mean:\n",
    "$$\n",
    "\\mu = \\frac{1}{n} \\sum_i x_i\n",
    "$$\n",
    "It tells us where each feature \"starts\" — e.g., one variable might average 100 while another averages 5.\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Covariance Structure\n",
    "\n",
    "Taking the covariance of the model:\n",
    "$$\n",
    "\\text{Cov}(x_i) = \\text{Cov}(W z_i + \\epsilon_i)\n",
    "= W\\,\\text{Cov}(z_i)\\,W^\\top + \\text{Cov}(\\epsilon_i)\n",
    "= W W^\\top + \\Psi\n",
    "$$\n",
    "\n",
    "- \\(W W^\\top\\): correlations due to shared latent factors\n",
    "- \\(\\Psi\\): diagonal matrix of independent noise\n",
    "\n",
    "This decomposition says:\n",
    "> \"Total variability = shared structure + feature-specific noise.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Geometric Intuition\n",
    "\n",
    "A multivariate Gaussian is fully described by:\n",
    "$$\n",
    "x \\sim \\mathcal{N}(\\mu,\\, \\Sigma)\n",
    "$$\n",
    "- **\\(\\mu\\)** shifts the center of the ellipse (mean of the cloud)\n",
    "- **\\(\\Sigma = W W^\\top + \\Psi\\)** determines the shape and tilt (covariance structure)\n",
    "\n",
    "So in 2D:\n",
    "- If \\(\\mu = (5, 10)\\), the ellipse is centered at (5, 10).\n",
    "- \\(W W^\\top\\) controls how features move together (tilt of the ellipse).\n",
    "- \\(\\Psi\\) controls individual spread per axis (roundness).\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- Mean and covariance are separate: \\(\\mu\\) sets the *location*, \\(W, \\Psi\\) set the *shape*.\n",
    "- Two features can start large (high means) yet still move together (high covariance).\n",
    "- The covariance term \\(W W^\\top\\) models *how one feature’s change influences another* through shared latent factors.\n"
   ],
   "id": "2d0d097f0467ebe3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ICA\n",
    "https://danieljyc.github.io/2014/06/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A015-3--%E7%8B%AC%E7%AB%8B%E6%88%90%E5%88%86%E5%88%86%E6%9E%90ica%EF%BC%88independent-component-analysis%EF%BC%89/\n",
    "# 2.5.6 Independent Component Analysis (ICA)\n",
    "\n",
    "## Idea\n",
    "ICA aims to recover **independent source signals** from observed mixtures.\n",
    "Given whitened data \\( P \\in \\mathbb{R}^{n \\times k} \\) (from PCA),\n",
    "ICA finds a transformation \\( R \\) such that:\n",
    "$$\n",
    "S = P R^\\top\n",
    "$$\n",
    "and the new components \\( S = [s_1, s_2, \\dots, s_k] \\) are **statistically independent**, i.e.\n",
    "$$\n",
    "p(s_1, s_2, ..., s_k) = \\prod_i p(s_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## PCA vs ICA\n",
    "\n",
    "- **PCA** ensures components are **uncorrelated** (Cov = I).\n",
    "  This means:\n",
    "  $$\n",
    "  E[p_i p_j] = 0 \\quad \\text{for } i \\ne j\n",
    "  $$\n",
    "  → their *dot product is 0*.\n",
    "\n",
    "- But **uncorrelated ≠ independent**.\n",
    "  Orthogonality (dot product = 0) only removes **linear** relationships —\n",
    "  not nonlinear dependencies.\n",
    "\n",
    "### Example\n",
    "Let:\n",
    "$$\n",
    "x \\sim \\text{Uniform}(-1,1), \\quad y = x^2\n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "E[xy] = 0 \\Rightarrow \\text{Cov}(x,y) = 0\n",
    "$$\n",
    "They’re **uncorrelated**, but clearly **not independent** — knowing \\(x\\) gives \\(y\\).\n",
    "\n",
    "> **So:**\n",
    "> Dot product \\(=0\\) ⟹ uncorrelated\n",
    "> but does **not** imply\n",
    "> \\( p(\\text{vector}_1, \\text{vector}_2) = p(\\text{vector}_1)p(\\text{vector}_2) \\)\n",
    "\n",
    "---\n",
    "\n",
    "## What ICA Does\n",
    "\n",
    "After PCA whitening (Cov = I), the data cloud is a **sphere** in feature space —\n",
    "no correlation, equal variance in all directions.\n",
    "\n",
    "ICA now searches for a **rotation** \\( R \\) (orthogonal matrix) such that\n",
    "the new axes (independent components) are **statistically independent**:\n",
    "$$\n",
    "S = P R^\\top\n",
    "$$\n",
    "\n",
    "Since rotation doesn’t change variance, ICA can freely “spin” this sphere until\n",
    "each axis (component) becomes maximally **non-Gaussian** —\n",
    "a proxy for statistical independence.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Objective\n",
    "\n",
    "ICA finds \\(R\\) (or equivalently \\(W = R^\\top\\)) that maximizes **non-Gaussianity**\n",
    "using contrast functions like kurtosis or negentropy.\n",
    "\n",
    "Typical FastICA update:\n",
    "$$\n",
    "w_i \\leftarrow E[P\\,g(w_i^\\top P)] - E[g'(w_i^\\top P)]w_i\n",
    "$$\n",
    "then normalize and orthogonalize \\(w_i\\).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Step | What happens | Purpose |\n",
    "|------|---------------|----------|\n",
    "| PCA | Decorrelates and whitens data (Cov = I) | Remove 2nd-order correlation |\n",
    "| ICA | Rotates whitened data | Remove higher-order dependence |\n",
    "| Goal | \\(p(s_1, s_2, ...)=\\prod_i p(s_i)\\) | Achieve independence |\n",
    "\n",
    "**PCA:** makes vectors orthogonal (uncorrelated)\n",
    "**ICA:** finds true independent sources\n",
    "\n",
    "> ICA solves what PCA can’t —\n",
    "> even if the dot product between vectors is 0,\n",
    "> ICA ensures their **joint probability truly factorizes.**\n"
   ],
   "id": "ef3a852a7ee90c74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.5.7 Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "## Concept\n",
    "NMF factorizes a **non-negative** matrix \\( X \\) into two smaller non-negative matrices:\n",
    "$$\n",
    "X \\approx W H\n",
    "$$\n",
    "where:\n",
    "- \\( W \\): latent basis (parts)\n",
    "- \\( H \\): activations (weights)\n",
    "- all entries ≥ 0\n",
    "\n",
    "This gives a **parts-based**, additive representation — features combine only by addition, not subtraction.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Non-negative\n",
    "- Prevents cancellation of effects (no “negative part” to offset a positive one).\n",
    "- Encourages **sparse**, interpretable components (e.g., eyes + nose + mouth = face).\n",
    "\n",
    "---\n",
    "\n",
    "## Training Objective\n",
    "Minimize reconstruction error:\n",
    "$$\n",
    "\\min_{W,H \\ge 0} \\|X - WH\\|_F^2\n",
    "$$\n",
    "using **multiplicative updates** that maintain non-negativity:\n",
    "$$\n",
    "H \\leftarrow H \\odot \\frac{W^\\top X}{W^\\top W H}, \\quad\n",
    "W \\leftarrow W \\odot \\frac{X H^\\top}{W H H^\\top}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "- PCA: combines features through positive and negative weights (global patterns)\n",
    "- **NMF:** only positive combinations (local, additive “parts”)\n",
    "\n",
    "| PCA | NMF |\n",
    "|------|------|\n",
    "| Components can cancel each other | Components add up |\n",
    "| Orthogonal basis | Non-orthogonal basis |\n",
    "| Focus: variance | Focus: additive interpretability |\n",
    "\n",
    "---\n",
    "\n",
    "# 2.5.8 Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "## Concept\n",
    "LDA is a **probabilistic topic model** for text.\n",
    "It assumes:\n",
    "- Each **document** is a **mixture of topics**.\n",
    "- Each **topic** is a **distribution over words**.\n",
    "\n",
    "Example:\n",
    "> “Cats play with yarn” → 70% *pets*, 30% *toys*.\n",
    "\n",
    "---\n",
    "\n",
    "## Generative Process\n",
    "For each topic \\(k\\):\n",
    "- Draw word distribution: \\( \\phi_k \\sim \\text{Dirichlet}(\\beta) \\)\n",
    "\n",
    "For each document \\(d\\):\n",
    "1. Draw topic mixture: \\( \\theta_d \\sim \\text{Dirichlet}(\\alpha) \\)\n",
    "2. For each word:\n",
    "   - Choose topic \\( z_{dn} \\sim \\text{Multinomial}(\\theta_d) \\)\n",
    "   - Choose word \\( w_{dn} \\sim \\text{Multinomial}(\\phi_{z_{dn}}) \\)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning\n",
    "Given only observed words, LDA infers:\n",
    "- \\( \\phi_k \\): topic-word distributions\n",
    "- \\( \\theta_d \\): topic proportions per document\n",
    "- \\( z_{dn} \\): topic assignment for each word\n",
    "\n",
    "Typically via:\n",
    "- **Variational Bayes** (optimization)\n",
    "- **Gibbs Sampling** (sampling-based posterior estimation)\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "- **NMF**: additive, deterministic parts of data.\n",
    "- **LDA**: probabilistic, interpretable topics in text.\n",
    "\n",
    "| NMF | LDA |\n",
    "|-----|-----|\n",
    "| Linear factorization \\(X \\approx WH\\) | Probabilistic model \\(p(w,z,\\theta,\\phi)\\) |\n",
    "| Non-negative constraints | Dirichlet priors |\n",
    "| Deterministic optimization | Bayesian inference |\n",
    "| Often used on term-frequency matrix | Used on bag-of-words corpus |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key takeaway:**\n",
    "- **NMF** → additive parts in numeric data.\n",
    "- **LDA** → probabilistic topics in text data.\n",
    "Both uncover latent structure — NMF through linear decomposition, LDA through probabilistic generative modeling.\n"
   ],
   "id": "d39f4df5a0f06e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
