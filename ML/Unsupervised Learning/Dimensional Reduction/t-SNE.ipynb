{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# t-SNE — Notebook-style Notes\n",
    "\n",
    "> **Goal:** compact but complete reference you can paste into a single Jupyter Markdown cell. Includes: quick stats refresher (z/t/p), t-distribution PDF, t-SNE formulation, why KL, why the *t* tail, optimization/gradient, hyperparams, tricks, pitfalls, and interview notes.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Quick refresher — z-test, t-test, p-value\n",
    "\n",
    "- **Z-test (known σ or large n):**\n",
    "\n",
    "  $$\n",
    "  z=\\frac{\\bar{x}-\\mu_0}{\\sigma/\\sqrt{n}},\\quad z\\sim \\mathcal N(0,1)\\text{ under }H_0\n",
    "  $$\n",
    "\n",
    "- **t-test (unknown σ, small/medium n):**\n",
    "\n",
    "  $$\n",
    "  t=\\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}},\\quad t\\sim t_\\nu,\\ \\nu=n-1\n",
    "  $$\n",
    "\n",
    "- **p-value:** probability of observing a statistic at least as extreme as the observed one **if** $H_0$ were true.\n",
    "\n",
    "### Student’s t-distribution PDF\n",
    "\n",
    "$$\n",
    "f(t)=\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}\n",
    "{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\n",
    "\\left(1+\\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\n",
    "$$\n",
    "\n",
    "- Fatter tails than normal; $t_\\nu \\to \\mathcal N(0,1)$ as $\\nu\\to\\infty$.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) What is t-SNE?\n",
    "\n",
    "**t-Distributed Stochastic Neighbor Embedding** — a **nonlinear** dimensionality-reduction method primarily for **visualization** (2D/3D). It preserves **local neighborhood structure** of high-dimensional data.\n",
    "\n",
    "- “SNE”: preserve neighbor **probabilities** rather than raw distances.\n",
    "- “t-Distributed”: use a **Student’s t** distribution in low-D to mitigate the **crowding problem**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) High-D similarities $P=\\{p_{ij}\\}$\n",
    "\n",
    "For data $x_1,\\dots,x_n\\in\\mathbb R^D$:\n",
    "\n",
    "1. Conditional similarities with a **Gaussian kernel**:\n",
    "\n",
    "   $$\n",
    "   p_{j|i}=\\frac{\\exp\\!\\left(-\\frac{\\lVert x_i-x_j\\rVert^2}{2\\sigma_i^2}\\right)}\n",
    "   {\\sum_{k\\neq i}\\exp\\!\\left(-\\frac{\\lVert x_i-x_k\\rVert^2}{2\\sigma_i^2}\\right)}\n",
    "   $$\n",
    "\n",
    "   - $\\sigma_i$ chosen via **perplexity** to set local neighborhood size.\n",
    "\n",
    "2. Symmetrize:\n",
    "\n",
    "   $$\n",
    "   p_{ij}=\\frac{p_{j|i}+p_{i|j}}{2n},\\qquad p_{ii}=0,\\ \\sum_{i\\ne j}p_{ij}=1\n",
    "   $$\n",
    "\n",
    "> Intuition: $p_{ij}$ encodes “who is near whom” in high-D, focusing on **local** neighborhoods.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Low-D similarities $Q=\\{q_{ij}\\}$\n",
    "\n",
    "For embedding points $y_i\\in\\mathbb R^{d}$ (usually $d=2$):\n",
    "\n",
    "$$\n",
    "q_{ij}=\\frac{(1+\\lVert y_i-y_j\\rVert^2)^{-1}}\n",
    "{\\sum_{k\\neq \\ell}(1+\\lVert y_k-y_\\ell\\rVert^2)^{-1}},\\qquad q_{ii}=0\n",
    "$$\n",
    "\n",
    "- This is a **Student’s t** kernel with 1 d.o.f. (Cauchy-like): **heavy tails**.\n",
    "- Both $P$ and $Q$ use **Euclidean distances**; the difference is the **decay** (Gaussian vs t-tail).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Objective (what is optimized)\n",
    "\n",
    "Minimize **KL divergence** from $P$ to $Q$:\n",
    "\n",
    "$$\n",
    "C(Y)=KL(P\\parallel Q)=\\sum_{i\\ne j}p_{ij}\\log\\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "- **Why KL (and not MSE on distances)?** It compares **distributions of similarities**, preserving **relative** neighbor structure.\n",
    "- **Asymmetry matters:** Large $p_{ij}$ (true neighbors) penalize heavily if $q_{ij}$ is too small ⇒ **prioritizes local structure**.\n",
    "\n",
    "> **Smaller KL is better.** $KL=0$ only if $P\\equiv Q$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) The gradient (how points move)\n",
    "\n",
    "The variables are **only** the low-D coordinates $Y=\\{y_i\\}$. The gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial y_i}\n",
    "=4\\sum_{j}(p_{ij}-q_{ij})\\,(y_i-y_j)\\,(1+\\lVert y_i-y_j\\rVert^2)^{-1}\n",
    "$$\n",
    "\n",
    "- If $p_{ij}>q_{ij}$ → **attraction** (too far → pull together).\n",
    "- If $p_{ij}<q_{ij}$ → **repulsion** (too close → push apart).\n",
    "- $(1+r^2)^{-1}$ gives **long-range but decaying** influence (from t-tail).\n",
    "\n",
    "**Update (batch GD with momentum):**\n",
    "\n",
    "$$\n",
    "y_i^{(t+1)}=y_i^{(t)}-\\eta\\frac{\\partial C}{\\partial y_i}+\\alpha(t)\\left[y_i^{(t)}-y_i^{(t-1)}\\right]\n",
    "$$\n",
    "\n",
    "> All points update **together each iteration** (full batch). No per-point SGD.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Why the **t** tail solves crowding (better visualization)\n",
    "\n",
    "- In high-D, many points are moderately far yet not neighbors → impossible to fit all separations in 2D.\n",
    "- A **Gaussian** low-D kernel makes far pairs $q_{ij}\\approx0$ ⇒ no repulsion ⇒ everything crowds at center.\n",
    "- The **t-distribution** decays slowly: $q_{ij}\\propto(1+r^2)^{-1}$ ⇒ even far pairs keep mild repulsion ⇒ clusters spread and remain separable.\n",
    "- With **KL’s asymmetry** (local preservation), t-SNE yields compact, well-separated clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Practicalities: hyperparameters & tricks\n",
    "\n",
    "- **perplexity:** 5–50; controls neighborhood size (larger → broader).\n",
    "- **learning_rate:** small → clumping; large → scatter. Typical 200–1000 (~ n/12 rule).\n",
    "- **n_iter:** 500–1000+.\n",
    "- **metric:** use cosine for embeddings (or L2-normalize then Euclidean ≈ cosine).\n",
    "- **init:** `'pca'` (stable) vs `'random'` (stochastic).\n",
    "- **early_exaggeration:** multiply $p_{ij}$ × 4–12 for first ~250 iters → stronger early attraction.\n",
    "- **Speed-ups:** Barnes–Hut ($O(n\\log n)$) or FIt-SNE/FFT for large n.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) What t-SNE is **not**\n",
    "\n",
    "- **No mapping** $f(x)=Wx$ → it learns only coordinates $Y$ → **visualization only**.\n",
    "- **Cannot project new points** (unless parametric t-SNE or approximation).\n",
    "- **Global distances/angles meaningless**; only **local structure** matters.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Sanity checks & troubleshooting\n",
    "\n",
    "- Run multiple seeds → clusters should broadly match.\n",
    "- **Perplexity sweep:** small → fine clusters; large → smoother.\n",
    "- **Learning rate:** collapse → increase slightly; oscillation → decrease.\n",
    "- **Normalize embeddings**; use cosine metric.\n",
    "- **Don’t over-interpret:** cluster size/gaps ≠ quantitative distance.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Interview-ready contrasts & sound bites\n",
    "\n",
    "- **PCA vs t-SNE:** PCA = linear/global; t-SNE = nonlinear/local.\n",
    "- **Why KL(P‖Q):** asymmetric → protects close neighbors.\n",
    "- **Why t-distribution:** heavy tails → long-range repulsion → less crowding.\n",
    "- **No transform W:** pairwise objective ⇒ non-parametric.\n",
    "- **UMAP vs t-SNE:** UMAP faster, supports `.transform()`, preserves some global geometry.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) Force-based intuition\n",
    "\n",
    "Each pair exerts forces:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial y_i}\n",
    "=4\\sum_j \\underbrace{(p_{ij}-q_{ij})}_{\\text{attract vs repel}}\n",
    "\\underbrace{(y_i-y_j)}_{\\text{direction}}\n",
    "\\underbrace{(1+\\lVert y_i-y_j\\rVert^2)^{-1}}_{\\text{decay (t-tail)}}\n",
    "$$\n",
    "\n",
    "- Attraction ∝ $p_{ij}$ (true neighbors).\n",
    "- Repulsion ∝ $q_{ij}$ (low-D similarity).\n",
    "- Equilibrium when forces balance → low KL.\n",
    "\n",
    "---\n",
    "\n",
    "## 12) Wrap-up TL;DR\n",
    "\n",
    "- **What:** map high-D data → 2D/3D by matching neighbor probabilities $P$ and $Q$.\n",
    "- **Loss:** $KL(P\\parallel Q)$ (minimized).\n",
    "- **Why it works:** KL asymmetry + t-tails → preserve local clusters, avoid crowding.\n",
    "- **Use:** visualization & exploration only.\n",
    "- **Mind:** hyperparams (perplexity, LR) and interpretability limits.\n"
   ],
   "id": "d3fb891703f1e20d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T07:06:01.260357Z",
     "start_time": "2025-10-14T07:06:01.258893Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "983243fe5020f28d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "11d260a5a885a42d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
