{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß© Rand Index (RI) and Adjusted Rand Index (ARI)\n",
    "\n",
    "## 1. Intuition\n",
    "\n",
    "Both RI and ARI measure how similar two partitions of the same dataset are:\n",
    "\n",
    "- **True labels**: ground truth classes\n",
    "- **Predicted labels**: clustering output\n",
    "\n",
    "They ignore the actual label names, only caring whether pairs of points are grouped together or not.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Pairwise relationships\n",
    "\n",
    "For \\( n \\) samples, there are \\( \\binom{n}{2} \\) possible pairs.\n",
    "\n",
    "For each pair \\((i, j)\\), there are four cases:\n",
    "\n",
    "| Type | True | Predicted | Count |\n",
    "|------|-------|------------|-------|\n",
    "| SS | same | same | \\( a \\) |\n",
    "| DD | diff | diff | \\( b \\) |\n",
    "| SD | same | diff | \\( c \\) |\n",
    "| DS | diff | same | \\( d \\) |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Rand Index (Unadjusted)\n",
    "\n",
    "The Rand Index is simply the proportion of pairwise agreements:\n",
    "\n",
    "$$\n",
    "RI = \\frac{a + b}{a + b + c + d} = \\frac{a + b}{\\binom{n}{2}}\n",
    "$$\n",
    "\n",
    "It measures **Observed / Max Possible**, since the maximum number of agreeing pairs is \\(\\binom{n}{2}\\).\n",
    "\n",
    "- Range: \\([0, 1]\\)\n",
    "- 1 ‚Üí perfect agreement\n",
    "- 0 ‚Üí total disagreement\n",
    "\n",
    "However, random labelings can yield *nonzero* RI due to chance.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Contingency Table\n",
    "\n",
    "Let \\( n_{ij} \\) be the number of samples in both true class \\( i \\) and predicted cluster \\( j \\):\n",
    "\n",
    "| True / Pred | C‚ÇÅ | C‚ÇÇ | ... | Row Sum |\n",
    "|--------------|----|----|------|----------|\n",
    "| T‚ÇÅ | \\( n_{11} \\) | \\( n_{12} \\) | ... | \\( a_1 \\) |\n",
    "| T‚ÇÇ | \\( n_{21} \\) | \\( n_{22} \\) | ... | \\( a_2 \\) |\n",
    "| ... | ... | ... | ... | ... |\n",
    "| Column Sum | \\( b_1 \\) | \\( b_2 \\) | ... | \\( n \\) |\n",
    "\n",
    "Here:\n",
    "- \\( a_i = \\sum_j n_{ij} \\) (true class sizes)\n",
    "- \\( b_j = \\sum_i n_{ij} \\) (predicted cluster sizes)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Observed Index\n",
    "\n",
    "Observed number of ‚Äúsame‚Äìsame‚Äù pairs in both partitions:\n",
    "\n",
    "$$\n",
    "\\text{Observed Index} = \\sum_{ij} \\binom{n_{ij}}{2}\n",
    "$$\n",
    "\n",
    "This counts how many sample pairs are in the same true class **and** the same predicted cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Expected Index\n",
    "\n",
    "We want to correct for **chance agreement**.\n",
    "Even random clusterings will have some overlap between within-class and within-cluster pairs.\n",
    "\n",
    "Probability that two samples are in the same true class:\n",
    "\n",
    "$$\n",
    "P_{\\text{true}} = \\frac{\\sum_i \\binom{a_i}{2}}{\\binom{n}{2}}\n",
    "$$\n",
    "\n",
    "Probability that two samples are in the same predicted cluster:\n",
    "\n",
    "$$\n",
    "P_{\\text{pred}} = \\frac{\\sum_j \\binom{b_j}{2}}{\\binom{n}{2}}\n",
    "$$\n",
    "\n",
    "Assuming independence, the expected probability they‚Äôre same‚Äìsame in both:\n",
    "\n",
    "$$\n",
    "P_{\\text{true}} \\times P_{\\text{pred}}\n",
    "$$\n",
    "\n",
    "Hence, expected number of such pairs:\n",
    "\n",
    "$$\n",
    "E[\\text{Index}] =\n",
    "\\binom{n}{2} \\, P_{\\text{true}} \\, P_{\\text{pred}} =\n",
    "\\frac{\n",
    "  \\left( \\sum_i \\binom{a_i}{2} \\right)\n",
    "  \\left( \\sum_j \\binom{b_j}{2} \\right)\n",
    "}{ \\binom{n}{2} }\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Max Possible Index\n",
    "\n",
    "The largest possible number of same‚Äìsame pairs happens when both partitions are perfectly aligned.\n",
    "Each partition defines a set of within-cluster pairs:\n",
    "\n",
    "- True: \\( |S_{\\text{true}}| = \\sum_i \\binom{a_i}{2} \\)\n",
    "- Pred: \\( |S_{\\text{pred}}| = \\sum_j \\binom{b_j}{2} \\)\n",
    "\n",
    "Their maximum intersection (if perfectly identical) is the symmetric mean:\n",
    "\n",
    "$$\n",
    "\\text{Max Index} =\n",
    "\\frac{1}{2}\n",
    "\\left[\n",
    "  \\sum_i \\binom{a_i}{2} +\n",
    "  \\sum_j \\binom{b_j}{2}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This ensures symmetry between the two partitions and bounds the overlap.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Adjusted Rand Index (ARI)\n",
    "\n",
    "We normalize by removing the expected random baseline and scaling by the maximum improvement possible:\n",
    "\n",
    "$$\n",
    "ARI =\n",
    "\\frac{\n",
    "  \\text{Observed Index} - E[\\text{Index}]\n",
    "}{\n",
    "  \\text{Max Index} - E[\\text{Index}]\n",
    "}\n",
    "$$\n",
    "\n",
    "Expanding all terms:\n",
    "\n",
    "$$\n",
    "ARI =\n",
    "\\frac{\n",
    "  \\sum_{ij} \\binom{n_{ij}}{2}\n",
    "  -\n",
    "  \\frac{ \\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2} }{ \\binom{n}{2} }\n",
    "}{\n",
    "  \\frac{1}{2}\n",
    "  \\left[\n",
    "    \\sum_i \\binom{a_i}{2}\n",
    "    +\n",
    "    \\sum_j \\binom{b_j}{2}\n",
    "  \\right]\n",
    "  -\n",
    "  \\frac{ \\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2} }{ \\binom{n}{2} }\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Relationship between RI and ARI\n",
    "\n",
    "| Concept | Formula | Description |\n",
    "|----------|----------|-------------|\n",
    "| **Unadjusted RI** | \\( \\frac{\\text{Observed}}{\\text{Max Possible}} \\) | Measures pairwise agreement directly |\n",
    "| **Adjusted RI (ARI)** | \\( \\frac{\\text{Observed} - \\text{Expected}}{\\text{Max Possible} - \\text{Expected}} \\) | Subtracts chance agreement, normalizes to [‚àí1, 1] |\n",
    "\n",
    "- \\( ARI = 1 \\): perfect alignment\n",
    "- \\( ARI = 0 \\): expected similarity for random labelings\n",
    "- \\( ARI < 0 \\): worse than random\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Intuition Summary\n",
    "\n",
    "- **Observed**: actual same‚Äìsame pairs in both clusterings\n",
    "- **Expected**: what you‚Äôd expect just by random overlap\n",
    "- **Max Possible**: upper bound if they matched perfectly\n",
    "\n",
    "ARI therefore tells you how much *better than chance* the clustering agreement is.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Scikit-learn usage\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import rand_score, adjusted_rand_score\n",
    "\n",
    "rand_score(true_labels, predicted_labels)\n",
    "adjusted_rand_score(true_labels, predicted_labels)\n"
   ],
   "id": "74077885b151fb78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üéØ Homogeneity, Completeness, and V-Measure\n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "When we have **ground truth labels** \\( K \\) and **predicted cluster labels** \\( C \\),\n",
    "we can evaluate clustering quality in two complementary ways:\n",
    "\n",
    "1. **Homogeneity** ‚Äî each cluster should contain *only members of a single class*\n",
    "   (no mixing inside clusters).\n",
    "\n",
    "2. **Completeness** ‚Äî all members of a given class should fall into *the same cluster*\n",
    "   (no splitting of a class across multiple clusters).\n",
    "\n",
    "3. **V-Measure** ‚Äî a balanced harmonic mean of both.\n",
    "\n",
    "These are **information-theoretic** metrics derived from **entropy** and **conditional entropy**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Entropy Setup\n",
    "\n",
    "Let:\n",
    "\n",
    "- \\( n \\): total number of samples\n",
    "- \\( K \\): true labels (classes)\n",
    "- \\( C \\): predicted cluster labels\n",
    "- \\( n_{k,c} \\): number of samples with true label \\( k \\) and cluster label \\( c \\)\n",
    "- \\( n_k = \\sum_c n_{k,c} \\): total samples in true class \\( k \\)\n",
    "- \\( n_c = \\sum_k n_{k,c} \\): total samples in predicted cluster \\( c \\)\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "H(K) = - \\sum_k \\frac{n_k}{n} \\log \\frac{n_k}{n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(C) = - \\sum_c \\frac{n_c}{n} \\log \\frac{n_c}{n}\n",
    "$$\n",
    "\n",
    "and the **conditional entropies**:\n",
    "\n",
    "$$\n",
    "H(K|C) = - \\sum_c \\sum_k \\frac{n_{k,c}}{n} \\log \\frac{n_{k,c}}{n_c}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(C|K) = - \\sum_k \\sum_c \\frac{n_{k,c}}{n} \\log \\frac{n_{k,c}}{n_k}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Homogeneity\n",
    "\n",
    "If every cluster contains only a single true class,\n",
    "then \\( H(K|C) = 0 \\).\n",
    "\n",
    "Thus we define:\n",
    "\n",
    "$$\n",
    "\\text{Homogeneity} = 1 - \\frac{H(K|C)}{H(K)}\n",
    "$$\n",
    "\n",
    "- 1 ‚Üí perfectly homogeneous (pure clusters)\n",
    "- 0 ‚Üí cluster assignment gives no info about true labels\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Completeness\n",
    "\n",
    "If all members of each class fall into a single cluster,\n",
    "then \\( H(C|K) = 0 \\).\n",
    "\n",
    "We define:\n",
    "\n",
    "$$\n",
    "\\text{Completeness} = 1 - \\frac{H(C|K)}{H(C)}\n",
    "$$\n",
    "\n",
    "- 1 ‚Üí perfectly complete (no class split)\n",
    "- 0 ‚Üí cluster labels independent of true labels\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Example Intuition\n",
    "\n",
    "Consider the following case:\n",
    "\n",
    "| True | Predicted cluster | Comment |\n",
    "|------|--------------------|----------|\n",
    "| A | 1 | good |\n",
    "| A | 1 | good |\n",
    "| A | 2 | split ‚Üí lowers completeness |\n",
    "| B | 2 | good |\n",
    "| B | 2 | good |\n",
    "| C | 3 | good |\n",
    "| C | 3 | good |\n",
    "| C | 3 | good |\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- Cluster **1**: contains only A ‚Üí high *homogeneity*.\n",
    "- But true class **A** is spread across clusters 1 and 2 ‚Üí lower *completeness*.\n",
    "- Thus, overall \\( V \\)-measure will fall between the two.\n",
    "\n",
    "This example helps visualize **why** we calculate \\( H(K|C) \\) (for cluster purity)\n",
    "and \\( H(C|K) \\) (for class completeness).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. V-Measure\n",
    "\n",
    "To balance both metrics, we define:\n",
    "\n",
    "$$\n",
    "V_\\beta =\n",
    "(1 + \\beta)\n",
    "\\frac{\n",
    "  \\text{Homogeneity} \\times \\text{Completeness}\n",
    "}{\n",
    "  (\\beta \\times \\text{Homogeneity}) + \\text{Completeness}\n",
    "}\n",
    "$$\n",
    "\n",
    "When \\( \\beta = 1 \\), it becomes the **harmonic mean**:\n",
    "\n",
    "$$\n",
    "V = 2\n",
    "\\frac{\n",
    "  \\text{Homogeneity} \\times \\text{Completeness}\n",
    "}{\n",
    "  \\text{Homogeneity} + \\text{Completeness}\n",
    "}\n",
    "$$\n",
    "\n",
    "- \\( \\beta > 1 \\): weight completeness more\n",
    "- \\( \\beta < 1 \\): weight homogeneity more\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Key Interpretations\n",
    "\n",
    "| Metric | Measures | Perfect when | Uses |\n",
    "|---------|-----------|---------------|------|\n",
    "| **Homogeneity** | cluster purity | each cluster contains only one class | detect over-mixed clusters |\n",
    "| **Completeness** | class compactness | all class members share one cluster | detect over-split classes |\n",
    "| **V-Measure** | trade-off between both | both pure and complete | balanced evaluation |\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Relationship to Entropy & Mutual Information\n",
    "\n",
    "- \\( H(K|C) \\) quantifies *how impure clusters are*.\n",
    "- \\( H(C|K) \\) quantifies *how fragmented classes are*.\n",
    "- Both derived from conditional entropy, normalized by total entropy to give values in \\([0,1]\\).\n",
    "\n",
    "They are also equivalent to **normalized mutual information (NMI)** under symmetric weighting.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Scikit-learn Usage\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "\n",
    "homogeneity_score(true_labels, predicted_labels)\n",
    "completeness_score(true_labels, predicted_labels)\n",
    "v_measure_score(true_labels, predicted_labels)\n"
   ],
   "id": "625d3a25afbe6be5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß© Mutual Information (MI), Normalized MI (NMI), and Adjusted MI (AMI)\n",
    "\n",
    "## 1. Intuition\n",
    "\n",
    "Mutual Information (MI) measures how much *knowing the predicted cluster labels* \\( C \\)\n",
    "reduces our uncertainty about the *true labels* \\( K \\).\n",
    "\n",
    "- If \\( C \\) and \\( K \\) are **independent** ‚Üí no information gain ‚Üí \\( MI = 0 \\).\n",
    "- If \\( C \\) perfectly predicts \\( K \\) ‚Üí complete information overlap ‚Üí \\( MI \\) is maximal.\n",
    "\n",
    "In clustering evaluation, we measure how much the two labelings share information.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Theoretical Definition\n",
    "\n",
    "From information theory:\n",
    "\n",
    "$$\n",
    "I(K, C) = \\sum_i \\sum_j P(i, j) \\log \\frac{P(i, j)}{P(i) P(j)}\n",
    "$$\n",
    "\n",
    "- \\( P(i, j) \\): joint probability of true class \\( i \\) and cluster \\( j \\)\n",
    "- \\( P(i) \\), \\( P(j) \\): marginal probabilities\n",
    "\n",
    "If \\( P(i, j) = P(i) P(j) \\) (independent), then \\( I(K, C) = 0 \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Empirical Form (Using Counts)\n",
    "\n",
    "We have empirical counts:\n",
    "\n",
    "- \\( n_{ij} \\): number of samples in both true class \\( i \\) and predicted cluster \\( j \\)\n",
    "- \\( n_i = \\sum_j n_{ij} \\): number in class \\( i \\)\n",
    "- \\( n_j = \\sum_i n_{ij} \\): number in cluster \\( j \\)\n",
    "- \\( n \\): total samples\n",
    "\n",
    "We estimate:\n",
    "\n",
    "$$\n",
    "P(i, j) = \\frac{n_{ij}}{n}, \\quad P(i) = \\frac{n_i}{n}, \\quad P(j) = \\frac{n_j}{n}\n",
    "$$\n",
    "\n",
    "Substitute into the definition:\n",
    "\n",
    "$$\n",
    "MI(K, C)\n",
    "= \\sum_i \\sum_j\n",
    "\\frac{n_{ij}}{n}\n",
    "\\log\n",
    "\\frac{\n",
    "  \\frac{n_{ij}}{n}\n",
    "}{\n",
    "  \\frac{n_i}{n} \\frac{n_j}{n}\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Simplifying the Expression\n",
    "\n",
    "Simplify the argument of the logarithm:\n",
    "\n",
    "$$\n",
    "\\frac{\n",
    "  \\frac{n_{ij}}{n}\n",
    "}{\n",
    "  \\frac{n_i}{n} \\frac{n_j}{n}\n",
    "}\n",
    "= \\frac{n_{ij}/n}{(n_i n_j)/n^2}\n",
    "= \\frac{n_{ij} \\, n}{n_i \\, n_j}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "MI(K, C)\n",
    "= \\sum_i \\sum_j\n",
    "\\frac{n_{ij}}{n}\n",
    "\\log\n",
    "\\frac{n_{ij} \\, n}{n_i \\, n_j}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why There Is an \\( n \\) in the Numerator\n",
    "\n",
    "That \\( n \\) appears because we converted from probabilities to counts.\n",
    "\n",
    "Originally, we had a *ratio of probabilities*:\n",
    "\n",
    "$$\n",
    "\\frac{P(i, j)}{P(i) P(j)}\n",
    "$$\n",
    "\n",
    "Substituting their empirical forms:\n",
    "\n",
    "$$\n",
    "\\frac{n_{ij}/n}{(n_i/n)(n_j/n)} = \\frac{n_{ij} n}{n_i n_j}\n",
    "$$\n",
    "\n",
    "The \\( n \\) ensures the ratio is **dimensionless** and correctly scales the joint probability\n",
    "to what would be expected under independence.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Alternative Form\n",
    "\n",
    "You may also see:\n",
    "\n",
    "$$\n",
    "MI(K, C) = \\frac{1}{n} \\sum_{i,j} n_{ij} \\log \\frac{n_{ij} n}{n_i n_j}\n",
    "$$\n",
    "\n",
    "Both forms are equivalent because:\n",
    "\n",
    "$$\n",
    "\\frac{n_{ij}}{n} \\log(\\cdot) = \\frac{1}{n} n_{ij} \\log(\\cdot)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Relation to Entropy\n",
    "\n",
    "MI can also be expressed as:\n",
    "\n",
    "$$\n",
    "MI(K, C) = H(K) + H(C) - H(K, C)\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "MI(K, C) = H(K) - H(K|C) = H(C) - H(C|K)\n",
    "$$\n",
    "\n",
    "This means MI quantifies the **reduction in uncertainty** of one variable when the other is known.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Normalized Mutual Information (NMI)\n",
    "\n",
    "Raw MI depends on the scale of entropies, so we normalize it:\n",
    "\n",
    "$$\n",
    "NMI(K, C) = \\frac{MI(K, C)}{\\sqrt{H(K) \\, H(C)}}\n",
    "$$\n",
    "\n",
    "Range: \\([0, 1]\\)\n",
    "- 1 ‚Üí identical partitions\n",
    "- 0 ‚Üí completely independent\n",
    "\n",
    "Alternative normalizations (like dividing by average entropy) exist, but the geometric mean is the symmetric one.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Adjusted Mutual Information (AMI)\n",
    "\n",
    "Random clusterings can still yield positive MI values.\n",
    "To correct for that, we adjust for the expected MI under independence:\n",
    "\n",
    "$$\n",
    "AMI =\n",
    "\\frac{\n",
    "  MI(K, C) - E[MI(K, C)]\n",
    "}{\n",
    "  \\max(H(K), H(C)) - E[MI(K, C)]\n",
    "}\n",
    "$$\n",
    "\n",
    "where \\( E[MI(K, C)] \\) is the **expected mutual information** if \\( K \\) and \\( C \\)\n",
    "were random labelings with the same size distributions.\n",
    "\n",
    "Range: \\([-1, 1]\\)\n",
    "- 1 ‚Üí perfect agreement\n",
    "- 0 ‚Üí random labeling baseline\n",
    "- < 0 ‚Üí worse than random\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Intuitive Analogy\n",
    "\n",
    "| Concept | Meaning |\n",
    "|----------|----------|\n",
    "| \\( H(K) \\) | Uncertainty in true labels |\n",
    "| \\( H(K|C) \\) | Remaining uncertainty after knowing clusters |\n",
    "| \\( MI = H(K) - H(K|C) \\) | Information gained about true labels from clustering |\n",
    "| \\( NMI \\) | Rescales MI to 0‚Äì1 |\n",
    "| \\( AMI \\) | Corrects MI for chance overlap |\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Summary Table\n",
    "\n",
    "| Metric | Formula | Range | Meaning |\n",
    "|---------|----------|--------|----------|\n",
    "| **MI** | \\( \\sum_{ij} \\frac{n_{ij}}{n} \\log \\frac{n_{ij} n}{n_i n_j} \\) | ‚â• 0 | Shared information between partitions |\n",
    "| **NMI** | \\( \\frac{MI}{\\sqrt{H(K)H(C)}} \\) | [0, 1] | Scale-free version of MI |\n",
    "| **AMI** | \\( \\frac{MI - E[MI]}{\\max(H(K), H(C)) - E[MI]} \\) | [‚àí1, 1] | Chance-adjusted MI |\n",
    "\n",
    "---\n",
    "\n",
    "## 12. When to Use\n",
    "\n",
    "| Scenario | Best Metric | Reason |\n",
    "|-----------|--------------|---------|\n",
    "| Compare clusterings on same dataset | **NMI** | Scale-free, symmetric |\n",
    "| Compare across datasets / cluster counts | **AMI** | Removes random-labeling bias |\n",
    "| Want raw info-theoretic score | **MI** | Shows total shared information |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary Insight**\n",
    "\n",
    "That ‚Äú\\( n \\)‚Äù inside the logarithm is not arbitrary ‚Äî it‚Äôs the artifact of moving\n",
    "from the probabilistic definition\n",
    "\\( P(i,j) / [P(i)P(j)] \\)\n",
    "to its empirical count-based version.\n",
    "\n",
    "It ensures that mutual information reflects how much more (or less) often a true‚Äìpredicted pair co-occurs than expected by pure chance.\n"
   ],
   "id": "c7ab556cc7320b9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß© Fowlkes‚ÄìMallows Index (FMI)\n",
    "\n",
    "## 1. Intuition\n",
    "\n",
    "The **Fowlkes‚ÄìMallows Index (FMI)** measures the similarity between two clusterings\n",
    "by comparing them as a **pairwise binary classification task**.\n",
    "\n",
    "Each pair of samples \\((x_i, x_j)\\) is treated as a separate data point:\n",
    "\n",
    "| Pair type | ‚ÄúGround truth‚Äù label | ‚ÄúPrediction‚Äù label |\n",
    "|------------|----------------------|--------------------|\n",
    "| Same true class | 1 (positive) | ? |\n",
    "| Different true class | 0 (negative) | ? |\n",
    "\n",
    "The clustering algorithm \"predicts\" whether a pair belongs to the same cluster (1) or not (0).\n",
    "\n",
    "Thus, FMI behaves like an **F1-score for pairwise clustering consistency**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Pairwise Confusion Matrix\n",
    "\n",
    "For all pairs of samples, we define:\n",
    "\n",
    "| Term | Meaning | Analogy |\n",
    "|------|----------|---------|\n",
    "| **TP (True Positive)** | Same true class *and* same predicted cluster | Correctly grouped pairs |\n",
    "| **FP (False Positive)** | Different true class but same predicted cluster | Wrongly grouped pairs |\n",
    "| **FN (False Negative)** | Same true class but different predicted clusters | Wrongly separated pairs |\n",
    "| **TN (True Negative)** | Different true class and different predicted clusters | Correctly separated pairs |\n",
    "\n",
    "Unlike the Rand Index, FMI ignores **TN** pairs because they dominate in large datasets\n",
    "and carry less useful information about cluster quality.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Pairwise Precision and Recall\n",
    "\n",
    "We define **pairwise precision** and **pairwise recall** as:\n",
    "\n",
    "$$\n",
    "P = \\frac{TP}{TP + FP} = \\frac{a}{a + b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R = \\frac{TP}{TP + FN} = \\frac{a}{a + c}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( a = TP \\): number of pairs that are both same-class and same-cluster\n",
    "- \\( b = FP \\): pairs in the same cluster but different classes\n",
    "- \\( c = FN \\): pairs in the same class but different clusters\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- **Precision (P)**: of all pairs clustered together, how many truly belong together?\n",
    "- **Recall (R)**: of all true same-class pairs, how many are clustered together?\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Fowlkes‚ÄìMallows Index Definition\n",
    "\n",
    "The FMI is the **geometric mean** of precision and recall:\n",
    "\n",
    "$$\n",
    "FMI = \\sqrt{P \\times R}\n",
    "$$\n",
    "\n",
    "or equivalently in terms of pair counts:\n",
    "\n",
    "$$\n",
    "FMI = \\frac{a}{\\sqrt{(a + b)(a + c)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why Geometric Mean?\n",
    "\n",
    "The geometric mean ensures that both precision and recall contribute symmetrically.\n",
    "It penalizes strong imbalance between the two while remaining scale-free and symmetric.\n",
    "\n",
    "If either precision or recall is zero, the whole FMI becomes zero.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Range and Interpretation\n",
    "\n",
    "- Range: \\( [0, 1] \\)\n",
    "  - \\( 1 \\): perfect clustering (every true pair grouped correctly)\n",
    "  - \\( 0 \\): no pairwise match\n",
    "- Symmetric: swapping true ‚Üî predicted labels does not change FMI\n",
    "- Independent of the number of clusters (unlike plain RI)\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Example\n",
    "\n",
    "Suppose we have the following counts:\n",
    "\n",
    "| Symbol | Meaning | Count |\n",
    "|---------|----------|-------|\n",
    "| \\( a \\) | same-class & same-cluster | 30 |\n",
    "| \\( b \\) | different-class but same-cluster | 10 |\n",
    "| \\( c \\) | same-class but different-cluster | 20 |\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "P = \\frac{30}{30 + 10} = 0.75, \\quad\n",
    "R = \\frac{30}{30 + 20} = 0.6\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "FMI = \\sqrt{0.75 \\times 0.6} = 0.67\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Relation to Rand and Adjusted Rand Indices\n",
    "\n",
    "| Metric | Includes TN? | Analogy | Focus |\n",
    "|---------|--------------|---------|--------|\n",
    "| **Rand Index (RI)** | ‚úÖ yes | Accuracy over all pairs | Overall agreement |\n",
    "| **Adjusted Rand Index (ARI)** | ‚úÖ yes (chance corrected) | Adjusted accuracy | Chance-adjusted consistency |\n",
    "| **Fowlkes‚ÄìMallows (FMI)** | ‚ùå no | F1-score on ‚Äúsame-cluster‚Äù pairs | Precision‚Äìrecall balance |\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Conceptual Summary\n",
    "\n",
    "| Concept | Analogy | Formula |\n",
    "|----------|----------|----------|\n",
    "| Pairwise Precision | Classification precision | \\( P = \\frac{a}{a + b} \\) |\n",
    "| Pairwise Recall | Classification recall | \\( R = \\frac{a}{a + c} \\) |\n",
    "| FMI | F1-like score for clustering | \\( FMI = \\sqrt{P \\times R} \\) |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary Insight**\n",
    "\n",
    "FMI views clustering as a **binary classification** over pairs of samples:\n",
    "\n",
    "- ‚ÄúSame cluster?‚Äù is the prediction.\n",
    "- ‚ÄúSame true class?‚Äù is the ground truth.\n",
    "\n",
    "It balances **pairwise precision and recall**,\n",
    "giving a symmetric, interpretable measure of how well the clusters match the real classes.\n"
   ],
   "id": "835a2584f9e9bb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß≠ Choosing the Right Supervised Clustering Metric\n",
    "\n",
    "Supervised clustering metrics require **ground truth labels** to compare\n",
    "against predicted cluster assignments.\n",
    "\n",
    "They can be grouped into three main families:\n",
    "\n",
    "1. **Pair-based metrics** ‚Äî compare pairs of samples.\n",
    "2. **Information-theoretic metrics** ‚Äî measure shared information.\n",
    "3. **Entropy-based purity metrics** ‚Äî measure cluster purity and completeness.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Pair-based Metrics\n",
    "\n",
    "### üß© Adjusted Rand Index (ARI)\n",
    "\n",
    "- **Measures:** overall pairwise agreement, adjusted for random chance.\n",
    "- **Formula:**\n",
    "\n",
    "  $$\n",
    "  ARI =\n",
    "  \\frac{\n",
    "    \\text{Index} - E[\\text{Index}]\n",
    "  }{\n",
    "    \\text{Max Index} - E[\\text{Index}]\n",
    "  }\n",
    "  $$\n",
    "\n",
    "- **Intuition:** evaluates how consistent pair relationships are between true and predicted partitions.\n",
    "- **Range:** \\( [-1, 1] \\)\n",
    "- **Best for:** comparing clustering results when chance similarity must be removed.\n",
    "- **Strengths:** chance correction, symmetric, robust.\n",
    "- **Weaknesses:** quadratic in \\( n \\) (pairwise comparisons).\n",
    "\n",
    "‚úÖ *Use ARI when comparing algorithms or assessing true agreement structurally.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Fowlkes‚ÄìMallows Index (FMI)\n",
    "\n",
    "- **Measures:** geometric mean of pairwise precision and recall.\n",
    "- **Formula:**\n",
    "\n",
    "  $$\n",
    "  FMI = \\sqrt{P \\times R} = \\frac{a}{\\sqrt{(a + b)(a + c)}}\n",
    "  $$\n",
    "\n",
    "  where \\( a = TP, b = FP, c = FN \\).\n",
    "\n",
    "- **Intuition:** treats clustering as a binary classification over pairs:\n",
    "  - \"Same cluster?\" = prediction\n",
    "  - \"Same true class?\" = ground truth\n",
    "- **Range:** \\( [0, 1] \\)\n",
    "- **Best for:** quick, interpretable balance between clustering precision and recall.\n",
    "- **Strengths:** symmetric, intuitive, scale-free.\n",
    "- **Weaknesses:** no chance correction.\n",
    "\n",
    "‚úÖ *Use FMI when you want a simple, interpretable ‚Äúpairwise F1-score‚Äù.*\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Information-Theoretic Metrics\n",
    "\n",
    "### üß© Mutual Information (MI)\n",
    "\n",
    "- **Measures:** shared information between true labels and predicted clusters.\n",
    "- **Formula:**\n",
    "\n",
    "  $$\n",
    "  MI(K, C) = \\sum_{i,j} \\frac{n_{ij}}{n} \\log \\frac{n_{ij} n}{n_i n_j}\n",
    "  $$\n",
    "\n",
    "- **Intuition:** how much knowing one labeling reduces uncertainty about the other.\n",
    "- **Range:** ‚â• 0 (unbounded)\n",
    "- **Best for:** theoretical analysis or conceptual understanding.\n",
    "- **Weakness:** depends on entropy scale (not normalized).\n",
    "\n",
    "‚ö†Ô∏è *Use MI only for conceptual exploration; not suitable for direct comparison.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Normalized Mutual Information (NMI)\n",
    "\n",
    "- **Measures:** MI scaled to \\([0, 1]\\).\n",
    "- **Formula:**\n",
    "\n",
    "  $$\n",
    "  NMI(K, C) = \\frac{MI(K, C)}{\\sqrt{H(K) \\, H(C)}}\n",
    "  $$\n",
    "\n",
    "- **Intuition:** proportion of shared information between true and predicted labels.\n",
    "- **Range:** \\( [0, 1] \\)\n",
    "- **Best for:** comparing clustering quality on the same dataset.\n",
    "- **Strengths:** normalized, symmetric, interpretable.\n",
    "- **Weaknesses:** still slightly biased toward more clusters.\n",
    "\n",
    "‚úÖ *Use NMI for normalized, interpretable comparisons across multiple runs.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Adjusted Mutual Information (AMI)\n",
    "\n",
    "- **Measures:** MI corrected for chance.\n",
    "- **Formula:**\n",
    "\n",
    "  $$\n",
    "  AMI =\n",
    "  \\frac{\n",
    "    MI(K, C) - E[MI(K, C)]\n",
    "  }{\n",
    "    \\max(H(K), H(C)) - E[MI(K, C)]\n",
    "  }\n",
    "  $$\n",
    "\n",
    "- **Intuition:** how much more information is shared than expected by random labelings.\n",
    "- **Range:** \\( [-1, 1] \\)\n",
    "- **Best for:** comparing clusterings across datasets or when cluster counts vary.\n",
    "- **Strengths:** chance-corrected, scale-invariant.\n",
    "- **Weaknesses:** slightly less intuitive to interpret.\n",
    "\n",
    "‚úÖ *Use AMI when comparing across datasets or cluster counts with fair normalization.*\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Entropy-Based Purity Metrics\n",
    "\n",
    "### üß© Homogeneity, Completeness, and V-Measure\n",
    "\n",
    "- **Formulas:**\n",
    "\n",
    "  $$\n",
    "  \\text{Homogeneity} = 1 - \\frac{H(K|C)}{H(K)}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{Completeness} = 1 - \\frac{H(C|K)}{H(C)}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  V = 2 \\frac{\\text{Homogeneity} \\times \\text{Completeness}}{\\text{Homogeneity} + \\text{Completeness}}\n",
    "  $$\n",
    "\n",
    "- **Intuition:**\n",
    "  - **Homogeneity:** each cluster should contain one class (purity).\n",
    "  - **Completeness:** each class should appear in one cluster (coverage).\n",
    "  - **V-Measure:** harmonic mean balancing both.\n",
    "- **Range:** \\( [0, 1] \\)\n",
    "- **Best for:** diagnosing *how* a clustering fails (mixing vs splitting).\n",
    "- **Strengths:** interpretable, symmetric, scale-free.\n",
    "- **Weaknesses:** no chance correction.\n",
    "\n",
    "‚úÖ *Use V-Measure for interpretability; use Homogeneity/Completeness to diagnose over-splitting or merging.*\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Quick Selection Guide\n",
    "\n",
    "| Goal / Situation | Recommended Metric | Reason |\n",
    "|------------------|--------------------|--------|\n",
    "| Need robust, chance-corrected similarity | **Adjusted Rand Index (ARI)** | Gold standard for supervised clustering |\n",
    "| Want interpretable, F1-like pairwise score | **Fowlkes‚ÄìMallows (FMI)** | Simple, symmetric, intuitive |\n",
    "| Want normalized info overlap | **Normalized Mutual Information (NMI)** | Easy 0‚Äì1 comparison |\n",
    "| Want chance-corrected info overlap | **Adjusted Mutual Information (AMI)** | Fair comparison across datasets |\n",
    "| Want to inspect purity vs fragmentation | **Homogeneity / Completeness / V-Measure** | Diagnostic interpretability |\n",
    "| Want conceptual measure of shared info | **Mutual Information (MI)** | Raw theoretical understanding |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Practical Tips\n",
    "\n",
    "- ‚úÖ **Use ARI** for robust quantitative comparison.\n",
    "- ‚úÖ **Use AMI** when number of clusters differs across algorithms.\n",
    "- ‚úÖ **Use V-Measure** when interpretability or diagnostic value is important.\n",
    "- ‚úÖ **Use FMI** for quick intuitive checks (pairwise F1-like metric).\n",
    "- ‚ö†Ô∏è Avoid plain RI or MI ‚Äî they‚Äôre unnormalized and can mislead.\n",
    "- üí° Compare multiple metrics to get both numerical and interpretive insight.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary Insight**\n",
    "\n",
    "| Family | Metric | What It Captures |\n",
    "|---------|---------|------------------|\n",
    "| **Pair-based** | ARI / FMI | Pairwise agreement (like accuracy or F1) |\n",
    "| **Information-theoretic** | MI / NMI / AMI | Information overlap and chance-corrected entropy |\n",
    "| **Entropy-based purity** | Homogeneity / Completeness / V | Purity vs coverage diagnostic |\n",
    "\n",
    "Together, they provide a comprehensive toolkit:\n",
    "- **ARI / AMI:** quantitative robustness\n",
    "- **V-Measure:** interpretability\n",
    "- **FMI:** intuition\n",
    "- **NMI:** normalized comparability\n"
   ],
   "id": "129b4511319eb955"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e0d01a67fa97a266"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
