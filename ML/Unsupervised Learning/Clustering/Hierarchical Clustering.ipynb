{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.3.6 Hierarchical Clustering ‚Äî scikit-learn Notes\n",
    "\n",
    "---\n",
    "\n",
    "## üß© What It Is\n",
    "Hierarchical clustering builds **nested clusters** by successively merging or splitting them.\n",
    "The resulting structure is a **tree (dendrogram)** where:\n",
    "- The **root** is one big cluster containing all samples.\n",
    "- The **leaves** are individual samples.\n",
    "- The **internal nodes** represent intermediate merges.\n",
    "\n",
    "By cutting the tree at different levels, we can obtain different numbers of clusters.\n",
    "In scikit-learn, this is mainly implemented via **Agglomerative Clustering** (bottom-up).\n",
    "\n",
    "---\n",
    "\n",
    "## üîΩ Agglomerative (Bottom-Up) Approach\n",
    "Each observation starts as its own cluster.\n",
    "At each step, the algorithm merges the **two clusters that are most similar**, according to a linkage criterion.\n",
    "\n",
    "### Linkage Types\n",
    "| Linkage | Merge rule | Typical behaviour |\n",
    "|----------|-------------|-------------------|\n",
    "| **Ward** | Minimises within-cluster variance (sum of squared differences) | Produces regular, compact clusters; only works with Euclidean distance |\n",
    "| **Complete** | Minimises the *maximum* distance between points in different clusters | Produces compact, evenly sized clusters |\n",
    "| **Average** | Minimises the *average* distance between points in different clusters | More balanced; supports non-Euclidean distances |\n",
    "| **Single** | Minimises the *minimum* distance between points in different clusters | Captures chain-like shapes; sensitive to noise (‚Äúrich-get-richer‚Äù) |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Choosing Linkage\n",
    "\n",
    "- **Ward** ‚Üí most regular cluster sizes but only with Euclidean metric.\n",
    "- **Single** ‚Üí flexible shapes but high sensitivity to noise.\n",
    "- **Average / Complete** ‚Üí can handle non-Euclidean metrics like cosine or L1.\n",
    "- The algorithm can exhibit ‚Äú**rich-get-richer**‚Äù behavior ‚Äî large clusters keep absorbing smaller ones.\n",
    "\n",
    "Guideline from the doc:\n",
    "> ‚ÄúSingle linkage is the worst for uneven clusters; Ward gives the most regular sizes.\n",
    "> For non-Euclidean metrics, average linkage is a good alternative.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Connectivity Constraints\n",
    "We can impose a **connectivity matrix** (often sparse) that defines which samples are allowed to merge.\n",
    "\n",
    "Use cases:\n",
    "- Spatial data (e.g., neighboring pixels in an image).\n",
    "- Graph data (e.g., K-nearest neighbors).\n",
    "\n",
    "Benefits:\n",
    "- Adds locality structure.\n",
    "- Can **speed up computation** by reducing possible merges.\n",
    "\n",
    "Caveats:\n",
    "- Can amplify ‚Äúrich-get-richer‚Äù effect for single/average/complete linkages.\n",
    "\n",
    "---\n",
    "\n",
    "## üìè Distance Metric (Affinity)\n",
    "For linkages other than Ward, the metric (affinity) can vary.\n",
    "\n",
    "Common choices:\n",
    "- **Euclidean** (default)\n",
    "- **L1 (Manhattan)** ‚Üí robust for sparse data\n",
    "- **Cosine** ‚Üí invariant to vector magnitude, good for text embeddings\n",
    "\n",
    "Rule of thumb:\n",
    "> Choose a metric that **maximizes inter-cluster distance** while **minimizing intra-cluster distance**.\n",
    "\n",
    "Ward linkage supports only Euclidean distance.\n",
    "\n",
    "---\n",
    "\n",
    "## üå≥ Dendrogram Visualization\n",
    "- Dendrograms show the **merge hierarchy**.\n",
    "- For small datasets, it‚Äôs helpful for visual diagnostics.\n",
    "- Each vertical split corresponds to a merge at a specific distance threshold.\n",
    "- Cutting at a horizontal distance level yields a chosen number of clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Practical Considerations\n",
    "\n",
    "- **Scalability**:\n",
    "  Without connectivity, the algorithm considers *all pairwise merges*, making it \\( O(n^3) \\) and slow for large datasets.\n",
    "  With connectivity, merges are limited to local neighborhoods, improving speed.\n",
    "\n",
    "- **Memory**:\n",
    "  Pairwise distances are \\( O(n^2) \\), which can be large.\n",
    "\n",
    "- **Transductive**:\n",
    "  Hierarchical clustering doesn‚Äôt easily assign new samples post-fit.\n",
    "  To label new points, one usually must re-fit or approximate via nearest cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÇÔ∏è Divisive Variant ‚Äî Bisecting K-Means\n",
    "An alternative hierarchical approach (top-down):\n",
    "- Start with one cluster containing all samples.\n",
    "- Repeatedly **bisect clusters** using K-Means until the desired number is reached.\n",
    "- Efficient when \\( n_{\\text{clusters}} \\ll n_{\\text{samples}} \\).\n",
    "\n",
    "---\n",
    "\n",
    "## üß† When to Use Hierarchical Clustering\n",
    "\n",
    "**Use it when:**\n",
    "- You want **hierarchical structure** (not just flat labels).\n",
    "- You need interpretability via dendrograms.\n",
    "- You can work with small-to-medium datasets.\n",
    "- You have graph/spatial data and can use connectivity constraints.\n",
    "\n",
    "**Avoid it when:**\n",
    "- You need scalability for very large datasets.\n",
    "- You must assign new/unseen samples quickly (online or streaming context).\n",
    "- You only need simple flat clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Takeaways\n",
    "\n",
    "- Linkage choice controls **merge strategy** and **cluster shape**.\n",
    "- Metric choice affects **distance sensitivity**.\n",
    "- Connectivity can encode **locality** or **graph structure**.\n",
    "- Dendrograms provide **interpretable visualization**.\n",
    "- Complexity grows quickly with number of samples.\n",
    "- Hierarchical clustering is **transductive**, not inductive.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Relevance for MLE Practice\n",
    "\n",
    "- **Preprocessing** (scaling, outlier handling) critically affects performance.\n",
    "- **Interpretability**: tree structure is valuable for explaining how clusters form.\n",
    "- **Dimensionality reduction** (PCA/t-SNE/UMAP) is often used before hierarchical clustering to stabilize results.\n",
    "- **Production caveats**: assigning new data points is non-trivial ‚Äî not ideal for online systems.\n",
    "- **Hyperparameters** (linkage, affinity, connectivity) should be tuned thoughtfully.\n",
    "- **Multi-resolution view**: dendrogram enables analysis at different cluster granularities.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Summary Table\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| Algorithm | Agglomerative clustering (bottom-up) |\n",
    "| Output | Dendrogram + flat cluster labels |\n",
    "| Linkages | Ward, complete, average, single |\n",
    "| Distance metric | Euclidean (Ward), others allowed for others |\n",
    "| Constraints | Optional connectivity matrix |\n",
    "| Complexity | \\( O(n^3) \\) naive, improved with constraints |\n",
    "| Inductive? | No ‚Äî transductive only |\n",
    "| Visualization | Dendrogram (merge tree) |\n",
    "| Alternative | Bisecting K-Means (top-down) |\n",
    "\n",
    "---\n"
   ],
   "id": "68c8f7bbf6052c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a6f3c24a6bbf915"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
