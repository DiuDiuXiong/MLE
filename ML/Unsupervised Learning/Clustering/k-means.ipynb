{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üåÄ K-Means Clustering\n",
    "\n",
    "## Intuition\n",
    "\n",
    "K-Means partitions data into *K* groups so that:\n",
    "- points within the same cluster are **as close as possible**,\n",
    "- points in different clusters are **as far apart as possible**.\n",
    "\n",
    "Each cluster is represented by its **centroid**, the mean of its members.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "We minimize the total **within-cluster sum of squares (WCSS)**:\n",
    "\n",
    "$$\n",
    "J = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( C_k \\) = set of points in cluster \\(k\\)\n",
    "- \\( \\mu_k \\) = centroid of cluster \\(k\\)\n",
    "- \\( \\|x_i - \\mu_k\\|^2 \\) = squared Euclidean distance\n",
    "\n",
    "Goal: find centroids \\( \\{\\mu_k\\}_{k=1}^{K} \\) that minimize \\(J\\).\n",
    "\n",
    "---\n",
    "\n",
    "## Lloyd‚Äôs Algorithm\n",
    "\n",
    "Iterate until convergence:\n",
    "\n",
    "1. **Assignment step (E-step)**\n",
    "   Assign each sample to the nearest centroid:\n",
    "   $$\n",
    "   c_i = \\arg\\min_k \\|x_i - \\mu_k\\|^2\n",
    "   $$\n",
    "\n",
    "2. **Update step (M-step)**\n",
    "   Recompute each centroid as the mean of its assigned points:\n",
    "   $$\n",
    "   \\mu_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i\n",
    "   $$\n",
    "\n",
    "Each step never increases \\(J\\), so the process converges.\n",
    "\n",
    "---\n",
    "\n",
    "## Convergence and Boundaries\n",
    "\n",
    "Points near the boundary between clusters can temporarily be closer to another centroid.\n",
    "In the next assignment step, they are simply **reassigned** to that nearer cluster,\n",
    "and centroids update again accordingly.\n",
    "\n",
    "The algorithm stops when assignments no longer change ‚Äî\n",
    "that‚Äôs a **local minimum** of \\(J\\).\n",
    "To avoid poor local minima, use **K-Means++** initialization or multiple random restarts (`n_init`).\n",
    "\n",
    "---\n",
    "\n",
    "## K-Means++ Initialization\n",
    "\n",
    "To pick smarter starting centroids:\n",
    "\n",
    "1. Choose one random data point \\(x_i\\) as the first centroid.\n",
    "2. For each remaining point, compute its squared distance to the **nearest** chosen centroid:\n",
    "   $$\n",
    "   D(x_i)^2 = \\min_{\\mu \\in C} \\|x_i - \\mu\\|^2\n",
    "   $$\n",
    "3. Sample the next centroid from the data points with probability proportional to \\(D(x_i)^2\\):\n",
    "   $$\n",
    "   p_i = \\frac{D(x_i)^2}{\\sum_j D(x_j)^2}\n",
    "   $$\n",
    "4. Repeat until \\(K\\) centroids are chosen.\n",
    "5. Proceed with normal Lloyd iterations.\n",
    "\n",
    "‚úÖ Each centroid starts as a **real data point** \\(x_i\\),\n",
    "ensuring diverse, well-spread initial centers.\n",
    "\n",
    "---\n",
    "\n",
    "## Mini-Batch K-Means\n",
    "\n",
    "For large datasets, we can update centroids using only **small random batches** of samples.\n",
    "\n",
    "For each mini-batch \\(B\\):\n",
    "\n",
    "1. Assign each \\(x_i \\in B\\) to its nearest centroid.\n",
    "2. Update the corresponding centroid incrementally:\n",
    "\n",
    "   $$\n",
    "   \\mu_k \\leftarrow \\mu_k + \\eta (x_i - \\mu_k)\n",
    "   $$\n",
    "\n",
    "If we set the learning rate \\( \\eta = \\frac{1}{t} \\),\n",
    "where \\(t\\) is the number of points assigned to cluster \\(k\\) so far,\n",
    "then the update is **exactly equivalent** to maintaining the running mean.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "\\mu_k^{(t)} &= \\mu_k^{(t-1)} + \\frac{1}{t}(x_t - \\mu_k^{(t-1)}) \\\\\n",
    "&= \\left(1 - \\frac{1}{t}\\right)\\mu_k^{(t-1)} + \\frac{1}{t}x_t \\\\\n",
    "&= \\frac{t-1}{t} \\cdot \\frac{1}{t-1}\\sum_{i=1}^{t-1} x_i + \\frac{1}{t}x_t \\\\\n",
    "&= \\frac{1}{t}\\sum_{i=1}^{t} x_i\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "‚úÖ So the incremental update keeps the **exact mean** while avoiding full dataset passes.\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing K\n",
    "\n",
    "### Elbow Method\n",
    "\n",
    "Compute total cost \\(J_K\\) for different values of \\(K\\):\n",
    "\n",
    "$$\n",
    "J_K = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2\n",
    "$$\n",
    "\n",
    "Plot \\(J_K\\) vs \\(K\\).\n",
    "As \\(K\\) increases, \\(J_K\\) decreases ‚Äî\n",
    "pick \\(K\\) at the **‚Äúelbow‚Äù**, where adding more clusters gives little improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### Silhouette Method\n",
    "\n",
    "For each point \\(x_i\\):\n",
    "\n",
    "- \\(a_i\\): average distance to other points in its **own** cluster.\n",
    "- \\(b_i\\): smallest average distance to points in any **other** cluster.\n",
    "\n",
    "Then silhouette score:\n",
    "\n",
    "$$\n",
    "s_i = \\frac{b_i - a_i}{\\max(a_i, b_i)}\n",
    "$$\n",
    "\n",
    "- \\(s_i \\approx 1\\): well-clustered\n",
    "- \\(s_i \\approx 0\\): near a boundary\n",
    "- \\(s_i < 0\\): probably misclassified\n",
    "\n",
    "Overall silhouette = mean of \\(s_i\\).\n",
    "Choose \\(K\\) that maximizes this value.\n",
    "\n",
    "---\n",
    "\n",
    "## Strengths & Weaknesses\n",
    "\n",
    "| ‚úÖ Strengths | ‚ö†Ô∏è Weaknesses |\n",
    "|--------------|---------------|\n",
    "| Fast and easy to implement | Assumes spherical clusters |\n",
    "| Works well for large, dense datasets | Sensitive to initialization and outliers |\n",
    "| Scales well with dimensions (after scaling) | Requires predefined K |\n",
    "| Interpretable centroids | Only finds convex clusters |\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use\n",
    "\n",
    "Use K-Means when clusters are roughly:\n",
    "- compact, convex, and similar in size,\n",
    "- distances are meaningful (e.g., Euclidean),\n",
    "- and you want fast, scalable results.\n",
    "\n",
    "Avoid K-Means for:\n",
    "- elongated or non-convex shapes (use DBSCAN or Spectral instead),\n",
    "- categorical or sparse data without a proper metric,\n",
    "- or data with strong outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "K-Means alternates between:\n",
    "- **Assignment:** move points to the nearest centroid.\n",
    "- **Update:** move centroids to the mean of assigned points.\n",
    "\n",
    "K-Means++ improves initialization.\n",
    "Mini-Batch accelerates convergence on large datasets.\n",
    "Elbow and Silhouette help estimate a good K.\n",
    "Convergence ensures all points are closest to their own centroid.\n",
    "\n"
   ],
   "id": "f22532c007e12768"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Elbow ‚Üí ‚ÄúHow much better does the fit get as I add more clusters?‚Äù\n",
    "\n",
    "Silhouette ‚Üí ‚ÄúAre clusters actually distinct and well-separated?‚Äù\n",
    "\n",
    "In practice:\n",
    "\n",
    "Start with Elbow for a rough range, then use Silhouette to fine-tune\n",
    "ùêæ\n",
    "K."
   ],
   "id": "930f8e9ca62545eb"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
