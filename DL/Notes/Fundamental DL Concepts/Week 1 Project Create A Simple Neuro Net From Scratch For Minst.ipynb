{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸš€ Project: Deep Learning From Scratch - NumPy MNIST Classifier\n",
    "\n",
    "**Goal:** Implement a fully functional, two-layer Neural Network using only Python and NumPy to classify the 10 digits (0-9) of the MNIST dataset.\n",
    "\n",
    "**Objective:** Achieve $\\approx 92\\%$ accuracy on the test set. Mastery of this project ensures deep understanding of all core DL concepts: Forward Propagation, Cross-Entropy Loss, and Backpropagation (Gradient Descent).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Implementation Stages\n",
    "\n",
    "### 1. Data Acquisition and Preprocessing\n",
    "\n",
    "The network expects vectorized, scaled, and correctly encoded data.\n",
    "\n",
    "* **1.1. Data Loading:** Load the training and testing data (e.g., via `numpy.load` or a dedicated library like `python-mnist`).\n",
    "* **1.2. Flattening and Reshaping:** Convert the $28 \\times 28$ input images into a $784$-element feature vector. Ensure the data matrix $X$ has a shape of $(\\text{features}, \\text{samples})$ or $(\\text{samples}, \\text{features})$ and be consistent.\n",
    "* **1.3. Normalization:** Scale pixel values from $[0, 255]$ to $[0, 1]$.\n",
    "* **1.4. One-Hot Encoding:** Convert integer labels (0-9) into 10-dimensional vectors ($Y$).\n",
    "\n",
    "### 2. Network Architecture & Parameter Initialization\n",
    "\n",
    "Define the structure and starting values for the learnable parameters.\n",
    "\n",
    "* **2.1. Define Structure:** Set the size of the Input ($\\text{N}_{in} = 784$), Hidden Layer ($\\text{N}_h$, e.g., 128), and Output Layer ($\\text{N}_{out} = 10$).\n",
    "* **2.2. Initialize $W$ and $b$:** Create two weight matrices ($W_1, W_2$) and two bias vectors ($b_1, b_2$) using small random numbers (e.g., `np.random.randn(...) * 0.01`).\n",
    "\n",
    "### 3. Core Mathematical Functions\n",
    "\n",
    "Implement the engine of the neural network.\n",
    "\n",
    "* **3.1. Activation Functions:**\n",
    "    * **ReLU:** $A = \\max(0, Z)$. Implement its derivative, $g'(Z)$.\n",
    "    * **Softmax:** Used on the output layer for multiclass probability $\\hat{Y}$.\n",
    "* **3.2. Forward Propagation:** Implement the function to calculate:\n",
    "    $$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "    $$A^{[1]} = \\text{ReLU}(Z^{[1]})$$\n",
    "    $$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
    "    $$\\hat{Y} = A^{[2]} = \\text{Softmax}(Z^{[2]})$$\n",
    "* **3.3. Loss Function:** Implement the **Cross-Entropy Loss** calculation between $Y$ (true labels) and $\\hat{Y}$ (predictions).\n",
    "\n",
    "### 4. Backpropagation and Gradient Update\n",
    "\n",
    "This is where the learning happensâ€”the crucial part of the project.\n",
    "\n",
    "* **4.1. Backward Propagation:** Implement the function to calculate the gradients of the Loss with respect to all parameters ($\\frac{\\partial L}{\\partial W^{[1]}}, \\frac{\\partial L}{\\partial b^{[1]}}, \\frac{\\partial L}{\\partial W^{[2]}}, \\frac{\\partial L}{\\partial b^{[2]}}$) using the Chain Rule.\n",
    "* **4.2. Parameter Update:** Update the parameters using the Gradient Descent rule with a defined $\\text{learning\\_rate} \\ (\\alpha)$:\n",
    "    $$W \\leftarrow W - \\alpha \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "### 5. Training Loop and Evaluation\n",
    "\n",
    "The final assembly of the components into a running training script.\n",
    "\n",
    "* **5.1. The Training Loop:** Iterate over a fixed number of **epochs**. Within each epoch, execute steps 3.2, 3.3, 4.1, and 4.2.\n",
    "* **5.2. Prediction and Accuracy:** Create a function to convert the output probabilities ($\\hat{Y}$) back into a class prediction and calculate the model's classification **accuracy**.\n",
    "* **5.3. Tracking:** Log the loss and accuracy metrics every $X$ epochs to visualize the learning process.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Metrics to Track:**\n",
    "* **Learning Rate** (e.g., $0.01$)\n",
    "* **Number of Epochs** (e.g., 100-500)\n",
    "* **Final Test Accuracy** (Target $\\ge 92\\%$)"
   ],
   "id": "24acfbaace205ee6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T00:07:44.768774Z",
     "start_time": "2025-12-17T00:07:34.429014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy\n",
    "# Part 1 Data Loading and Preparation (using sklearn as helper)\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split # Crucial for shuffling and splitting!\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# --- 1. Load Full MNIST Data (28x28 = 784 features) ---\n",
    "# X will be a NumPy array (due to astype)\n",
    "# y will be a Pandas Series\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X = mnist.data.astype(np.float64)  # Features (70000, 784)\n",
    "y = mnist.target.astype(np.int64) # Targets (70000,) - This is a Pandas Series!\n",
    "\n",
    "print(\"--- Initial Data Shapes (Full MNIST) ---\")\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Targets (y): {y.shape}\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Train-Test Split with Shuffling and Stratification ---\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "# X_train_raw/X_test_raw are NumPy arrays.\n",
    "# y_train_raw/y_test_raw are Pandas Series objects.\n",
    "\n",
    "\n",
    "# --- 3. Feature Scaling (Normalization) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "\n",
    "# --- 4. Target Encoding (One-Hot) ---\n",
    "\n",
    "# *** FIX IS HERE ***\n",
    "# Convert the Pandas Series (y_train_raw/y_test_raw) to a NumPy array using .to_numpy()\n",
    "y_train_reshaped = y_train_raw.to_numpy().reshape(-1, 1)\n",
    "y_test_reshaped = y_test_raw.to_numpy().reshape(-1, 1)\n",
    "# *******************\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(y_train_reshaped)\n",
    "\n",
    "Y_train_encoded = encoder.transform(y_train_reshaped)\n",
    "Y_test_encoded = encoder.transform(y_test_reshaped)\n",
    "\n",
    "\n",
    "# --- 5. CRUCIAL STEP: Apply DL Convention Transpose ---\n",
    "# X: (784, M) features x samples\n",
    "# Y: (10, M) classes x samples\n",
    "\n",
    "X_train_DL = X_train_scaled.T\n",
    "X_test_DL = X_test_scaled.T\n",
    "\n",
    "Y_train_DL = Y_train_encoded.T\n",
    "Y_test_DL = Y_test_encoded.T\n",
    "\n",
    "\n",
    "# --- Final Verification ---\n",
    "print(\"--- Final Shapes for DL Implementation (MNIST 784) ---\")\n",
    "M_train = X_train_DL.shape[1]\n",
    "M_test = X_test_DL.shape[1]\n",
    "print(f\"Total Training Samples (M): {M_train}\") # Should be 59500 (70000 * 0.85)\n",
    "print(f\"Total Test Samples (M): {M_test}\\n\")   # Should be 10500 (70000 * 0.15)\n",
    "print(f\"X_train (Features, Samples): {X_train_DL.shape}\") # Should be (784, 59500)\n",
    "print(f\"Y_train (Classes, Samples):  {Y_train_DL.shape}\")  # Should be (10, 59500)"
   ],
   "id": "136b4618d7c9acac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Data Shapes (Full MNIST) ---\n",
      "Features (X): (70000, 784)\n",
      "Targets (y): (70000,)\n",
      "\n",
      "--- Final Shapes for DL Implementation (MNIST 784) ---\n",
      "Total Training Samples (M): 59500\n",
      "Total Test Samples (M): 10500\n",
      "\n",
      "X_train (Features, Samples): (784, 59500)\n",
      "Y_train (Classes, Samples):  (10, 59500)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:55:45.491985Z",
     "start_time": "2025-12-16T23:55:45.351353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ],
   "id": "e71b39da0f432c11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:55:48.134027Z",
     "start_time": "2025-12-16T23:55:48.106907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional\n",
    "\n",
    "NDArray = np.ndarray\n",
    "LEAKY_SLOPE = 0.01\n",
    "\n",
    "class BasicComponent(ABC):\n",
    "    \"\"\"Abstract base class for a computational graph node with minimal caching.\"\"\"\n",
    "\n",
    "    # Stores the input to the component (A_prev or Z) for use in the backward pass.\n",
    "    input_cache: Optional[NDArray] = None\n",
    "\n",
    "    # Derivative of Loss w.r.t input (dL/dA_prev or dL/dZ) - generally not stored here\n",
    "    # but passed along the backward chain.\n",
    "\n",
    "    # Graph Pointers (Crucial for connecting the network structure)\n",
    "    prev: Union['BasicComponent', None] = None\n",
    "    next: Union['BasicComponent', None] = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, input_data: NDArray) -> NDArray:\n",
    "        \"\"\"Performs the forward pass, caches input, and returns output.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, dL_doutput: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Calculates dL/dInput and returns it to the previous layer.\n",
    "        dL_doutput is the incoming derivative from the *next* component.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# --- 1. Activation Component: Leaky ReLU ---\n",
    "class ReLU(BasicComponent):\n",
    "    \"\"\"\n",
    "    Leaky ReLU Activation (A = max(0, Z) or alpha * Z).\n",
    "    \"\"\"\n",
    "    def __init__(self, prev_component: Optional[BasicComponent] = None):\n",
    "        self.prev = prev_component\n",
    "\n",
    "    def forward(self, Z: NDArray) -> NDArray:\n",
    "        \"\"\"A = Leaky ReLU(Z). Caches Z.\"\"\"\n",
    "        self.input_cache = Z # Cache Z (the input)\n",
    "        A = np.where(Z > 0, Z, Z * LEAKY_SLOPE)\n",
    "        return A\n",
    "\n",
    "    def backward(self, dL_dA: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Calculates dL/dZ = dL/dA * dA/dZ and returns dL/dZ.\n",
    "        \"\"\"\n",
    "        Z = self.input_cache\n",
    "\n",
    "        # dA/dZ mask: 1 where Z > 0, alpha where Z <= 0\n",
    "        dA_dZ = np.where(Z > 0, 1.0, LEAKY_SLOPE)\n",
    "\n",
    "        # Element-wise multiplication\n",
    "        dL_dZ = dL_dA * dA_dZ\n",
    "\n",
    "        # NOTE: We return the derivative directly instead of storing it\n",
    "        return dL_dZ\n",
    "\n",
    "# --- 2. Output/Loss Component: Softmax + CrossEntropy ---\n",
    "class SoftmaxCrossEntropy(BasicComponent):\n",
    "    \"\"\"\n",
    "    Combines Softmax activation and Cross-Entropy Loss.\n",
    "    Requires y_true for the derivative calculation.\n",
    "    \"\"\"\n",
    "    def __init__(self, y_true: NDArray, prev_component: Optional[BasicComponent] = None):\n",
    "        self.prev = prev_component\n",
    "        self.y_true = y_true # True labels (10, M)\n",
    "        self.Y_pred: Optional[NDArray] = None # We MUST cache Y_pred for the backward pass\n",
    "\n",
    "    def forward(self, Z: NDArray) -> NDArray:\n",
    "        \"\"\"Calculates stable Softmax probabilities (Y_pred).\"\"\"\n",
    "        self.input_cache = Z\n",
    "\n",
    "        # Numerical Stability\n",
    "        Z_shifted = Z - np.max(Z, axis=0, keepdims=True)\n",
    "        exp_Z = np.exp(Z_shifted)\n",
    "\n",
    "        # Softmax\n",
    "        A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "        self.Y_pred = A\n",
    "        return A # Returns Y_pred\n",
    "\n",
    "    def backward(self, dL_doutput: NDArray = None) -> NDArray:\n",
    "        \"\"\"\n",
    "        Calculates dL/dZ = Y_pred - Y_true.\n",
    "        \"\"\"\n",
    "        # The crucial simplified gradient for the combined component\n",
    "        dL_dZ = self.Y_pred - self.y_true\n",
    "\n",
    "        # Average over the number of samples (M) for the overall gradient\n",
    "        M = self.Y_pred.shape[1]\n",
    "        dL_dZ_avg = dL_dZ / M\n",
    "\n",
    "        # NOTE: We return the derivative (dL/dZ) directly\n",
    "        return dL_dZ_avg\n",
    "\n",
    "class Linear(BasicComponent):\n",
    "    \"\"\"\n",
    "    Linear/Affine Transformation: Z = W * A_prev + b\n",
    "    Manages W, b, and their gradients (dW, db).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int, prev_component: Optional[BasicComponent] = None):\n",
    "        self.prev = prev_component\n",
    "\n",
    "        # He Initialization\n",
    "        self.W = np.random.randn(output_size, input_size) * np.sqrt(2.0/input_size)\n",
    "        # b: (output_size, 1)\n",
    "        self.b = np.zeros((output_size, 1))\n",
    "\n",
    "        # Gradient Accumulators (dW and db will be accumulated in backward pass)\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.db = np.zeros(self.b.shape)\n",
    "\n",
    "    def forward(self, A_prev: NDArray) -> NDArray:\n",
    "        \"\"\"Z = W * A_prev + b. Caches A_prev.\"\"\"\n",
    "        self.input_cache = A_prev # A_prev has shape (input_size, M)\n",
    "\n",
    "        # Matrix multiplication\n",
    "        Z = self.W @ A_prev + self.b\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dL_dZ: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Calculates dL/dW, dL/db (accumulated), and dL/dA_prev (passed back).\n",
    "        dL_dZ is the incoming derivative from the next component.\n",
    "        \"\"\"\n",
    "        A_prev = self.input_cache # (input_size, M)\n",
    "\n",
    "        # 1. Gradient w.r.t Bias (dL/db)\n",
    "        # Sum dL/dZ across all samples (axis=1).\n",
    "        # ACCUMULATE: += is used because this gradient is summed over all samples in the mini-batch\n",
    "        # before a single update step.\n",
    "        db = np.sum(dL_dZ, axis=1, keepdims=True)\n",
    "        self.db += db\n",
    "\n",
    "        # 2. Gradient w.r.t Weights (dL/dW)\n",
    "        # dL/dW = dL/dZ @ (A_prev)^T\n",
    "        # ACCUMULATE: += is used for the same reason as db.\n",
    "        dW = dL_dZ @ A_prev.T\n",
    "        self.dW += dW\n",
    "\n",
    "        # 3. Gradient w.r.t Input (dL/dA_prev)\n",
    "        # dL/dA_prev = W^T @ dL/dZ.\n",
    "        # NO ACCUMULATION: This is an intermediate error signal (not a parameter gradient)\n",
    "        # and is passed directly to the previous layer.\n",
    "        dL_dA_prev = self.W.T @ dL_dZ\n",
    "\n",
    "        return dL_dA_prev\n",
    "\n",
    "    def reset_gradients(self):\n",
    "        \"\"\"Resets accumulated dW and db to zero for the next mini-batch/epoch.\"\"\"\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.db = np.zeros(self.b.shape)"
   ],
   "id": "290eca7fe1d99383",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:59:26.414925Z",
     "start_time": "2025-12-16T23:56:01.111477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Training ---\n",
    "# Full Batch Training - No need to reset_gradients inside the loop if using full batch\n",
    "# and not accumulating across epochs, but it's good practice.\n",
    "\n",
    "iteration = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# --- Setup Graph ---\n",
    "linear_1 = Linear(784, 256, prev_component=None)\n",
    "relu_1 = ReLU(prev_component=linear_1)\n",
    "linear_2 = Linear(256, 10, prev_component=relu_1)\n",
    "\n",
    "# Note: y_true is set once for the entire training set\n",
    "softmax_1 = SoftmaxCrossEntropy(prev_component=linear_2, y_true=Y_train_DL)\n",
    "\n",
    "# Get total test samples for accurate calculation\n",
    "M_test = Y_test_DL.shape[1]\n",
    "\n",
    "for i in range(iteration):\n",
    "    # --- Forward Pass ---\n",
    "    # The result of the final forward pass is stored internally in softmax_1.Y_pred\n",
    "    y_pred = softmax_1.forward(\n",
    "        linear_2.forward(\n",
    "            relu_1.forward(\n",
    "                linear_1.forward(X_train_DL)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Backward Pass ---\n",
    "    # Starts with softmax_1.backward() which returns dL/dZ_L2\n",
    "    # The subsequent calls pass the derivative backward\n",
    "    dL_dA1 = linear_2.backward(softmax_1.backward())\n",
    "    dL_dZ1 = relu_1.backward(dL_dA1)\n",
    "    linear_1.backward(dL_dZ1) # Final backward call consumes dL_dZ1\n",
    "\n",
    "    # --- Parameter Update ---\n",
    "    # The accumulated dW/db are used for the update\n",
    "    linear_1.b -= learning_rate * linear_1.db\n",
    "    linear_1.W -= learning_rate * linear_1.dW\n",
    "    linear_2.b -= learning_rate * linear_2.db\n",
    "    linear_2.W -= learning_rate * linear_2.dW\n",
    "\n",
    "    # Reset gradients after the update (necessary if running multiple passes per epoch,\n",
    "    # but still clean practice)\n",
    "    linear_1.reset_gradients()\n",
    "    linear_2.reset_gradients()\n",
    "\n",
    "    # --- Accuracy Calculation (Corrected) ---\n",
    "    # 1. Forward pass for TEST data (no caching, no backward)\n",
    "    y_test_pred = softmax_1.forward(\n",
    "        linear_2.forward(\n",
    "            relu_1.forward(\n",
    "                linear_1.forward(X_test_DL)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2. Get the index of the highest probability (0-9) for predicted and true\n",
    "    y_test_pred_indices = np.argmax(y_test_pred, axis=0)\n",
    "    y_test_true_indices = np.argmax(Y_test_DL, axis=0)\n",
    "\n",
    "    # 3. Calculate accuracy: count matches and divide by total samples\n",
    "    correct_predictions = (y_test_pred_indices == y_test_true_indices).sum()\n",
    "    accuracy = correct_predictions / M_test * 100.0\n",
    "\n",
    "    print(f\"Accuracy for iteration {i}: {accuracy:.2f}%\")"
   ],
   "id": "c2ecbecbbaacf48b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for iteration 0: 15.83%\n",
      "Accuracy for iteration 1: 17.86%\n",
      "Accuracy for iteration 2: 20.11%\n",
      "Accuracy for iteration 3: 22.31%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 53\u001B[39m\n\u001B[32m     47\u001B[39m linear_2.reset_gradients()\n\u001B[32m     49\u001B[39m \u001B[38;5;66;03m# --- Accuracy Calculation (Corrected) ---\u001B[39;00m\n\u001B[32m     50\u001B[39m \u001B[38;5;66;03m# 1. Forward pass for TEST data (no caching, no backward)\u001B[39;00m\n\u001B[32m     51\u001B[39m y_test_pred = softmax_1.forward(\n\u001B[32m     52\u001B[39m     linear_2.forward(\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m         \u001B[43mrelu_1\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m            \u001B[49m\u001B[43mlinear_1\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test_DL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     56\u001B[39m     )\n\u001B[32m     57\u001B[39m )\n\u001B[32m     59\u001B[39m \u001B[38;5;66;03m# 2. Get the index of the highest probability (0-9) for predicted and true\u001B[39;00m\n\u001B[32m     60\u001B[39m y_test_pred_indices = np.argmax(y_test_pred, axis=\u001B[32m0\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 42\u001B[39m, in \u001B[36mReLU.forward\u001B[39m\u001B[34m(self, Z)\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, prev_component: Optional[BasicComponent] = \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m     40\u001B[39m     \u001B[38;5;28mself\u001B[39m.prev = prev_component\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, Z: NDArray) -> NDArray:\n\u001B[32m     43\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"A = Leaky ReLU(Z). Caches Z.\"\"\"\u001B[39;00m\n\u001B[32m     44\u001B[39m     \u001B[38;5;28mself\u001B[39m.input_cache = Z \u001B[38;5;66;03m# Cache Z (the input)\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:43:08.450173Z",
     "start_time": "2025-12-16T16:08:37.789480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Configuration ---\n",
    "# Assuming NDArray, Linear, ReLU, SoftmaxCrossEntropy, and the DL data variables\n",
    "# (X_train_DL, Y_train_DL, X_test_DL, Y_test_DL) are already defined.\n",
    "\n",
    "# --- Define Hyperparameters ---\n",
    "iteration = 1000 # Increased iterations for better results\n",
    "learning_rate = 0.01\n",
    "M_test = X_test_DL.shape[1] # Number of test samples\n",
    "\n",
    "# --- 1. Define 3-Layer Architecture (784 -> 256 -> 128 -> 10) ---\n",
    "print(\"--- Creating 3-Layer Network Structure ---\")\n",
    "# Layer 1: Input (784) -> Hidden (256)\n",
    "linear_1 = Linear(784, 256, prev_component=None)\n",
    "relu_1 = ReLU(prev_component=linear_1)\n",
    "\n",
    "# Layer 2: Hidden (256) -> Hidden (128)\n",
    "linear_2 = Linear(256, 128, prev_component=relu_1)\n",
    "relu_2 = ReLU(prev_component=linear_2)\n",
    "\n",
    "# Layer 3: Hidden (128) -> Output (10)\n",
    "linear_3 = Linear(128, 10, prev_component=relu_2)\n",
    "softmax_1 = SoftmaxCrossEntropy(prev_component=linear_3, y_true=Y_train_DL)\n",
    "\n",
    "# List of all trainable components for easy iteration\n",
    "trainable_components = [linear_1, linear_2, linear_3]\n",
    "print(f\"Network defined: {linear_1.__class__.__name__} -> ... -> {linear_3.__class__.__name__} -> Softmax\")\n",
    "\n",
    "# --- 2. Training Loop ---\n",
    "for i in range(iteration):\n",
    "    # 2.1. Reset Gradients for the new epoch/batch\n",
    "    for comp in trainable_components:\n",
    "        comp.reset_gradients()\n",
    "\n",
    "    # 2.2. Forward Pass (Training Data)\n",
    "    z1 = linear_1.forward(X_train_DL)\n",
    "    a1 = relu_1.forward(z1)\n",
    "    z2 = linear_2.forward(a1)\n",
    "    a2 = relu_2.forward(z2)      # New Layer\n",
    "    z3 = linear_3.forward(a2)    # New Layer\n",
    "    y_pred = softmax_1.forward(z3)\n",
    "\n",
    "    # 2.3. Backward Pass (Reverse Chain)\n",
    "    dL_dZ3 = softmax_1.backward()\n",
    "    dL_dA2 = linear_3.backward(dL_dZ3)\n",
    "    dL_dZ2 = relu_2.backward(dL_dA2)        # New Layer\n",
    "    dL_dA1 = linear_2.backward(dL_dZ2)\n",
    "    dL_dZ1 = relu_1.backward(dL_dA1)\n",
    "    linear_1.backward(dL_dZ1)\n",
    "\n",
    "    # 2.4. Parameter Update (Gradient Descent)\n",
    "    for comp in trainable_components:\n",
    "        comp.b -= learning_rate * comp.db\n",
    "        comp.W -= learning_rate * comp.dW\n",
    "\n",
    "    # 2.5. Accuracy Calculation (Test Data)\n",
    "    # Forward Pass on TEST data (must replicate the full chain for evaluation)\n",
    "    test_z1 = linear_1.forward(X_test_DL)\n",
    "    test_a1 = relu_1.forward(test_z1)\n",
    "    test_z2 = linear_2.forward(test_a1)\n",
    "    test_a2 = relu_2.forward(test_z2)\n",
    "    test_z3 = linear_3.forward(test_a2)\n",
    "    y_test_pred = softmax_1.forward(test_z3)\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    y_test_pred_indices = np.argmax(y_test_pred, axis=0)\n",
    "    y_test_true_indices = np.argmax(Y_test_DL, axis=0)\n",
    "    correct_predictions = (y_test_pred_indices == y_test_true_indices).sum()\n",
    "    accuracy = correct_predictions / M_test * 100.0\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Accuracy for iteration {i}: {accuracy:.2f}%\")\n",
    "\n",
    "print(f\"\\nFinal Accuracy after {iteration} iterations: {accuracy:.2f}%\")"
   ],
   "id": "1d5287bdee0d2b57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating 3-Layer Network Structure ---\n",
      "Network defined: Linear -> ... -> Linear -> Softmax\n",
      "Accuracy for iteration 0: 10.10%\n",
      "Accuracy for iteration 50: 66.89%\n",
      "Accuracy for iteration 100: 78.00%\n",
      "Accuracy for iteration 150: 82.55%\n",
      "Accuracy for iteration 200: 84.90%\n",
      "Accuracy for iteration 250: 86.42%\n",
      "Accuracy for iteration 300: 87.27%\n",
      "Accuracy for iteration 350: 87.99%\n",
      "Accuracy for iteration 400: 88.73%\n",
      "Accuracy for iteration 450: 89.30%\n",
      "Accuracy for iteration 500: 89.55%\n",
      "Accuracy for iteration 550: 89.87%\n",
      "Accuracy for iteration 600: 90.24%\n",
      "Accuracy for iteration 650: 90.57%\n",
      "Accuracy for iteration 700: 90.88%\n",
      "Accuracy for iteration 750: 91.13%\n",
      "Accuracy for iteration 800: 91.39%\n",
      "Accuracy for iteration 850: 91.56%\n",
      "Accuracy for iteration 900: 91.70%\n",
      "Accuracy for iteration 950: 91.78%\n",
      "\n",
      "Final Accuracy after 1000 iterations: 91.90%\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPU Implementation with torch library",
   "id": "d04e07d289fb7ae4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T00:52:57.052233Z",
     "start_time": "2025-12-17T00:52:46.795063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 0. Define Device ---\n",
    "# Check if CUDA is available and set the device object accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Using device: {device} ---\")\n",
    "\n",
    "# --- 1. Data Loading and Splitting (CPU/NumPy Stage) ---\n",
    "\n",
    "# Load Full MNIST Data (70000 samples, 784 features)\n",
    "# parser='auto' is required for modern sklearn versions\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X = mnist.data.astype(np.float32) # Use float32 to match torch.float default\n",
    "y = mnist.target.astype(np.int64)\n",
    "\n",
    "# Train-Test Split (15% test set)\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature Scaling (Normalization) - Keeps data in NumPy/CPU\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "# --- 2. Conversion and Transfer (CPU/NumPy -> GPU/PyTorch) ---\n",
    "\n",
    "# PyTorch tensors for features must be float\n",
    "# The shape is (M, 784) - PyTorch prefers (Samples, Features) for Linear layers\n",
    "X_train_DL_GPU = torch.from_numpy(X_train_scaled).float().to(device)\n",
    "X_test_DL_GPU = torch.from_numpy(X_test_scaled).float().to(device)\n",
    "\n",
    "# Labels (Y) must be LongTensors of class indices (0-9) for CrossEntropyLoss\n",
    "# y_train_raw/y_test_raw are already simple index arrays from fetch_openml\n",
    "# We cast them to int64 (NumPy) then to LongTensor (PyTorch)\n",
    "Y_train_DL_GPU = torch.from_numpy(y_train_raw.to_numpy()).long().to(device)\n",
    "Y_test_DL_GPU = torch.from_numpy(y_test_raw.to_numpy()).long().to(device)\n",
    "\n",
    "# --- 3. Verification ---\n",
    "print(\"\\n--- Final GPU Tensor Check ---\")\n",
    "print(f\"X_train shape: {X_train_DL_GPU.shape}, Device: {X_train_DL_GPU.device}, Dtype: {X_train_DL_GPU.dtype}\")\n",
    "print(f\"Y_train shape: {Y_train_DL_GPU.shape}, Device: {Y_train_DL_GPU.device}, Dtype: {Y_train_DL_GPU.dtype}\")"
   ],
   "id": "b764d5efddf3e12c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using device: cuda:0 ---\n",
      "\n",
      "--- Final GPU Tensor Check ---\n",
      "X_train shape: torch.Size([59500, 784]), Device: cuda:0, Dtype: torch.float32\n",
      "Y_train shape: torch.Size([59500]), Device: cuda:0, Dtype: torch.int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T01:14:34.858690Z",
     "start_time": "2025-12-17T01:14:27.119690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Assuming device is defined (e.g., torch.device(\"cuda:0\"))\n",
    "# Assuming X_train_DL_GPU and Y_train_DL_GPU are your GPU Tensors\n",
    "\n",
    "# --- 1. Corrected Model Definition (3-Layer Architecture) ---\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # We are using a 3-layer architecture (784 -> 256 -> 128 -> 10)\n",
    "    # The output of the final Linear layer is the raw scores (logits)\n",
    "    def __init__(self):\n",
    "        super().__init__() # CRITICAL FIX: Correctly call the parent constructor\n",
    "        self.architecture = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            # Layer 2\n",
    "            nn.Linear(256, 128), # Added a 128-neuron hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            # Layer 3 (Output)\n",
    "            nn.Linear(128, 10)   # Output layer produces 10 logits (raw scores)\n",
    "            # Softmax is NOT included here, as CrossEntropyLoss handles it internally\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.architecture(x)\n",
    "\n",
    "# --- 2. Training Setup ---\n",
    "epoch = 1000\n",
    "learning_rate = 0.1\n",
    "ADAM_LR = 0.001\n",
    "\n",
    "model = NeuralNetwork().to(device) # Move the entire model to GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate) # Use PyTorch's SGD optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=ADAM_LR)\n",
    "\n",
    "# --- 3. Training Loop with Correct Accuracy Calculation ---\n",
    "for i in range(epoch):\n",
    "    # Set the model to training mode (important for later layers like Dropout/BatchNorm)\n",
    "    model.train()\n",
    "\n",
    "    # 1. Zero Gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 2. Forward Pass (on GPU data)\n",
    "    logits = model(X_train_DL_GPU)\n",
    "\n",
    "    # 3. Compute Loss\n",
    "    # Criterion expects (Logits, True Indices)\n",
    "    loss = criterion(logits, Y_train_DL_GPU)\n",
    "\n",
    "    # 4. Backward Pass (Autograd calculates all gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer Step (Updates all parameters using the computed gradients and LR)\n",
    "    optimizer.step()\n",
    "\n",
    "    # --- ACCURACY CALCULATION (The Answer to your question) ---\n",
    "    model.eval() # Set the model to evaluation mode (e.g., disables dropout)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # 1. TRAIN Accuracy (Your current calculation)\n",
    "        logits_train = model(X_train_DL_GPU)\n",
    "        _, predicted_train = torch.max(logits_train.data, 1)\n",
    "        train_accuracy = 100 * (predicted_train == Y_train_DL_GPU).sum().item() / Y_train_DL_GPU.size(0)\n",
    "\n",
    "        # 2. TEST Accuracy (The critical new metric)\n",
    "        logits_test = model(X_test_DL_GPU)\n",
    "        _, predicted_test = torch.max(logits_test.data, 1)\n",
    "        test_accuracy = 100 * (predicted_test == Y_test_DL_GPU).sum().item() / Y_test_DL_GPU.size(0)\n",
    "\n",
    "    # Set model back to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Printing the key metrics\n",
    "    print(f\"Epoch {i+1:03d} | Loss: {loss.item():.4f} | Train Acc: {train_accuracy:.2f}% | Test Acc: {test_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Complete. Final Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "ac5f3ce401f8424a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 2.3212 | Train Acc: 34.96% | Test Acc: 35.60%\n",
      "Epoch 002 | Loss: 2.2243 | Train Acc: 59.98% | Test Acc: 60.19%\n",
      "Epoch 003 | Loss: 2.1322 | Train Acc: 68.82% | Test Acc: 69.06%\n",
      "Epoch 004 | Loss: 2.0302 | Train Acc: 71.81% | Test Acc: 71.90%\n",
      "Epoch 005 | Loss: 1.9174 | Train Acc: 72.43% | Test Acc: 72.47%\n",
      "Epoch 006 | Loss: 1.7967 | Train Acc: 72.08% | Test Acc: 72.13%\n",
      "Epoch 007 | Loss: 1.6685 | Train Acc: 71.85% | Test Acc: 71.72%\n",
      "Epoch 008 | Loss: 1.5341 | Train Acc: 71.98% | Test Acc: 71.80%\n",
      "Epoch 009 | Loss: 1.4073 | Train Acc: 72.45% | Test Acc: 72.17%\n",
      "Epoch 010 | Loss: 1.2877 | Train Acc: 73.46% | Test Acc: 73.11%\n",
      "Epoch 011 | Loss: 1.1752 | Train Acc: 74.84% | Test Acc: 74.84%\n",
      "Epoch 012 | Loss: 1.0780 | Train Acc: 76.56% | Test Acc: 76.51%\n",
      "Epoch 013 | Loss: 0.9899 | Train Acc: 78.21% | Test Acc: 78.00%\n",
      "Epoch 014 | Loss: 0.9133 | Train Acc: 79.80% | Test Acc: 79.52%\n",
      "Epoch 015 | Loss: 0.8470 | Train Acc: 81.27% | Test Acc: 81.09%\n",
      "Epoch 016 | Loss: 0.7938 | Train Acc: 82.55% | Test Acc: 82.28%\n",
      "Epoch 017 | Loss: 0.7442 | Train Acc: 83.77% | Test Acc: 83.38%\n",
      "Epoch 018 | Loss: 0.7053 | Train Acc: 84.69% | Test Acc: 84.37%\n",
      "Epoch 019 | Loss: 0.6742 | Train Acc: 85.56% | Test Acc: 85.31%\n",
      "Epoch 020 | Loss: 0.6455 | Train Acc: 86.23% | Test Acc: 85.99%\n",
      "Epoch 021 | Loss: 0.6211 | Train Acc: 86.83% | Test Acc: 86.42%\n",
      "Epoch 022 | Loss: 0.6022 | Train Acc: 87.32% | Test Acc: 86.83%\n",
      "Epoch 023 | Loss: 0.5784 | Train Acc: 87.70% | Test Acc: 87.24%\n",
      "Epoch 024 | Loss: 0.5606 | Train Acc: 88.17% | Test Acc: 87.68%\n",
      "Epoch 025 | Loss: 0.5397 | Train Acc: 88.58% | Test Acc: 88.08%\n",
      "Epoch 026 | Loss: 0.5221 | Train Acc: 88.90% | Test Acc: 88.41%\n",
      "Epoch 027 | Loss: 0.5046 | Train Acc: 89.22% | Test Acc: 88.72%\n",
      "Epoch 028 | Loss: 0.4916 | Train Acc: 89.55% | Test Acc: 89.03%\n",
      "Epoch 029 | Loss: 0.4740 | Train Acc: 89.86% | Test Acc: 89.30%\n",
      "Epoch 030 | Loss: 0.4654 | Train Acc: 90.11% | Test Acc: 89.60%\n",
      "Epoch 031 | Loss: 0.4494 | Train Acc: 90.39% | Test Acc: 89.90%\n",
      "Epoch 032 | Loss: 0.4388 | Train Acc: 90.63% | Test Acc: 90.20%\n",
      "Epoch 033 | Loss: 0.4260 | Train Acc: 90.89% | Test Acc: 90.48%\n",
      "Epoch 034 | Loss: 0.4167 | Train Acc: 91.09% | Test Acc: 90.74%\n",
      "Epoch 035 | Loss: 0.4074 | Train Acc: 91.26% | Test Acc: 91.02%\n",
      "Epoch 036 | Loss: 0.4019 | Train Acc: 91.48% | Test Acc: 91.18%\n",
      "Epoch 037 | Loss: 0.3918 | Train Acc: 91.69% | Test Acc: 91.37%\n",
      "Epoch 038 | Loss: 0.3804 | Train Acc: 91.84% | Test Acc: 91.43%\n",
      "Epoch 039 | Loss: 0.3803 | Train Acc: 91.99% | Test Acc: 91.50%\n",
      "Epoch 040 | Loss: 0.3706 | Train Acc: 92.17% | Test Acc: 91.62%\n",
      "Epoch 041 | Loss: 0.3644 | Train Acc: 92.29% | Test Acc: 91.73%\n",
      "Epoch 042 | Loss: 0.3560 | Train Acc: 92.43% | Test Acc: 91.81%\n",
      "Epoch 043 | Loss: 0.3498 | Train Acc: 92.53% | Test Acc: 91.84%\n",
      "Epoch 044 | Loss: 0.3457 | Train Acc: 92.64% | Test Acc: 91.90%\n",
      "Epoch 045 | Loss: 0.3409 | Train Acc: 92.78% | Test Acc: 92.10%\n",
      "Epoch 046 | Loss: 0.3373 | Train Acc: 92.90% | Test Acc: 92.12%\n",
      "Epoch 047 | Loss: 0.3302 | Train Acc: 92.98% | Test Acc: 92.24%\n",
      "Epoch 048 | Loss: 0.3282 | Train Acc: 93.09% | Test Acc: 92.32%\n",
      "Epoch 049 | Loss: 0.3202 | Train Acc: 93.18% | Test Acc: 92.39%\n",
      "Epoch 050 | Loss: 0.3194 | Train Acc: 93.27% | Test Acc: 92.50%\n",
      "Epoch 051 | Loss: 0.3152 | Train Acc: 93.39% | Test Acc: 92.58%\n",
      "Epoch 052 | Loss: 0.3112 | Train Acc: 93.49% | Test Acc: 92.67%\n",
      "Epoch 053 | Loss: 0.3069 | Train Acc: 93.59% | Test Acc: 92.76%\n",
      "Epoch 054 | Loss: 0.3041 | Train Acc: 93.67% | Test Acc: 92.86%\n",
      "Epoch 055 | Loss: 0.2997 | Train Acc: 93.74% | Test Acc: 92.88%\n",
      "Epoch 056 | Loss: 0.2984 | Train Acc: 93.83% | Test Acc: 92.93%\n",
      "Epoch 057 | Loss: 0.2950 | Train Acc: 93.92% | Test Acc: 93.05%\n",
      "Epoch 058 | Loss: 0.2940 | Train Acc: 93.99% | Test Acc: 93.13%\n",
      "Epoch 059 | Loss: 0.2860 | Train Acc: 94.07% | Test Acc: 93.18%\n",
      "Epoch 060 | Loss: 0.2841 | Train Acc: 94.14% | Test Acc: 93.27%\n",
      "Epoch 061 | Loss: 0.2830 | Train Acc: 94.20% | Test Acc: 93.32%\n",
      "Epoch 062 | Loss: 0.2810 | Train Acc: 94.27% | Test Acc: 93.42%\n",
      "Epoch 063 | Loss: 0.2766 | Train Acc: 94.33% | Test Acc: 93.50%\n",
      "Epoch 064 | Loss: 0.2744 | Train Acc: 94.41% | Test Acc: 93.56%\n",
      "Epoch 065 | Loss: 0.2743 | Train Acc: 94.47% | Test Acc: 93.60%\n",
      "Epoch 066 | Loss: 0.2698 | Train Acc: 94.52% | Test Acc: 93.67%\n",
      "Epoch 067 | Loss: 0.2690 | Train Acc: 94.56% | Test Acc: 93.70%\n",
      "Epoch 068 | Loss: 0.2661 | Train Acc: 94.63% | Test Acc: 93.76%\n",
      "Epoch 069 | Loss: 0.2635 | Train Acc: 94.70% | Test Acc: 93.85%\n",
      "Epoch 070 | Loss: 0.2613 | Train Acc: 94.75% | Test Acc: 93.90%\n",
      "Epoch 071 | Loss: 0.2575 | Train Acc: 94.81% | Test Acc: 93.96%\n",
      "Epoch 072 | Loss: 0.2583 | Train Acc: 94.87% | Test Acc: 93.97%\n",
      "Epoch 073 | Loss: 0.2551 | Train Acc: 94.91% | Test Acc: 94.00%\n",
      "Epoch 074 | Loss: 0.2557 | Train Acc: 94.95% | Test Acc: 94.03%\n",
      "Epoch 075 | Loss: 0.2521 | Train Acc: 95.01% | Test Acc: 94.05%\n",
      "Epoch 076 | Loss: 0.2491 | Train Acc: 95.06% | Test Acc: 94.11%\n",
      "Epoch 077 | Loss: 0.2456 | Train Acc: 95.09% | Test Acc: 94.16%\n",
      "Epoch 078 | Loss: 0.2460 | Train Acc: 95.13% | Test Acc: 94.19%\n",
      "Epoch 079 | Loss: 0.2453 | Train Acc: 95.16% | Test Acc: 94.21%\n",
      "Epoch 080 | Loss: 0.2398 | Train Acc: 95.20% | Test Acc: 94.24%\n",
      "Epoch 081 | Loss: 0.2422 | Train Acc: 95.26% | Test Acc: 94.30%\n",
      "Epoch 082 | Loss: 0.2380 | Train Acc: 95.31% | Test Acc: 94.31%\n",
      "Epoch 083 | Loss: 0.2356 | Train Acc: 95.35% | Test Acc: 94.35%\n",
      "Epoch 084 | Loss: 0.2354 | Train Acc: 95.39% | Test Acc: 94.42%\n",
      "Epoch 085 | Loss: 0.2328 | Train Acc: 95.44% | Test Acc: 94.46%\n",
      "Epoch 086 | Loss: 0.2306 | Train Acc: 95.48% | Test Acc: 94.49%\n",
      "Epoch 087 | Loss: 0.2305 | Train Acc: 95.52% | Test Acc: 94.52%\n",
      "Epoch 088 | Loss: 0.2297 | Train Acc: 95.55% | Test Acc: 94.53%\n",
      "Epoch 089 | Loss: 0.2281 | Train Acc: 95.58% | Test Acc: 94.54%\n",
      "Epoch 090 | Loss: 0.2270 | Train Acc: 95.62% | Test Acc: 94.58%\n",
      "Epoch 091 | Loss: 0.2240 | Train Acc: 95.66% | Test Acc: 94.59%\n",
      "Epoch 092 | Loss: 0.2202 | Train Acc: 95.70% | Test Acc: 94.65%\n",
      "Epoch 093 | Loss: 0.2229 | Train Acc: 95.73% | Test Acc: 94.68%\n",
      "Epoch 094 | Loss: 0.2176 | Train Acc: 95.77% | Test Acc: 94.70%\n",
      "Epoch 095 | Loss: 0.2182 | Train Acc: 95.80% | Test Acc: 94.70%\n",
      "Epoch 096 | Loss: 0.2177 | Train Acc: 95.83% | Test Acc: 94.72%\n",
      "Epoch 097 | Loss: 0.2154 | Train Acc: 95.86% | Test Acc: 94.77%\n",
      "Epoch 098 | Loss: 0.2121 | Train Acc: 95.89% | Test Acc: 94.80%\n",
      "Epoch 099 | Loss: 0.2105 | Train Acc: 95.94% | Test Acc: 94.81%\n",
      "Epoch 100 | Loss: 0.2143 | Train Acc: 95.96% | Test Acc: 94.86%\n",
      "Epoch 101 | Loss: 0.2085 | Train Acc: 96.00% | Test Acc: 94.89%\n",
      "Epoch 102 | Loss: 0.2108 | Train Acc: 96.03% | Test Acc: 94.95%\n",
      "Epoch 103 | Loss: 0.2064 | Train Acc: 96.04% | Test Acc: 94.99%\n",
      "Epoch 104 | Loss: 0.2063 | Train Acc: 96.09% | Test Acc: 95.00%\n",
      "Epoch 105 | Loss: 0.2065 | Train Acc: 96.12% | Test Acc: 95.05%\n",
      "Epoch 106 | Loss: 0.2028 | Train Acc: 96.17% | Test Acc: 95.09%\n",
      "Epoch 107 | Loss: 0.2040 | Train Acc: 96.21% | Test Acc: 95.11%\n",
      "Epoch 108 | Loss: 0.1986 | Train Acc: 96.25% | Test Acc: 95.14%\n",
      "Epoch 109 | Loss: 0.1994 | Train Acc: 96.30% | Test Acc: 95.20%\n",
      "Epoch 110 | Loss: 0.2009 | Train Acc: 96.34% | Test Acc: 95.23%\n",
      "Epoch 111 | Loss: 0.1995 | Train Acc: 96.37% | Test Acc: 95.24%\n",
      "Epoch 112 | Loss: 0.1966 | Train Acc: 96.39% | Test Acc: 95.25%\n",
      "Epoch 113 | Loss: 0.1972 | Train Acc: 96.42% | Test Acc: 95.28%\n",
      "Epoch 114 | Loss: 0.1939 | Train Acc: 96.47% | Test Acc: 95.27%\n",
      "Epoch 115 | Loss: 0.1923 | Train Acc: 96.48% | Test Acc: 95.28%\n",
      "Epoch 116 | Loss: 0.1937 | Train Acc: 96.51% | Test Acc: 95.34%\n",
      "Epoch 117 | Loss: 0.1895 | Train Acc: 96.55% | Test Acc: 95.36%\n",
      "Epoch 118 | Loss: 0.1918 | Train Acc: 96.57% | Test Acc: 95.36%\n",
      "Epoch 119 | Loss: 0.1860 | Train Acc: 96.59% | Test Acc: 95.37%\n",
      "Epoch 120 | Loss: 0.1881 | Train Acc: 96.62% | Test Acc: 95.37%\n",
      "Epoch 121 | Loss: 0.1853 | Train Acc: 96.64% | Test Acc: 95.36%\n",
      "Epoch 122 | Loss: 0.1853 | Train Acc: 96.67% | Test Acc: 95.39%\n",
      "Epoch 123 | Loss: 0.1872 | Train Acc: 96.70% | Test Acc: 95.43%\n",
      "Epoch 124 | Loss: 0.1829 | Train Acc: 96.72% | Test Acc: 95.43%\n",
      "Epoch 125 | Loss: 0.1824 | Train Acc: 96.74% | Test Acc: 95.43%\n",
      "Epoch 126 | Loss: 0.1801 | Train Acc: 96.76% | Test Acc: 95.49%\n",
      "Epoch 127 | Loss: 0.1804 | Train Acc: 96.78% | Test Acc: 95.50%\n",
      "Epoch 128 | Loss: 0.1813 | Train Acc: 96.81% | Test Acc: 95.54%\n",
      "Epoch 129 | Loss: 0.1781 | Train Acc: 96.83% | Test Acc: 95.57%\n",
      "Epoch 130 | Loss: 0.1796 | Train Acc: 96.87% | Test Acc: 95.58%\n",
      "Epoch 131 | Loss: 0.1795 | Train Acc: 96.89% | Test Acc: 95.61%\n",
      "Epoch 132 | Loss: 0.1776 | Train Acc: 96.91% | Test Acc: 95.64%\n",
      "Epoch 133 | Loss: 0.1780 | Train Acc: 96.92% | Test Acc: 95.63%\n",
      "Epoch 134 | Loss: 0.1733 | Train Acc: 96.94% | Test Acc: 95.68%\n",
      "Epoch 135 | Loss: 0.1713 | Train Acc: 96.96% | Test Acc: 95.70%\n",
      "Epoch 136 | Loss: 0.1734 | Train Acc: 96.99% | Test Acc: 95.75%\n",
      "Epoch 137 | Loss: 0.1725 | Train Acc: 97.02% | Test Acc: 95.83%\n",
      "Epoch 138 | Loss: 0.1703 | Train Acc: 97.06% | Test Acc: 95.84%\n",
      "Epoch 139 | Loss: 0.1712 | Train Acc: 97.07% | Test Acc: 95.85%\n",
      "Epoch 140 | Loss: 0.1700 | Train Acc: 97.08% | Test Acc: 95.86%\n",
      "Epoch 141 | Loss: 0.1701 | Train Acc: 97.10% | Test Acc: 95.90%\n",
      "Epoch 142 | Loss: 0.1677 | Train Acc: 97.13% | Test Acc: 95.90%\n",
      "Epoch 143 | Loss: 0.1661 | Train Acc: 97.16% | Test Acc: 95.92%\n",
      "Epoch 144 | Loss: 0.1642 | Train Acc: 97.18% | Test Acc: 95.92%\n",
      "Epoch 145 | Loss: 0.1653 | Train Acc: 97.21% | Test Acc: 95.91%\n",
      "Epoch 146 | Loss: 0.1645 | Train Acc: 97.22% | Test Acc: 95.92%\n",
      "Epoch 147 | Loss: 0.1632 | Train Acc: 97.24% | Test Acc: 95.97%\n",
      "Epoch 148 | Loss: 0.1627 | Train Acc: 97.26% | Test Acc: 95.96%\n",
      "Epoch 149 | Loss: 0.1617 | Train Acc: 97.28% | Test Acc: 95.94%\n",
      "Epoch 150 | Loss: 0.1623 | Train Acc: 97.32% | Test Acc: 95.95%\n",
      "Epoch 151 | Loss: 0.1585 | Train Acc: 97.34% | Test Acc: 95.98%\n",
      "Epoch 152 | Loss: 0.1594 | Train Acc: 97.36% | Test Acc: 96.06%\n",
      "Epoch 153 | Loss: 0.1576 | Train Acc: 97.38% | Test Acc: 96.10%\n",
      "Epoch 154 | Loss: 0.1594 | Train Acc: 97.41% | Test Acc: 96.10%\n",
      "Epoch 155 | Loss: 0.1583 | Train Acc: 97.43% | Test Acc: 96.12%\n",
      "Epoch 156 | Loss: 0.1591 | Train Acc: 97.45% | Test Acc: 96.14%\n",
      "Epoch 157 | Loss: 0.1546 | Train Acc: 97.46% | Test Acc: 96.17%\n",
      "Epoch 158 | Loss: 0.1567 | Train Acc: 97.47% | Test Acc: 96.17%\n",
      "Epoch 159 | Loss: 0.1538 | Train Acc: 97.49% | Test Acc: 96.16%\n",
      "Epoch 160 | Loss: 0.1543 | Train Acc: 97.51% | Test Acc: 96.17%\n",
      "Epoch 161 | Loss: 0.1526 | Train Acc: 97.54% | Test Acc: 96.18%\n",
      "Epoch 162 | Loss: 0.1527 | Train Acc: 97.57% | Test Acc: 96.19%\n",
      "Epoch 163 | Loss: 0.1523 | Train Acc: 97.59% | Test Acc: 96.19%\n",
      "Epoch 164 | Loss: 0.1494 | Train Acc: 97.60% | Test Acc: 96.19%\n",
      "Epoch 165 | Loss: 0.1506 | Train Acc: 97.63% | Test Acc: 96.22%\n",
      "Epoch 166 | Loss: 0.1475 | Train Acc: 97.65% | Test Acc: 96.22%\n",
      "Epoch 167 | Loss: 0.1471 | Train Acc: 97.66% | Test Acc: 96.23%\n",
      "Epoch 168 | Loss: 0.1479 | Train Acc: 97.68% | Test Acc: 96.24%\n",
      "Epoch 169 | Loss: 0.1465 | Train Acc: 97.70% | Test Acc: 96.27%\n",
      "Epoch 170 | Loss: 0.1502 | Train Acc: 97.71% | Test Acc: 96.27%\n",
      "Epoch 171 | Loss: 0.1481 | Train Acc: 97.72% | Test Acc: 96.30%\n",
      "Epoch 172 | Loss: 0.1452 | Train Acc: 97.74% | Test Acc: 96.32%\n",
      "Epoch 173 | Loss: 0.1446 | Train Acc: 97.77% | Test Acc: 96.30%\n",
      "Epoch 174 | Loss: 0.1449 | Train Acc: 97.78% | Test Acc: 96.35%\n",
      "Epoch 175 | Loss: 0.1456 | Train Acc: 97.80% | Test Acc: 96.36%\n",
      "Epoch 176 | Loss: 0.1411 | Train Acc: 97.81% | Test Acc: 96.36%\n",
      "Epoch 177 | Loss: 0.1438 | Train Acc: 97.82% | Test Acc: 96.35%\n",
      "Epoch 178 | Loss: 0.1413 | Train Acc: 97.83% | Test Acc: 96.37%\n",
      "Epoch 179 | Loss: 0.1423 | Train Acc: 97.86% | Test Acc: 96.39%\n",
      "Epoch 180 | Loss: 0.1385 | Train Acc: 97.88% | Test Acc: 96.41%\n",
      "Epoch 181 | Loss: 0.1381 | Train Acc: 97.89% | Test Acc: 96.42%\n",
      "Epoch 182 | Loss: 0.1381 | Train Acc: 97.89% | Test Acc: 96.46%\n",
      "Epoch 183 | Loss: 0.1391 | Train Acc: 97.90% | Test Acc: 96.46%\n",
      "Epoch 184 | Loss: 0.1378 | Train Acc: 97.92% | Test Acc: 96.48%\n",
      "Epoch 185 | Loss: 0.1390 | Train Acc: 97.93% | Test Acc: 96.49%\n",
      "Epoch 186 | Loss: 0.1375 | Train Acc: 97.93% | Test Acc: 96.49%\n",
      "Epoch 187 | Loss: 0.1350 | Train Acc: 97.94% | Test Acc: 96.51%\n",
      "Epoch 188 | Loss: 0.1349 | Train Acc: 97.97% | Test Acc: 96.53%\n",
      "Epoch 189 | Loss: 0.1315 | Train Acc: 97.99% | Test Acc: 96.54%\n",
      "Epoch 190 | Loss: 0.1350 | Train Acc: 98.00% | Test Acc: 96.55%\n",
      "Epoch 191 | Loss: 0.1347 | Train Acc: 98.02% | Test Acc: 96.54%\n",
      "Epoch 192 | Loss: 0.1369 | Train Acc: 98.04% | Test Acc: 96.54%\n",
      "Epoch 193 | Loss: 0.1346 | Train Acc: 98.05% | Test Acc: 96.56%\n",
      "Epoch 194 | Loss: 0.1315 | Train Acc: 98.05% | Test Acc: 96.58%\n",
      "Epoch 195 | Loss: 0.1328 | Train Acc: 98.07% | Test Acc: 96.58%\n",
      "Epoch 196 | Loss: 0.1307 | Train Acc: 98.09% | Test Acc: 96.58%\n",
      "Epoch 197 | Loss: 0.1336 | Train Acc: 98.11% | Test Acc: 96.62%\n",
      "Epoch 198 | Loss: 0.1293 | Train Acc: 98.12% | Test Acc: 96.64%\n",
      "Epoch 199 | Loss: 0.1309 | Train Acc: 98.13% | Test Acc: 96.68%\n",
      "Epoch 200 | Loss: 0.1293 | Train Acc: 98.14% | Test Acc: 96.69%\n",
      "Epoch 201 | Loss: 0.1279 | Train Acc: 98.16% | Test Acc: 96.70%\n",
      "Epoch 202 | Loss: 0.1261 | Train Acc: 98.17% | Test Acc: 96.71%\n",
      "Epoch 203 | Loss: 0.1281 | Train Acc: 98.17% | Test Acc: 96.70%\n",
      "Epoch 204 | Loss: 0.1269 | Train Acc: 98.17% | Test Acc: 96.70%\n",
      "Epoch 205 | Loss: 0.1259 | Train Acc: 98.19% | Test Acc: 96.70%\n",
      "Epoch 206 | Loss: 0.1268 | Train Acc: 98.21% | Test Acc: 96.70%\n",
      "Epoch 207 | Loss: 0.1299 | Train Acc: 98.21% | Test Acc: 96.71%\n",
      "Epoch 208 | Loss: 0.1261 | Train Acc: 98.21% | Test Acc: 96.70%\n",
      "Epoch 209 | Loss: 0.1245 | Train Acc: 98.24% | Test Acc: 96.70%\n",
      "Epoch 210 | Loss: 0.1244 | Train Acc: 98.24% | Test Acc: 96.70%\n",
      "Epoch 211 | Loss: 0.1244 | Train Acc: 98.25% | Test Acc: 96.73%\n",
      "Epoch 212 | Loss: 0.1261 | Train Acc: 98.26% | Test Acc: 96.75%\n",
      "Epoch 213 | Loss: 0.1239 | Train Acc: 98.27% | Test Acc: 96.74%\n",
      "Epoch 214 | Loss: 0.1227 | Train Acc: 98.29% | Test Acc: 96.79%\n",
      "Epoch 215 | Loss: 0.1233 | Train Acc: 98.29% | Test Acc: 96.78%\n",
      "Epoch 216 | Loss: 0.1229 | Train Acc: 98.31% | Test Acc: 96.77%\n",
      "Epoch 217 | Loss: 0.1210 | Train Acc: 98.32% | Test Acc: 96.79%\n",
      "Epoch 218 | Loss: 0.1195 | Train Acc: 98.34% | Test Acc: 96.78%\n",
      "Epoch 219 | Loss: 0.1197 | Train Acc: 98.35% | Test Acc: 96.80%\n",
      "Epoch 220 | Loss: 0.1196 | Train Acc: 98.36% | Test Acc: 96.81%\n",
      "Epoch 221 | Loss: 0.1198 | Train Acc: 98.37% | Test Acc: 96.83%\n",
      "Epoch 222 | Loss: 0.1171 | Train Acc: 98.37% | Test Acc: 96.89%\n",
      "Epoch 223 | Loss: 0.1182 | Train Acc: 98.39% | Test Acc: 96.90%\n",
      "Epoch 224 | Loss: 0.1191 | Train Acc: 98.37% | Test Acc: 96.86%\n",
      "Epoch 225 | Loss: 0.1183 | Train Acc: 98.39% | Test Acc: 96.87%\n",
      "Epoch 226 | Loss: 0.1184 | Train Acc: 98.41% | Test Acc: 96.90%\n",
      "Epoch 227 | Loss: 0.1165 | Train Acc: 98.41% | Test Acc: 96.90%\n",
      "Epoch 228 | Loss: 0.1145 | Train Acc: 98.41% | Test Acc: 96.90%\n",
      "Epoch 229 | Loss: 0.1166 | Train Acc: 98.43% | Test Acc: 96.92%\n",
      "Epoch 230 | Loss: 0.1169 | Train Acc: 98.43% | Test Acc: 96.88%\n",
      "Epoch 231 | Loss: 0.1150 | Train Acc: 98.44% | Test Acc: 96.88%\n",
      "Epoch 232 | Loss: 0.1132 | Train Acc: 98.45% | Test Acc: 96.89%\n",
      "Epoch 233 | Loss: 0.1141 | Train Acc: 98.46% | Test Acc: 96.86%\n",
      "Epoch 234 | Loss: 0.1114 | Train Acc: 98.46% | Test Acc: 96.89%\n",
      "Epoch 235 | Loss: 0.1149 | Train Acc: 98.48% | Test Acc: 96.90%\n",
      "Epoch 236 | Loss: 0.1128 | Train Acc: 98.49% | Test Acc: 96.92%\n",
      "Epoch 237 | Loss: 0.1144 | Train Acc: 98.51% | Test Acc: 96.94%\n",
      "Epoch 238 | Loss: 0.1105 | Train Acc: 98.52% | Test Acc: 96.97%\n",
      "Epoch 239 | Loss: 0.1124 | Train Acc: 98.53% | Test Acc: 96.96%\n",
      "Epoch 240 | Loss: 0.1097 | Train Acc: 98.54% | Test Acc: 96.96%\n",
      "Epoch 241 | Loss: 0.1129 | Train Acc: 98.55% | Test Acc: 96.96%\n",
      "Epoch 242 | Loss: 0.1083 | Train Acc: 98.55% | Test Acc: 96.95%\n",
      "Epoch 243 | Loss: 0.1123 | Train Acc: 98.56% | Test Acc: 96.95%\n",
      "Epoch 244 | Loss: 0.1081 | Train Acc: 98.57% | Test Acc: 96.93%\n",
      "Epoch 245 | Loss: 0.1073 | Train Acc: 98.58% | Test Acc: 96.93%\n",
      "Epoch 246 | Loss: 0.1095 | Train Acc: 98.59% | Test Acc: 96.94%\n",
      "Epoch 247 | Loss: 0.1084 | Train Acc: 98.60% | Test Acc: 96.96%\n",
      "Epoch 248 | Loss: 0.1097 | Train Acc: 98.62% | Test Acc: 96.96%\n",
      "Epoch 249 | Loss: 0.1102 | Train Acc: 98.63% | Test Acc: 96.98%\n",
      "Epoch 250 | Loss: 0.1069 | Train Acc: 98.63% | Test Acc: 97.00%\n",
      "Epoch 251 | Loss: 0.1077 | Train Acc: 98.63% | Test Acc: 97.00%\n",
      "Epoch 252 | Loss: 0.1053 | Train Acc: 98.63% | Test Acc: 97.01%\n",
      "Epoch 253 | Loss: 0.1057 | Train Acc: 98.64% | Test Acc: 97.02%\n",
      "Epoch 254 | Loss: 0.1075 | Train Acc: 98.65% | Test Acc: 97.05%\n",
      "Epoch 255 | Loss: 0.1046 | Train Acc: 98.67% | Test Acc: 97.04%\n",
      "Epoch 256 | Loss: 0.1056 | Train Acc: 98.68% | Test Acc: 97.03%\n",
      "Epoch 257 | Loss: 0.1071 | Train Acc: 98.68% | Test Acc: 97.03%\n",
      "Epoch 258 | Loss: 0.1038 | Train Acc: 98.69% | Test Acc: 97.04%\n",
      "Epoch 259 | Loss: 0.1049 | Train Acc: 98.69% | Test Acc: 97.05%\n",
      "Epoch 260 | Loss: 0.1045 | Train Acc: 98.69% | Test Acc: 97.03%\n",
      "Epoch 261 | Loss: 0.1059 | Train Acc: 98.70% | Test Acc: 97.04%\n",
      "Epoch 262 | Loss: 0.1029 | Train Acc: 98.71% | Test Acc: 97.02%\n",
      "Epoch 263 | Loss: 0.1030 | Train Acc: 98.73% | Test Acc: 97.05%\n",
      "Epoch 264 | Loss: 0.1003 | Train Acc: 98.74% | Test Acc: 97.06%\n",
      "Epoch 265 | Loss: 0.1024 | Train Acc: 98.76% | Test Acc: 97.07%\n",
      "Epoch 266 | Loss: 0.1014 | Train Acc: 98.77% | Test Acc: 97.09%\n",
      "Epoch 267 | Loss: 0.1026 | Train Acc: 98.78% | Test Acc: 97.10%\n",
      "Epoch 268 | Loss: 0.1000 | Train Acc: 98.79% | Test Acc: 97.11%\n",
      "Epoch 269 | Loss: 0.1024 | Train Acc: 98.80% | Test Acc: 97.14%\n",
      "Epoch 270 | Loss: 0.0986 | Train Acc: 98.81% | Test Acc: 97.12%\n",
      "Epoch 271 | Loss: 0.0987 | Train Acc: 98.81% | Test Acc: 97.13%\n",
      "Epoch 272 | Loss: 0.1027 | Train Acc: 98.81% | Test Acc: 97.10%\n",
      "Epoch 273 | Loss: 0.0993 | Train Acc: 98.81% | Test Acc: 97.10%\n",
      "Epoch 274 | Loss: 0.0992 | Train Acc: 98.82% | Test Acc: 97.10%\n",
      "Epoch 275 | Loss: 0.0993 | Train Acc: 98.83% | Test Acc: 97.12%\n",
      "Epoch 276 | Loss: 0.0979 | Train Acc: 98.85% | Test Acc: 97.14%\n",
      "Epoch 277 | Loss: 0.0998 | Train Acc: 98.86% | Test Acc: 97.15%\n",
      "Epoch 278 | Loss: 0.1002 | Train Acc: 98.87% | Test Acc: 97.15%\n",
      "Epoch 279 | Loss: 0.0973 | Train Acc: 98.87% | Test Acc: 97.17%\n",
      "Epoch 280 | Loss: 0.0990 | Train Acc: 98.88% | Test Acc: 97.19%\n",
      "Epoch 281 | Loss: 0.0959 | Train Acc: 98.88% | Test Acc: 97.22%\n",
      "Epoch 282 | Loss: 0.0966 | Train Acc: 98.88% | Test Acc: 97.23%\n",
      "Epoch 283 | Loss: 0.0969 | Train Acc: 98.89% | Test Acc: 97.25%\n",
      "Epoch 284 | Loss: 0.0959 | Train Acc: 98.89% | Test Acc: 97.27%\n",
      "Epoch 285 | Loss: 0.0955 | Train Acc: 98.90% | Test Acc: 97.19%\n",
      "Epoch 286 | Loss: 0.0987 | Train Acc: 98.90% | Test Acc: 97.17%\n",
      "Epoch 287 | Loss: 0.0934 | Train Acc: 98.91% | Test Acc: 97.17%\n",
      "Epoch 288 | Loss: 0.0950 | Train Acc: 98.91% | Test Acc: 97.19%\n",
      "Epoch 289 | Loss: 0.0968 | Train Acc: 98.93% | Test Acc: 97.22%\n",
      "Epoch 290 | Loss: 0.0958 | Train Acc: 98.93% | Test Acc: 97.25%\n",
      "Epoch 291 | Loss: 0.0935 | Train Acc: 98.94% | Test Acc: 97.28%\n",
      "Epoch 292 | Loss: 0.0938 | Train Acc: 98.94% | Test Acc: 97.28%\n",
      "Epoch 293 | Loss: 0.0910 | Train Acc: 98.95% | Test Acc: 97.28%\n",
      "Epoch 294 | Loss: 0.0920 | Train Acc: 98.96% | Test Acc: 97.26%\n",
      "Epoch 295 | Loss: 0.0929 | Train Acc: 98.96% | Test Acc: 97.27%\n",
      "Epoch 296 | Loss: 0.0930 | Train Acc: 98.97% | Test Acc: 97.28%\n",
      "Epoch 297 | Loss: 0.0919 | Train Acc: 98.98% | Test Acc: 97.26%\n",
      "Epoch 298 | Loss: 0.0910 | Train Acc: 98.99% | Test Acc: 97.23%\n",
      "Epoch 299 | Loss: 0.0915 | Train Acc: 99.00% | Test Acc: 97.23%\n",
      "Epoch 300 | Loss: 0.0910 | Train Acc: 99.00% | Test Acc: 97.23%\n",
      "Epoch 301 | Loss: 0.0899 | Train Acc: 99.00% | Test Acc: 97.22%\n",
      "Epoch 302 | Loss: 0.0908 | Train Acc: 99.01% | Test Acc: 97.23%\n",
      "Epoch 303 | Loss: 0.0881 | Train Acc: 99.01% | Test Acc: 97.25%\n",
      "Epoch 304 | Loss: 0.0913 | Train Acc: 99.02% | Test Acc: 97.27%\n",
      "Epoch 305 | Loss: 0.0901 | Train Acc: 99.03% | Test Acc: 97.26%\n",
      "Epoch 306 | Loss: 0.0919 | Train Acc: 99.03% | Test Acc: 97.28%\n",
      "Epoch 307 | Loss: 0.0915 | Train Acc: 99.03% | Test Acc: 97.28%\n",
      "Epoch 308 | Loss: 0.0909 | Train Acc: 99.04% | Test Acc: 97.25%\n",
      "Epoch 309 | Loss: 0.0877 | Train Acc: 99.04% | Test Acc: 97.27%\n",
      "Epoch 310 | Loss: 0.0902 | Train Acc: 99.06% | Test Acc: 97.27%\n",
      "Epoch 311 | Loss: 0.0866 | Train Acc: 99.06% | Test Acc: 97.25%\n",
      "Epoch 312 | Loss: 0.0871 | Train Acc: 99.06% | Test Acc: 97.28%\n",
      "Epoch 313 | Loss: 0.0871 | Train Acc: 99.07% | Test Acc: 97.28%\n",
      "Epoch 314 | Loss: 0.0865 | Train Acc: 99.07% | Test Acc: 97.28%\n",
      "Epoch 315 | Loss: 0.0887 | Train Acc: 99.06% | Test Acc: 97.28%\n",
      "Epoch 316 | Loss: 0.0897 | Train Acc: 99.07% | Test Acc: 97.30%\n",
      "Epoch 317 | Loss: 0.0867 | Train Acc: 99.08% | Test Acc: 97.32%\n",
      "Epoch 318 | Loss: 0.0880 | Train Acc: 99.09% | Test Acc: 97.29%\n",
      "Epoch 319 | Loss: 0.0862 | Train Acc: 99.09% | Test Acc: 97.27%\n",
      "Epoch 320 | Loss: 0.0853 | Train Acc: 99.10% | Test Acc: 97.28%\n",
      "Epoch 321 | Loss: 0.0856 | Train Acc: 99.11% | Test Acc: 97.28%\n",
      "Epoch 322 | Loss: 0.0832 | Train Acc: 99.11% | Test Acc: 97.27%\n",
      "Epoch 323 | Loss: 0.0861 | Train Acc: 99.12% | Test Acc: 97.29%\n",
      "Epoch 324 | Loss: 0.0867 | Train Acc: 99.12% | Test Acc: 97.29%\n",
      "Epoch 325 | Loss: 0.0849 | Train Acc: 99.12% | Test Acc: 97.27%\n",
      "Epoch 326 | Loss: 0.0858 | Train Acc: 99.13% | Test Acc: 97.31%\n",
      "Epoch 327 | Loss: 0.0819 | Train Acc: 99.14% | Test Acc: 97.34%\n",
      "Epoch 328 | Loss: 0.0840 | Train Acc: 99.13% | Test Acc: 97.32%\n",
      "Epoch 329 | Loss: 0.0834 | Train Acc: 99.14% | Test Acc: 97.33%\n",
      "Epoch 330 | Loss: 0.0846 | Train Acc: 99.15% | Test Acc: 97.34%\n",
      "Epoch 331 | Loss: 0.0872 | Train Acc: 99.15% | Test Acc: 97.34%\n",
      "Epoch 332 | Loss: 0.0831 | Train Acc: 99.16% | Test Acc: 97.35%\n",
      "Epoch 333 | Loss: 0.0830 | Train Acc: 99.16% | Test Acc: 97.38%\n",
      "Epoch 334 | Loss: 0.0837 | Train Acc: 99.16% | Test Acc: 97.40%\n",
      "Epoch 335 | Loss: 0.0848 | Train Acc: 99.17% | Test Acc: 97.41%\n",
      "Epoch 336 | Loss: 0.0826 | Train Acc: 99.18% | Test Acc: 97.40%\n",
      "Epoch 337 | Loss: 0.0815 | Train Acc: 99.18% | Test Acc: 97.37%\n",
      "Epoch 338 | Loss: 0.0819 | Train Acc: 99.20% | Test Acc: 97.39%\n",
      "Epoch 339 | Loss: 0.0826 | Train Acc: 99.20% | Test Acc: 97.37%\n",
      "Epoch 340 | Loss: 0.0808 | Train Acc: 99.20% | Test Acc: 97.37%\n",
      "Epoch 341 | Loss: 0.0817 | Train Acc: 99.21% | Test Acc: 97.38%\n",
      "Epoch 342 | Loss: 0.0807 | Train Acc: 99.21% | Test Acc: 97.41%\n",
      "Epoch 343 | Loss: 0.0829 | Train Acc: 99.22% | Test Acc: 97.41%\n",
      "Epoch 344 | Loss: 0.0805 | Train Acc: 99.22% | Test Acc: 97.39%\n",
      "Epoch 345 | Loss: 0.0825 | Train Acc: 99.23% | Test Acc: 97.39%\n",
      "Epoch 346 | Loss: 0.0793 | Train Acc: 99.24% | Test Acc: 97.40%\n",
      "Epoch 347 | Loss: 0.0810 | Train Acc: 99.25% | Test Acc: 97.41%\n",
      "Epoch 348 | Loss: 0.0805 | Train Acc: 99.26% | Test Acc: 97.40%\n",
      "Epoch 349 | Loss: 0.0799 | Train Acc: 99.27% | Test Acc: 97.43%\n",
      "Epoch 350 | Loss: 0.0809 | Train Acc: 99.27% | Test Acc: 97.42%\n",
      "Epoch 351 | Loss: 0.0794 | Train Acc: 99.28% | Test Acc: 97.41%\n",
      "Epoch 352 | Loss: 0.0783 | Train Acc: 99.28% | Test Acc: 97.41%\n",
      "Epoch 353 | Loss: 0.0801 | Train Acc: 99.28% | Test Acc: 97.44%\n",
      "Epoch 354 | Loss: 0.0796 | Train Acc: 99.29% | Test Acc: 97.45%\n",
      "Epoch 355 | Loss: 0.0769 | Train Acc: 99.30% | Test Acc: 97.47%\n",
      "Epoch 356 | Loss: 0.0770 | Train Acc: 99.30% | Test Acc: 97.46%\n",
      "Epoch 357 | Loss: 0.0784 | Train Acc: 99.31% | Test Acc: 97.43%\n",
      "Epoch 358 | Loss: 0.0771 | Train Acc: 99.31% | Test Acc: 97.45%\n",
      "Epoch 359 | Loss: 0.0768 | Train Acc: 99.31% | Test Acc: 97.46%\n",
      "Epoch 360 | Loss: 0.0797 | Train Acc: 99.31% | Test Acc: 97.47%\n",
      "Epoch 361 | Loss: 0.0777 | Train Acc: 99.30% | Test Acc: 97.47%\n",
      "Epoch 362 | Loss: 0.0766 | Train Acc: 99.31% | Test Acc: 97.50%\n",
      "Epoch 363 | Loss: 0.0781 | Train Acc: 99.31% | Test Acc: 97.46%\n",
      "Epoch 364 | Loss: 0.0757 | Train Acc: 99.31% | Test Acc: 97.48%\n",
      "Epoch 365 | Loss: 0.0769 | Train Acc: 99.32% | Test Acc: 97.47%\n",
      "Epoch 366 | Loss: 0.0733 | Train Acc: 99.34% | Test Acc: 97.46%\n",
      "Epoch 367 | Loss: 0.0752 | Train Acc: 99.34% | Test Acc: 97.43%\n",
      "Epoch 368 | Loss: 0.0774 | Train Acc: 99.34% | Test Acc: 97.41%\n",
      "Epoch 369 | Loss: 0.0749 | Train Acc: 99.35% | Test Acc: 97.42%\n",
      "Epoch 370 | Loss: 0.0766 | Train Acc: 99.35% | Test Acc: 97.46%\n",
      "Epoch 371 | Loss: 0.0768 | Train Acc: 99.36% | Test Acc: 97.46%\n",
      "Epoch 372 | Loss: 0.0744 | Train Acc: 99.37% | Test Acc: 97.45%\n",
      "Epoch 373 | Loss: 0.0743 | Train Acc: 99.36% | Test Acc: 97.44%\n",
      "Epoch 374 | Loss: 0.0750 | Train Acc: 99.37% | Test Acc: 97.47%\n",
      "Epoch 375 | Loss: 0.0734 | Train Acc: 99.37% | Test Acc: 97.46%\n",
      "Epoch 376 | Loss: 0.0743 | Train Acc: 99.38% | Test Acc: 97.49%\n",
      "Epoch 377 | Loss: 0.0757 | Train Acc: 99.38% | Test Acc: 97.48%\n",
      "Epoch 378 | Loss: 0.0719 | Train Acc: 99.39% | Test Acc: 97.46%\n",
      "Epoch 379 | Loss: 0.0740 | Train Acc: 99.39% | Test Acc: 97.46%\n",
      "Epoch 380 | Loss: 0.0724 | Train Acc: 99.39% | Test Acc: 97.47%\n",
      "Epoch 381 | Loss: 0.0711 | Train Acc: 99.40% | Test Acc: 97.48%\n",
      "Epoch 382 | Loss: 0.0742 | Train Acc: 99.41% | Test Acc: 97.49%\n",
      "Epoch 383 | Loss: 0.0719 | Train Acc: 99.42% | Test Acc: 97.45%\n",
      "Epoch 384 | Loss: 0.0707 | Train Acc: 99.43% | Test Acc: 97.47%\n",
      "Epoch 385 | Loss: 0.0709 | Train Acc: 99.43% | Test Acc: 97.46%\n",
      "Epoch 386 | Loss: 0.0723 | Train Acc: 99.43% | Test Acc: 97.44%\n",
      "Epoch 387 | Loss: 0.0735 | Train Acc: 99.43% | Test Acc: 97.42%\n",
      "Epoch 388 | Loss: 0.0727 | Train Acc: 99.44% | Test Acc: 97.43%\n",
      "Epoch 389 | Loss: 0.0710 | Train Acc: 99.44% | Test Acc: 97.43%\n",
      "Epoch 390 | Loss: 0.0714 | Train Acc: 99.44% | Test Acc: 97.47%\n",
      "Epoch 391 | Loss: 0.0701 | Train Acc: 99.43% | Test Acc: 97.48%\n",
      "Epoch 392 | Loss: 0.0718 | Train Acc: 99.43% | Test Acc: 97.48%\n",
      "Epoch 393 | Loss: 0.0725 | Train Acc: 99.44% | Test Acc: 97.45%\n",
      "Epoch 394 | Loss: 0.0721 | Train Acc: 99.45% | Test Acc: 97.46%\n",
      "Epoch 395 | Loss: 0.0705 | Train Acc: 99.45% | Test Acc: 97.44%\n",
      "Epoch 396 | Loss: 0.0705 | Train Acc: 99.46% | Test Acc: 97.43%\n",
      "Epoch 397 | Loss: 0.0703 | Train Acc: 99.47% | Test Acc: 97.43%\n",
      "Epoch 398 | Loss: 0.0728 | Train Acc: 99.47% | Test Acc: 97.46%\n",
      "Epoch 399 | Loss: 0.0688 | Train Acc: 99.47% | Test Acc: 97.50%\n",
      "Epoch 400 | Loss: 0.0700 | Train Acc: 99.48% | Test Acc: 97.54%\n",
      "Epoch 401 | Loss: 0.0716 | Train Acc: 99.48% | Test Acc: 97.55%\n",
      "Epoch 402 | Loss: 0.0713 | Train Acc: 99.48% | Test Acc: 97.58%\n",
      "Epoch 403 | Loss: 0.0700 | Train Acc: 99.48% | Test Acc: 97.59%\n",
      "Epoch 404 | Loss: 0.0693 | Train Acc: 99.48% | Test Acc: 97.56%\n",
      "Epoch 405 | Loss: 0.0677 | Train Acc: 99.48% | Test Acc: 97.52%\n",
      "Epoch 406 | Loss: 0.0694 | Train Acc: 99.49% | Test Acc: 97.52%\n",
      "Epoch 407 | Loss: 0.0687 | Train Acc: 99.50% | Test Acc: 97.51%\n",
      "Epoch 408 | Loss: 0.0687 | Train Acc: 99.50% | Test Acc: 97.49%\n",
      "Epoch 409 | Loss: 0.0676 | Train Acc: 99.50% | Test Acc: 97.48%\n",
      "Epoch 410 | Loss: 0.0672 | Train Acc: 99.51% | Test Acc: 97.49%\n",
      "Epoch 411 | Loss: 0.0685 | Train Acc: 99.50% | Test Acc: 97.51%\n",
      "Epoch 412 | Loss: 0.0693 | Train Acc: 99.51% | Test Acc: 97.49%\n",
      "Epoch 413 | Loss: 0.0669 | Train Acc: 99.52% | Test Acc: 97.50%\n",
      "Epoch 414 | Loss: 0.0659 | Train Acc: 99.51% | Test Acc: 97.48%\n",
      "Epoch 415 | Loss: 0.0646 | Train Acc: 99.51% | Test Acc: 97.47%\n",
      "Epoch 416 | Loss: 0.0660 | Train Acc: 99.51% | Test Acc: 97.51%\n",
      "Epoch 417 | Loss: 0.0680 | Train Acc: 99.52% | Test Acc: 97.49%\n",
      "Epoch 418 | Loss: 0.0656 | Train Acc: 99.53% | Test Acc: 97.45%\n",
      "Epoch 419 | Loss: 0.0674 | Train Acc: 99.52% | Test Acc: 97.48%\n",
      "Epoch 420 | Loss: 0.0672 | Train Acc: 99.53% | Test Acc: 97.48%\n",
      "Epoch 421 | Loss: 0.0671 | Train Acc: 99.53% | Test Acc: 97.49%\n",
      "Epoch 422 | Loss: 0.0664 | Train Acc: 99.53% | Test Acc: 97.50%\n",
      "Epoch 423 | Loss: 0.0630 | Train Acc: 99.53% | Test Acc: 97.50%\n",
      "Epoch 424 | Loss: 0.0661 | Train Acc: 99.54% | Test Acc: 97.53%\n",
      "Epoch 425 | Loss: 0.0659 | Train Acc: 99.54% | Test Acc: 97.54%\n",
      "Epoch 426 | Loss: 0.0633 | Train Acc: 99.54% | Test Acc: 97.57%\n",
      "Epoch 427 | Loss: 0.0660 | Train Acc: 99.56% | Test Acc: 97.50%\n",
      "Epoch 428 | Loss: 0.0641 | Train Acc: 99.57% | Test Acc: 97.51%\n",
      "Epoch 429 | Loss: 0.0638 | Train Acc: 99.57% | Test Acc: 97.52%\n",
      "Epoch 430 | Loss: 0.0657 | Train Acc: 99.58% | Test Acc: 97.49%\n",
      "Epoch 431 | Loss: 0.0645 | Train Acc: 99.59% | Test Acc: 97.48%\n",
      "Epoch 432 | Loss: 0.0653 | Train Acc: 99.59% | Test Acc: 97.50%\n",
      "Epoch 433 | Loss: 0.0643 | Train Acc: 99.58% | Test Acc: 97.54%\n",
      "Epoch 434 | Loss: 0.0639 | Train Acc: 99.58% | Test Acc: 97.52%\n",
      "Epoch 435 | Loss: 0.0631 | Train Acc: 99.57% | Test Acc: 97.52%\n",
      "Epoch 436 | Loss: 0.0641 | Train Acc: 99.59% | Test Acc: 97.49%\n",
      "Epoch 437 | Loss: 0.0651 | Train Acc: 99.59% | Test Acc: 97.48%\n",
      "Epoch 438 | Loss: 0.0627 | Train Acc: 99.60% | Test Acc: 97.50%\n",
      "Epoch 439 | Loss: 0.0636 | Train Acc: 99.60% | Test Acc: 97.50%\n",
      "Epoch 440 | Loss: 0.0623 | Train Acc: 99.60% | Test Acc: 97.50%\n",
      "Epoch 441 | Loss: 0.0630 | Train Acc: 99.61% | Test Acc: 97.53%\n",
      "Epoch 442 | Loss: 0.0633 | Train Acc: 99.61% | Test Acc: 97.53%\n",
      "Epoch 443 | Loss: 0.0635 | Train Acc: 99.60% | Test Acc: 97.54%\n",
      "Epoch 444 | Loss: 0.0624 | Train Acc: 99.60% | Test Acc: 97.56%\n",
      "Epoch 445 | Loss: 0.0621 | Train Acc: 99.61% | Test Acc: 97.55%\n",
      "Epoch 446 | Loss: 0.0619 | Train Acc: 99.61% | Test Acc: 97.53%\n",
      "Epoch 447 | Loss: 0.0615 | Train Acc: 99.62% | Test Acc: 97.52%\n",
      "Epoch 448 | Loss: 0.0626 | Train Acc: 99.62% | Test Acc: 97.53%\n",
      "Epoch 449 | Loss: 0.0615 | Train Acc: 99.62% | Test Acc: 97.54%\n",
      "Epoch 450 | Loss: 0.0617 | Train Acc: 99.62% | Test Acc: 97.55%\n",
      "Epoch 451 | Loss: 0.0603 | Train Acc: 99.62% | Test Acc: 97.60%\n",
      "Epoch 452 | Loss: 0.0619 | Train Acc: 99.63% | Test Acc: 97.60%\n",
      "Epoch 453 | Loss: 0.0638 | Train Acc: 99.63% | Test Acc: 97.62%\n",
      "Epoch 454 | Loss: 0.0624 | Train Acc: 99.64% | Test Acc: 97.60%\n",
      "Epoch 455 | Loss: 0.0603 | Train Acc: 99.64% | Test Acc: 97.58%\n",
      "Epoch 456 | Loss: 0.0601 | Train Acc: 99.64% | Test Acc: 97.56%\n",
      "Epoch 457 | Loss: 0.0606 | Train Acc: 99.63% | Test Acc: 97.56%\n",
      "Epoch 458 | Loss: 0.0601 | Train Acc: 99.64% | Test Acc: 97.54%\n",
      "Epoch 459 | Loss: 0.0609 | Train Acc: 99.64% | Test Acc: 97.56%\n",
      "Epoch 460 | Loss: 0.0591 | Train Acc: 99.65% | Test Acc: 97.58%\n",
      "Epoch 461 | Loss: 0.0611 | Train Acc: 99.66% | Test Acc: 97.55%\n",
      "Epoch 462 | Loss: 0.0608 | Train Acc: 99.65% | Test Acc: 97.53%\n",
      "Epoch 463 | Loss: 0.0608 | Train Acc: 99.66% | Test Acc: 97.52%\n",
      "Epoch 464 | Loss: 0.0603 | Train Acc: 99.66% | Test Acc: 97.52%\n",
      "Epoch 465 | Loss: 0.0597 | Train Acc: 99.66% | Test Acc: 97.55%\n",
      "Epoch 466 | Loss: 0.0600 | Train Acc: 99.66% | Test Acc: 97.59%\n",
      "Epoch 467 | Loss: 0.0581 | Train Acc: 99.66% | Test Acc: 97.58%\n",
      "Epoch 468 | Loss: 0.0596 | Train Acc: 99.67% | Test Acc: 97.59%\n",
      "Epoch 469 | Loss: 0.0592 | Train Acc: 99.67% | Test Acc: 97.61%\n",
      "Epoch 470 | Loss: 0.0588 | Train Acc: 99.67% | Test Acc: 97.59%\n",
      "Epoch 471 | Loss: 0.0578 | Train Acc: 99.66% | Test Acc: 97.63%\n",
      "Epoch 472 | Loss: 0.0575 | Train Acc: 99.66% | Test Acc: 97.62%\n",
      "Epoch 473 | Loss: 0.0585 | Train Acc: 99.66% | Test Acc: 97.65%\n",
      "Epoch 474 | Loss: 0.0590 | Train Acc: 99.67% | Test Acc: 97.65%\n",
      "Epoch 475 | Loss: 0.0584 | Train Acc: 99.66% | Test Acc: 97.62%\n",
      "Epoch 476 | Loss: 0.0600 | Train Acc: 99.68% | Test Acc: 97.60%\n",
      "Epoch 477 | Loss: 0.0576 | Train Acc: 99.69% | Test Acc: 97.61%\n",
      "Epoch 478 | Loss: 0.0580 | Train Acc: 99.69% | Test Acc: 97.58%\n",
      "Epoch 479 | Loss: 0.0563 | Train Acc: 99.69% | Test Acc: 97.60%\n",
      "Epoch 480 | Loss: 0.0584 | Train Acc: 99.69% | Test Acc: 97.57%\n",
      "Epoch 481 | Loss: 0.0581 | Train Acc: 99.70% | Test Acc: 97.59%\n",
      "Epoch 482 | Loss: 0.0562 | Train Acc: 99.70% | Test Acc: 97.58%\n",
      "Epoch 483 | Loss: 0.0561 | Train Acc: 99.69% | Test Acc: 97.60%\n",
      "Epoch 484 | Loss: 0.0586 | Train Acc: 99.70% | Test Acc: 97.59%\n",
      "Epoch 485 | Loss: 0.0570 | Train Acc: 99.70% | Test Acc: 97.59%\n",
      "Epoch 486 | Loss: 0.0561 | Train Acc: 99.70% | Test Acc: 97.55%\n",
      "Epoch 487 | Loss: 0.0557 | Train Acc: 99.70% | Test Acc: 97.56%\n",
      "Epoch 488 | Loss: 0.0560 | Train Acc: 99.70% | Test Acc: 97.57%\n",
      "Epoch 489 | Loss: 0.0574 | Train Acc: 99.71% | Test Acc: 97.59%\n",
      "Epoch 490 | Loss: 0.0539 | Train Acc: 99.70% | Test Acc: 97.61%\n",
      "Epoch 491 | Loss: 0.0580 | Train Acc: 99.70% | Test Acc: 97.61%\n",
      "Epoch 492 | Loss: 0.0556 | Train Acc: 99.70% | Test Acc: 97.61%\n",
      "Epoch 493 | Loss: 0.0552 | Train Acc: 99.71% | Test Acc: 97.60%\n",
      "Epoch 494 | Loss: 0.0564 | Train Acc: 99.71% | Test Acc: 97.62%\n",
      "Epoch 495 | Loss: 0.0545 | Train Acc: 99.72% | Test Acc: 97.62%\n",
      "Epoch 496 | Loss: 0.0550 | Train Acc: 99.72% | Test Acc: 97.63%\n",
      "Epoch 497 | Loss: 0.0554 | Train Acc: 99.73% | Test Acc: 97.62%\n",
      "Epoch 498 | Loss: 0.0552 | Train Acc: 99.73% | Test Acc: 97.63%\n",
      "Epoch 499 | Loss: 0.0546 | Train Acc: 99.73% | Test Acc: 97.64%\n",
      "Epoch 500 | Loss: 0.0551 | Train Acc: 99.73% | Test Acc: 97.63%\n",
      "Epoch 501 | Loss: 0.0566 | Train Acc: 99.73% | Test Acc: 97.60%\n",
      "Epoch 502 | Loss: 0.0564 | Train Acc: 99.73% | Test Acc: 97.57%\n",
      "Epoch 503 | Loss: 0.0549 | Train Acc: 99.73% | Test Acc: 97.58%\n",
      "Epoch 504 | Loss: 0.0569 | Train Acc: 99.73% | Test Acc: 97.60%\n",
      "Epoch 505 | Loss: 0.0548 | Train Acc: 99.74% | Test Acc: 97.60%\n",
      "Epoch 506 | Loss: 0.0550 | Train Acc: 99.74% | Test Acc: 97.61%\n",
      "Epoch 507 | Loss: 0.0554 | Train Acc: 99.74% | Test Acc: 97.59%\n",
      "Epoch 508 | Loss: 0.0524 | Train Acc: 99.74% | Test Acc: 97.60%\n",
      "Epoch 509 | Loss: 0.0536 | Train Acc: 99.75% | Test Acc: 97.62%\n",
      "Epoch 510 | Loss: 0.0531 | Train Acc: 99.74% | Test Acc: 97.63%\n",
      "Epoch 511 | Loss: 0.0547 | Train Acc: 99.74% | Test Acc: 97.63%\n",
      "Epoch 512 | Loss: 0.0536 | Train Acc: 99.74% | Test Acc: 97.64%\n",
      "Epoch 513 | Loss: 0.0555 | Train Acc: 99.74% | Test Acc: 97.63%\n",
      "Epoch 514 | Loss: 0.0537 | Train Acc: 99.74% | Test Acc: 97.63%\n",
      "Epoch 515 | Loss: 0.0538 | Train Acc: 99.74% | Test Acc: 97.63%\n",
      "Epoch 516 | Loss: 0.0540 | Train Acc: 99.75% | Test Acc: 97.62%\n",
      "Epoch 517 | Loss: 0.0533 | Train Acc: 99.75% | Test Acc: 97.66%\n",
      "Epoch 518 | Loss: 0.0536 | Train Acc: 99.75% | Test Acc: 97.63%\n",
      "Epoch 519 | Loss: 0.0528 | Train Acc: 99.76% | Test Acc: 97.60%\n",
      "Epoch 520 | Loss: 0.0527 | Train Acc: 99.76% | Test Acc: 97.62%\n",
      "Epoch 521 | Loss: 0.0535 | Train Acc: 99.76% | Test Acc: 97.62%\n",
      "Epoch 522 | Loss: 0.0555 | Train Acc: 99.77% | Test Acc: 97.66%\n",
      "Epoch 523 | Loss: 0.0542 | Train Acc: 99.77% | Test Acc: 97.66%\n",
      "Epoch 524 | Loss: 0.0520 | Train Acc: 99.78% | Test Acc: 97.67%\n",
      "Epoch 525 | Loss: 0.0536 | Train Acc: 99.77% | Test Acc: 97.66%\n",
      "Epoch 526 | Loss: 0.0518 | Train Acc: 99.77% | Test Acc: 97.68%\n",
      "Epoch 527 | Loss: 0.0517 | Train Acc: 99.77% | Test Acc: 97.69%\n",
      "Epoch 528 | Loss: 0.0512 | Train Acc: 99.77% | Test Acc: 97.68%\n",
      "Epoch 529 | Loss: 0.0549 | Train Acc: 99.77% | Test Acc: 97.66%\n",
      "Epoch 530 | Loss: 0.0530 | Train Acc: 99.77% | Test Acc: 97.66%\n",
      "Epoch 531 | Loss: 0.0534 | Train Acc: 99.78% | Test Acc: 97.65%\n",
      "Epoch 532 | Loss: 0.0512 | Train Acc: 99.78% | Test Acc: 97.66%\n",
      "Epoch 533 | Loss: 0.0498 | Train Acc: 99.78% | Test Acc: 97.68%\n",
      "Epoch 534 | Loss: 0.0528 | Train Acc: 99.78% | Test Acc: 97.70%\n",
      "Epoch 535 | Loss: 0.0524 | Train Acc: 99.78% | Test Acc: 97.69%\n",
      "Epoch 536 | Loss: 0.0509 | Train Acc: 99.78% | Test Acc: 97.67%\n",
      "Epoch 537 | Loss: 0.0521 | Train Acc: 99.78% | Test Acc: 97.70%\n",
      "Epoch 538 | Loss: 0.0499 | Train Acc: 99.78% | Test Acc: 97.68%\n",
      "Epoch 539 | Loss: 0.0519 | Train Acc: 99.78% | Test Acc: 97.66%\n",
      "Epoch 540 | Loss: 0.0516 | Train Acc: 99.78% | Test Acc: 97.70%\n",
      "Epoch 541 | Loss: 0.0523 | Train Acc: 99.78% | Test Acc: 97.70%\n",
      "Epoch 542 | Loss: 0.0489 | Train Acc: 99.78% | Test Acc: 97.70%\n",
      "Epoch 543 | Loss: 0.0507 | Train Acc: 99.78% | Test Acc: 97.68%\n",
      "Epoch 544 | Loss: 0.0500 | Train Acc: 99.78% | Test Acc: 97.70%\n",
      "Epoch 545 | Loss: 0.0514 | Train Acc: 99.79% | Test Acc: 97.64%\n",
      "Epoch 546 | Loss: 0.0506 | Train Acc: 99.79% | Test Acc: 97.64%\n",
      "Epoch 547 | Loss: 0.0503 | Train Acc: 99.79% | Test Acc: 97.64%\n",
      "Epoch 548 | Loss: 0.0508 | Train Acc: 99.79% | Test Acc: 97.66%\n",
      "Epoch 549 | Loss: 0.0512 | Train Acc: 99.79% | Test Acc: 97.67%\n",
      "Epoch 550 | Loss: 0.0511 | Train Acc: 99.79% | Test Acc: 97.70%\n",
      "Epoch 551 | Loss: 0.0488 | Train Acc: 99.79% | Test Acc: 97.69%\n",
      "Epoch 552 | Loss: 0.0490 | Train Acc: 99.80% | Test Acc: 97.68%\n",
      "Epoch 553 | Loss: 0.0481 | Train Acc: 99.80% | Test Acc: 97.68%\n",
      "Epoch 554 | Loss: 0.0482 | Train Acc: 99.80% | Test Acc: 97.67%\n",
      "Epoch 555 | Loss: 0.0505 | Train Acc: 99.81% | Test Acc: 97.71%\n",
      "Epoch 556 | Loss: 0.0502 | Train Acc: 99.81% | Test Acc: 97.73%\n",
      "Epoch 557 | Loss: 0.0473 | Train Acc: 99.81% | Test Acc: 97.72%\n",
      "Epoch 558 | Loss: 0.0480 | Train Acc: 99.81% | Test Acc: 97.73%\n",
      "Epoch 559 | Loss: 0.0488 | Train Acc: 99.82% | Test Acc: 97.70%\n",
      "Epoch 560 | Loss: 0.0503 | Train Acc: 99.82% | Test Acc: 97.70%\n",
      "Epoch 561 | Loss: 0.0478 | Train Acc: 99.82% | Test Acc: 97.71%\n",
      "Epoch 562 | Loss: 0.0501 | Train Acc: 99.82% | Test Acc: 97.71%\n",
      "Epoch 563 | Loss: 0.0474 | Train Acc: 99.82% | Test Acc: 97.70%\n",
      "Epoch 564 | Loss: 0.0483 | Train Acc: 99.82% | Test Acc: 97.71%\n",
      "Epoch 565 | Loss: 0.0486 | Train Acc: 99.83% | Test Acc: 97.70%\n",
      "Epoch 566 | Loss: 0.0475 | Train Acc: 99.83% | Test Acc: 97.71%\n",
      "Epoch 567 | Loss: 0.0492 | Train Acc: 99.83% | Test Acc: 97.71%\n",
      "Epoch 568 | Loss: 0.0483 | Train Acc: 99.84% | Test Acc: 97.73%\n",
      "Epoch 569 | Loss: 0.0476 | Train Acc: 99.84% | Test Acc: 97.74%\n",
      "Epoch 570 | Loss: 0.0480 | Train Acc: 99.84% | Test Acc: 97.72%\n",
      "Epoch 571 | Loss: 0.0501 | Train Acc: 99.84% | Test Acc: 97.69%\n",
      "Epoch 572 | Loss: 0.0482 | Train Acc: 99.84% | Test Acc: 97.69%\n",
      "Epoch 573 | Loss: 0.0455 | Train Acc: 99.84% | Test Acc: 97.70%\n",
      "Epoch 574 | Loss: 0.0486 | Train Acc: 99.84% | Test Acc: 97.70%\n",
      "Epoch 575 | Loss: 0.0486 | Train Acc: 99.83% | Test Acc: 97.70%\n",
      "Epoch 576 | Loss: 0.0461 | Train Acc: 99.83% | Test Acc: 97.71%\n",
      "Epoch 577 | Loss: 0.0480 | Train Acc: 99.84% | Test Acc: 97.71%\n",
      "Epoch 578 | Loss: 0.0455 | Train Acc: 99.84% | Test Acc: 97.70%\n",
      "Epoch 579 | Loss: 0.0492 | Train Acc: 99.84% | Test Acc: 97.71%\n",
      "Epoch 580 | Loss: 0.0455 | Train Acc: 99.84% | Test Acc: 97.74%\n",
      "Epoch 581 | Loss: 0.0480 | Train Acc: 99.84% | Test Acc: 97.74%\n",
      "Epoch 582 | Loss: 0.0465 | Train Acc: 99.84% | Test Acc: 97.75%\n",
      "Epoch 583 | Loss: 0.0465 | Train Acc: 99.84% | Test Acc: 97.74%\n",
      "Epoch 584 | Loss: 0.0478 | Train Acc: 99.84% | Test Acc: 97.72%\n",
      "Epoch 585 | Loss: 0.0467 | Train Acc: 99.85% | Test Acc: 97.70%\n",
      "Epoch 586 | Loss: 0.0457 | Train Acc: 99.84% | Test Acc: 97.70%\n",
      "Epoch 587 | Loss: 0.0451 | Train Acc: 99.85% | Test Acc: 97.70%\n",
      "Epoch 588 | Loss: 0.0463 | Train Acc: 99.85% | Test Acc: 97.70%\n",
      "Epoch 589 | Loss: 0.0478 | Train Acc: 99.85% | Test Acc: 97.71%\n",
      "Epoch 590 | Loss: 0.0473 | Train Acc: 99.85% | Test Acc: 97.71%\n",
      "Epoch 591 | Loss: 0.0453 | Train Acc: 99.86% | Test Acc: 97.72%\n",
      "Epoch 592 | Loss: 0.0470 | Train Acc: 99.86% | Test Acc: 97.70%\n",
      "Epoch 593 | Loss: 0.0458 | Train Acc: 99.86% | Test Acc: 97.72%\n",
      "Epoch 594 | Loss: 0.0478 | Train Acc: 99.86% | Test Acc: 97.70%\n",
      "Epoch 595 | Loss: 0.0451 | Train Acc: 99.86% | Test Acc: 97.70%\n",
      "Epoch 596 | Loss: 0.0451 | Train Acc: 99.85% | Test Acc: 97.70%\n",
      "Epoch 597 | Loss: 0.0464 | Train Acc: 99.86% | Test Acc: 97.70%\n",
      "Epoch 598 | Loss: 0.0466 | Train Acc: 99.86% | Test Acc: 97.71%\n",
      "Epoch 599 | Loss: 0.0460 | Train Acc: 99.86% | Test Acc: 97.71%\n",
      "Epoch 600 | Loss: 0.0450 | Train Acc: 99.86% | Test Acc: 97.72%\n",
      "Epoch 601 | Loss: 0.0451 | Train Acc: 99.86% | Test Acc: 97.71%\n",
      "Epoch 602 | Loss: 0.0456 | Train Acc: 99.87% | Test Acc: 97.70%\n",
      "Epoch 603 | Loss: 0.0463 | Train Acc: 99.87% | Test Acc: 97.71%\n",
      "Epoch 604 | Loss: 0.0447 | Train Acc: 99.86% | Test Acc: 97.74%\n",
      "Epoch 605 | Loss: 0.0452 | Train Acc: 99.86% | Test Acc: 97.74%\n",
      "Epoch 606 | Loss: 0.0438 | Train Acc: 99.86% | Test Acc: 97.73%\n",
      "Epoch 607 | Loss: 0.0453 | Train Acc: 99.86% | Test Acc: 97.72%\n",
      "Epoch 608 | Loss: 0.0438 | Train Acc: 99.86% | Test Acc: 97.72%\n",
      "Epoch 609 | Loss: 0.0446 | Train Acc: 99.86% | Test Acc: 97.73%\n",
      "Epoch 610 | Loss: 0.0448 | Train Acc: 99.86% | Test Acc: 97.70%\n",
      "Epoch 611 | Loss: 0.0418 | Train Acc: 99.86% | Test Acc: 97.70%\n",
      "Epoch 612 | Loss: 0.0430 | Train Acc: 99.87% | Test Acc: 97.72%\n",
      "Epoch 613 | Loss: 0.0456 | Train Acc: 99.87% | Test Acc: 97.76%\n",
      "Epoch 614 | Loss: 0.0453 | Train Acc: 99.88% | Test Acc: 97.81%\n",
      "Epoch 615 | Loss: 0.0436 | Train Acc: 99.87% | Test Acc: 97.78%\n",
      "Epoch 616 | Loss: 0.0452 | Train Acc: 99.88% | Test Acc: 97.79%\n",
      "Epoch 617 | Loss: 0.0460 | Train Acc: 99.88% | Test Acc: 97.75%\n",
      "Epoch 618 | Loss: 0.0449 | Train Acc: 99.88% | Test Acc: 97.72%\n",
      "Epoch 619 | Loss: 0.0434 | Train Acc: 99.88% | Test Acc: 97.70%\n",
      "Epoch 620 | Loss: 0.0437 | Train Acc: 99.87% | Test Acc: 97.71%\n",
      "Epoch 621 | Loss: 0.0439 | Train Acc: 99.87% | Test Acc: 97.72%\n",
      "Epoch 622 | Loss: 0.0432 | Train Acc: 99.87% | Test Acc: 97.74%\n",
      "Epoch 623 | Loss: 0.0420 | Train Acc: 99.88% | Test Acc: 97.75%\n",
      "Epoch 624 | Loss: 0.0427 | Train Acc: 99.88% | Test Acc: 97.80%\n",
      "Epoch 625 | Loss: 0.0437 | Train Acc: 99.88% | Test Acc: 97.78%\n",
      "Epoch 626 | Loss: 0.0437 | Train Acc: 99.88% | Test Acc: 97.82%\n",
      "Epoch 627 | Loss: 0.0416 | Train Acc: 99.88% | Test Acc: 97.80%\n",
      "Epoch 628 | Loss: 0.0426 | Train Acc: 99.88% | Test Acc: 97.80%\n",
      "Epoch 629 | Loss: 0.0425 | Train Acc: 99.89% | Test Acc: 97.78%\n",
      "Epoch 630 | Loss: 0.0439 | Train Acc: 99.89% | Test Acc: 97.79%\n",
      "Epoch 631 | Loss: 0.0412 | Train Acc: 99.89% | Test Acc: 97.74%\n",
      "Epoch 632 | Loss: 0.0423 | Train Acc: 99.89% | Test Acc: 97.72%\n",
      "Epoch 633 | Loss: 0.0421 | Train Acc: 99.89% | Test Acc: 97.77%\n",
      "Epoch 634 | Loss: 0.0422 | Train Acc: 99.89% | Test Acc: 97.77%\n",
      "Epoch 635 | Loss: 0.0429 | Train Acc: 99.89% | Test Acc: 97.78%\n",
      "Epoch 636 | Loss: 0.0443 | Train Acc: 99.89% | Test Acc: 97.76%\n",
      "Epoch 637 | Loss: 0.0417 | Train Acc: 99.89% | Test Acc: 97.75%\n",
      "Epoch 638 | Loss: 0.0418 | Train Acc: 99.90% | Test Acc: 97.76%\n",
      "Epoch 639 | Loss: 0.0423 | Train Acc: 99.89% | Test Acc: 97.72%\n",
      "Epoch 640 | Loss: 0.0419 | Train Acc: 99.90% | Test Acc: 97.71%\n",
      "Epoch 641 | Loss: 0.0432 | Train Acc: 99.90% | Test Acc: 97.70%\n",
      "Epoch 642 | Loss: 0.0432 | Train Acc: 99.90% | Test Acc: 97.73%\n",
      "Epoch 643 | Loss: 0.0394 | Train Acc: 99.90% | Test Acc: 97.73%\n",
      "Epoch 644 | Loss: 0.0424 | Train Acc: 99.89% | Test Acc: 97.76%\n",
      "Epoch 645 | Loss: 0.0402 | Train Acc: 99.89% | Test Acc: 97.79%\n",
      "Epoch 646 | Loss: 0.0416 | Train Acc: 99.89% | Test Acc: 97.79%\n",
      "Epoch 647 | Loss: 0.0419 | Train Acc: 99.89% | Test Acc: 97.81%\n",
      "Epoch 648 | Loss: 0.0428 | Train Acc: 99.89% | Test Acc: 97.83%\n",
      "Epoch 649 | Loss: 0.0427 | Train Acc: 99.90% | Test Acc: 97.83%\n",
      "Epoch 650 | Loss: 0.0409 | Train Acc: 99.90% | Test Acc: 97.86%\n",
      "Epoch 651 | Loss: 0.0415 | Train Acc: 99.91% | Test Acc: 97.79%\n",
      "Epoch 652 | Loss: 0.0446 | Train Acc: 99.91% | Test Acc: 97.79%\n",
      "Epoch 653 | Loss: 0.0411 | Train Acc: 99.90% | Test Acc: 97.76%\n",
      "Epoch 654 | Loss: 0.0418 | Train Acc: 99.90% | Test Acc: 97.79%\n",
      "Epoch 655 | Loss: 0.0414 | Train Acc: 99.90% | Test Acc: 97.79%\n",
      "Epoch 656 | Loss: 0.0421 | Train Acc: 99.90% | Test Acc: 97.75%\n",
      "Epoch 657 | Loss: 0.0409 | Train Acc: 99.90% | Test Acc: 97.81%\n",
      "Epoch 658 | Loss: 0.0410 | Train Acc: 99.89% | Test Acc: 97.79%\n",
      "Epoch 659 | Loss: 0.0421 | Train Acc: 99.89% | Test Acc: 97.77%\n",
      "Epoch 660 | Loss: 0.0405 | Train Acc: 99.89% | Test Acc: 97.78%\n",
      "Epoch 661 | Loss: 0.0427 | Train Acc: 99.89% | Test Acc: 97.77%\n",
      "Epoch 662 | Loss: 0.0407 | Train Acc: 99.89% | Test Acc: 97.76%\n",
      "Epoch 663 | Loss: 0.0410 | Train Acc: 99.90% | Test Acc: 97.77%\n",
      "Epoch 664 | Loss: 0.0409 | Train Acc: 99.90% | Test Acc: 97.80%\n",
      "Epoch 665 | Loss: 0.0409 | Train Acc: 99.90% | Test Acc: 97.81%\n",
      "Epoch 666 | Loss: 0.0397 | Train Acc: 99.91% | Test Acc: 97.79%\n",
      "Epoch 667 | Loss: 0.0414 | Train Acc: 99.90% | Test Acc: 97.77%\n",
      "Epoch 668 | Loss: 0.0408 | Train Acc: 99.90% | Test Acc: 97.78%\n",
      "Epoch 669 | Loss: 0.0397 | Train Acc: 99.90% | Test Acc: 97.77%\n",
      "Epoch 670 | Loss: 0.0380 | Train Acc: 99.91% | Test Acc: 97.75%\n",
      "Epoch 671 | Loss: 0.0399 | Train Acc: 99.91% | Test Acc: 97.76%\n",
      "Epoch 672 | Loss: 0.0401 | Train Acc: 99.91% | Test Acc: 97.75%\n",
      "Epoch 673 | Loss: 0.0411 | Train Acc: 99.91% | Test Acc: 97.76%\n",
      "Epoch 674 | Loss: 0.0401 | Train Acc: 99.91% | Test Acc: 97.79%\n",
      "Epoch 675 | Loss: 0.0413 | Train Acc: 99.92% | Test Acc: 97.78%\n",
      "Epoch 676 | Loss: 0.0379 | Train Acc: 99.92% | Test Acc: 97.77%\n",
      "Epoch 677 | Loss: 0.0383 | Train Acc: 99.92% | Test Acc: 97.76%\n",
      "Epoch 678 | Loss: 0.0397 | Train Acc: 99.91% | Test Acc: 97.75%\n",
      "Epoch 679 | Loss: 0.0390 | Train Acc: 99.92% | Test Acc: 97.77%\n",
      "Epoch 680 | Loss: 0.0397 | Train Acc: 99.92% | Test Acc: 97.71%\n",
      "Epoch 681 | Loss: 0.0399 | Train Acc: 99.91% | Test Acc: 97.69%\n",
      "Epoch 682 | Loss: 0.0379 | Train Acc: 99.91% | Test Acc: 97.70%\n",
      "Epoch 683 | Loss: 0.0399 | Train Acc: 99.91% | Test Acc: 97.71%\n",
      "Epoch 684 | Loss: 0.0395 | Train Acc: 99.90% | Test Acc: 97.71%\n",
      "Epoch 685 | Loss: 0.0394 | Train Acc: 99.90% | Test Acc: 97.70%\n",
      "Epoch 686 | Loss: 0.0384 | Train Acc: 99.90% | Test Acc: 97.73%\n",
      "Epoch 687 | Loss: 0.0394 | Train Acc: 99.90% | Test Acc: 97.76%\n",
      "Epoch 688 | Loss: 0.0392 | Train Acc: 99.91% | Test Acc: 97.75%\n",
      "Epoch 689 | Loss: 0.0371 | Train Acc: 99.91% | Test Acc: 97.77%\n",
      "Epoch 690 | Loss: 0.0396 | Train Acc: 99.91% | Test Acc: 97.78%\n",
      "Epoch 691 | Loss: 0.0384 | Train Acc: 99.92% | Test Acc: 97.77%\n",
      "Epoch 692 | Loss: 0.0393 | Train Acc: 99.92% | Test Acc: 97.76%\n",
      "Epoch 693 | Loss: 0.0396 | Train Acc: 99.92% | Test Acc: 97.76%\n",
      "Epoch 694 | Loss: 0.0371 | Train Acc: 99.91% | Test Acc: 97.78%\n",
      "Epoch 695 | Loss: 0.0370 | Train Acc: 99.92% | Test Acc: 97.77%\n",
      "Epoch 696 | Loss: 0.0390 | Train Acc: 99.92% | Test Acc: 97.76%\n",
      "Epoch 697 | Loss: 0.0377 | Train Acc: 99.92% | Test Acc: 97.79%\n",
      "Epoch 698 | Loss: 0.0393 | Train Acc: 99.92% | Test Acc: 97.79%\n",
      "Epoch 699 | Loss: 0.0387 | Train Acc: 99.91% | Test Acc: 97.75%\n",
      "Epoch 700 | Loss: 0.0373 | Train Acc: 99.92% | Test Acc: 97.77%\n",
      "Epoch 701 | Loss: 0.0382 | Train Acc: 99.92% | Test Acc: 97.77%\n",
      "Epoch 702 | Loss: 0.0385 | Train Acc: 99.92% | Test Acc: 97.75%\n",
      "Epoch 703 | Loss: 0.0386 | Train Acc: 99.92% | Test Acc: 97.78%\n",
      "Epoch 704 | Loss: 0.0371 | Train Acc: 99.93% | Test Acc: 97.79%\n",
      "Epoch 705 | Loss: 0.0364 | Train Acc: 99.93% | Test Acc: 97.82%\n",
      "Epoch 706 | Loss: 0.0376 | Train Acc: 99.93% | Test Acc: 97.81%\n",
      "Epoch 707 | Loss: 0.0382 | Train Acc: 99.93% | Test Acc: 97.76%\n",
      "Epoch 708 | Loss: 0.0382 | Train Acc: 99.93% | Test Acc: 97.78%\n",
      "Epoch 709 | Loss: 0.0382 | Train Acc: 99.93% | Test Acc: 97.77%\n",
      "Epoch 710 | Loss: 0.0380 | Train Acc: 99.93% | Test Acc: 97.76%\n",
      "Epoch 711 | Loss: 0.0384 | Train Acc: 99.93% | Test Acc: 97.79%\n",
      "Epoch 712 | Loss: 0.0358 | Train Acc: 99.93% | Test Acc: 97.81%\n",
      "Epoch 713 | Loss: 0.0365 | Train Acc: 99.93% | Test Acc: 97.81%\n",
      "Epoch 714 | Loss: 0.0365 | Train Acc: 99.93% | Test Acc: 97.78%\n",
      "Epoch 715 | Loss: 0.0366 | Train Acc: 99.93% | Test Acc: 97.74%\n",
      "Epoch 716 | Loss: 0.0379 | Train Acc: 99.93% | Test Acc: 97.75%\n",
      "Epoch 717 | Loss: 0.0390 | Train Acc: 99.93% | Test Acc: 97.74%\n",
      "Epoch 718 | Loss: 0.0372 | Train Acc: 99.93% | Test Acc: 97.74%\n",
      "Epoch 719 | Loss: 0.0391 | Train Acc: 99.93% | Test Acc: 97.74%\n",
      "Epoch 720 | Loss: 0.0390 | Train Acc: 99.94% | Test Acc: 97.74%\n",
      "Epoch 721 | Loss: 0.0371 | Train Acc: 99.94% | Test Acc: 97.75%\n",
      "Epoch 722 | Loss: 0.0371 | Train Acc: 99.94% | Test Acc: 97.74%\n",
      "Epoch 723 | Loss: 0.0396 | Train Acc: 99.93% | Test Acc: 97.75%\n",
      "Epoch 724 | Loss: 0.0370 | Train Acc: 99.94% | Test Acc: 97.73%\n",
      "Epoch 725 | Loss: 0.0370 | Train Acc: 99.93% | Test Acc: 97.74%\n",
      "Epoch 726 | Loss: 0.0385 | Train Acc: 99.94% | Test Acc: 97.78%\n",
      "Epoch 727 | Loss: 0.0360 | Train Acc: 99.94% | Test Acc: 97.78%\n",
      "Epoch 728 | Loss: 0.0369 | Train Acc: 99.93% | Test Acc: 97.77%\n",
      "Epoch 729 | Loss: 0.0369 | Train Acc: 99.94% | Test Acc: 97.81%\n",
      "Epoch 730 | Loss: 0.0371 | Train Acc: 99.94% | Test Acc: 97.81%\n",
      "Epoch 731 | Loss: 0.0361 | Train Acc: 99.94% | Test Acc: 97.81%\n",
      "Epoch 732 | Loss: 0.0367 | Train Acc: 99.94% | Test Acc: 97.82%\n",
      "Epoch 733 | Loss: 0.0367 | Train Acc: 99.94% | Test Acc: 97.80%\n",
      "Epoch 734 | Loss: 0.0378 | Train Acc: 99.93% | Test Acc: 97.77%\n",
      "Epoch 735 | Loss: 0.0352 | Train Acc: 99.94% | Test Acc: 97.72%\n",
      "Epoch 736 | Loss: 0.0354 | Train Acc: 99.94% | Test Acc: 97.72%\n",
      "Epoch 737 | Loss: 0.0358 | Train Acc: 99.94% | Test Acc: 97.73%\n",
      "Epoch 738 | Loss: 0.0372 | Train Acc: 99.94% | Test Acc: 97.77%\n",
      "Epoch 739 | Loss: 0.0355 | Train Acc: 99.94% | Test Acc: 97.79%\n",
      "Epoch 740 | Loss: 0.0363 | Train Acc: 99.94% | Test Acc: 97.83%\n",
      "Epoch 741 | Loss: 0.0366 | Train Acc: 99.94% | Test Acc: 97.82%\n",
      "Epoch 742 | Loss: 0.0361 | Train Acc: 99.94% | Test Acc: 97.83%\n",
      "Epoch 743 | Loss: 0.0361 | Train Acc: 99.94% | Test Acc: 97.83%\n",
      "Epoch 744 | Loss: 0.0354 | Train Acc: 99.94% | Test Acc: 97.81%\n",
      "Epoch 745 | Loss: 0.0352 | Train Acc: 99.94% | Test Acc: 97.79%\n",
      "Epoch 746 | Loss: 0.0351 | Train Acc: 99.95% | Test Acc: 97.78%\n",
      "Epoch 747 | Loss: 0.0362 | Train Acc: 99.95% | Test Acc: 97.77%\n",
      "Epoch 748 | Loss: 0.0360 | Train Acc: 99.94% | Test Acc: 97.80%\n",
      "Epoch 749 | Loss: 0.0362 | Train Acc: 99.94% | Test Acc: 97.81%\n",
      "Epoch 750 | Loss: 0.0348 | Train Acc: 99.95% | Test Acc: 97.80%\n",
      "Epoch 751 | Loss: 0.0350 | Train Acc: 99.95% | Test Acc: 97.83%\n",
      "Epoch 752 | Loss: 0.0359 | Train Acc: 99.94% | Test Acc: 97.80%\n",
      "Epoch 753 | Loss: 0.0359 | Train Acc: 99.94% | Test Acc: 97.85%\n",
      "Epoch 754 | Loss: 0.0359 | Train Acc: 99.94% | Test Acc: 97.87%\n",
      "Epoch 755 | Loss: 0.0353 | Train Acc: 99.94% | Test Acc: 97.87%\n",
      "Epoch 756 | Loss: 0.0363 | Train Acc: 99.94% | Test Acc: 97.85%\n",
      "Epoch 757 | Loss: 0.0362 | Train Acc: 99.94% | Test Acc: 97.83%\n",
      "Epoch 758 | Loss: 0.0357 | Train Acc: 99.94% | Test Acc: 97.87%\n",
      "Epoch 759 | Loss: 0.0359 | Train Acc: 99.94% | Test Acc: 97.89%\n",
      "Epoch 760 | Loss: 0.0351 | Train Acc: 99.94% | Test Acc: 97.88%\n",
      "Epoch 761 | Loss: 0.0326 | Train Acc: 99.95% | Test Acc: 97.87%\n",
      "Epoch 762 | Loss: 0.0351 | Train Acc: 99.95% | Test Acc: 97.82%\n",
      "Epoch 763 | Loss: 0.0334 | Train Acc: 99.95% | Test Acc: 97.81%\n",
      "Epoch 764 | Loss: 0.0337 | Train Acc: 99.95% | Test Acc: 97.80%\n",
      "Epoch 765 | Loss: 0.0353 | Train Acc: 99.95% | Test Acc: 97.80%\n",
      "Epoch 766 | Loss: 0.0359 | Train Acc: 99.95% | Test Acc: 97.80%\n",
      "Epoch 767 | Loss: 0.0341 | Train Acc: 99.95% | Test Acc: 97.82%\n",
      "Epoch 768 | Loss: 0.0345 | Train Acc: 99.95% | Test Acc: 97.84%\n",
      "Epoch 769 | Loss: 0.0332 | Train Acc: 99.95% | Test Acc: 97.84%\n",
      "Epoch 770 | Loss: 0.0342 | Train Acc: 99.95% | Test Acc: 97.85%\n",
      "Epoch 771 | Loss: 0.0338 | Train Acc: 99.95% | Test Acc: 97.85%\n",
      "Epoch 772 | Loss: 0.0332 | Train Acc: 99.95% | Test Acc: 97.80%\n",
      "Epoch 773 | Loss: 0.0343 | Train Acc: 99.95% | Test Acc: 97.79%\n",
      "Epoch 774 | Loss: 0.0327 | Train Acc: 99.95% | Test Acc: 97.82%\n",
      "Epoch 775 | Loss: 0.0343 | Train Acc: 99.95% | Test Acc: 97.81%\n",
      "Epoch 776 | Loss: 0.0334 | Train Acc: 99.95% | Test Acc: 97.82%\n",
      "Epoch 777 | Loss: 0.0336 | Train Acc: 99.96% | Test Acc: 97.84%\n",
      "Epoch 778 | Loss: 0.0340 | Train Acc: 99.95% | Test Acc: 97.85%\n",
      "Epoch 779 | Loss: 0.0347 | Train Acc: 99.95% | Test Acc: 97.86%\n",
      "Epoch 780 | Loss: 0.0371 | Train Acc: 99.95% | Test Acc: 97.84%\n",
      "Epoch 781 | Loss: 0.0328 | Train Acc: 99.95% | Test Acc: 97.80%\n",
      "Epoch 782 | Loss: 0.0347 | Train Acc: 99.95% | Test Acc: 97.80%\n",
      "Epoch 783 | Loss: 0.0331 | Train Acc: 99.96% | Test Acc: 97.81%\n",
      "Epoch 784 | Loss: 0.0346 | Train Acc: 99.96% | Test Acc: 97.81%\n",
      "Epoch 785 | Loss: 0.0334 | Train Acc: 99.96% | Test Acc: 97.82%\n",
      "Epoch 786 | Loss: 0.0335 | Train Acc: 99.96% | Test Acc: 97.83%\n",
      "Epoch 787 | Loss: 0.0348 | Train Acc: 99.96% | Test Acc: 97.81%\n",
      "Epoch 788 | Loss: 0.0351 | Train Acc: 99.96% | Test Acc: 97.82%\n",
      "Epoch 789 | Loss: 0.0326 | Train Acc: 99.96% | Test Acc: 97.86%\n",
      "Epoch 790 | Loss: 0.0327 | Train Acc: 99.96% | Test Acc: 97.85%\n",
      "Epoch 791 | Loss: 0.0328 | Train Acc: 99.96% | Test Acc: 97.82%\n",
      "Epoch 792 | Loss: 0.0318 | Train Acc: 99.96% | Test Acc: 97.82%\n",
      "Epoch 793 | Loss: 0.0343 | Train Acc: 99.96% | Test Acc: 97.85%\n",
      "Epoch 794 | Loss: 0.0340 | Train Acc: 99.97% | Test Acc: 97.79%\n",
      "Epoch 795 | Loss: 0.0318 | Train Acc: 99.96% | Test Acc: 97.78%\n",
      "Epoch 796 | Loss: 0.0332 | Train Acc: 99.96% | Test Acc: 97.79%\n",
      "Epoch 797 | Loss: 0.0328 | Train Acc: 99.96% | Test Acc: 97.79%\n",
      "Epoch 798 | Loss: 0.0329 | Train Acc: 99.96% | Test Acc: 97.83%\n",
      "Epoch 799 | Loss: 0.0349 | Train Acc: 99.96% | Test Acc: 97.77%\n",
      "Epoch 800 | Loss: 0.0330 | Train Acc: 99.96% | Test Acc: 97.76%\n",
      "Epoch 801 | Loss: 0.0325 | Train Acc: 99.97% | Test Acc: 97.78%\n",
      "Epoch 802 | Loss: 0.0340 | Train Acc: 99.97% | Test Acc: 97.76%\n",
      "Epoch 803 | Loss: 0.0327 | Train Acc: 99.97% | Test Acc: 97.75%\n",
      "Epoch 804 | Loss: 0.0321 | Train Acc: 99.97% | Test Acc: 97.71%\n",
      "Epoch 805 | Loss: 0.0339 | Train Acc: 99.97% | Test Acc: 97.71%\n",
      "Epoch 806 | Loss: 0.0328 | Train Acc: 99.97% | Test Acc: 97.70%\n",
      "Epoch 807 | Loss: 0.0327 | Train Acc: 99.97% | Test Acc: 97.73%\n",
      "Epoch 808 | Loss: 0.0326 | Train Acc: 99.97% | Test Acc: 97.76%\n",
      "Epoch 809 | Loss: 0.0328 | Train Acc: 99.97% | Test Acc: 97.79%\n",
      "Epoch 810 | Loss: 0.0331 | Train Acc: 99.97% | Test Acc: 97.79%\n",
      "Epoch 811 | Loss: 0.0297 | Train Acc: 99.97% | Test Acc: 97.80%\n",
      "Epoch 812 | Loss: 0.0316 | Train Acc: 99.96% | Test Acc: 97.78%\n",
      "Epoch 813 | Loss: 0.0327 | Train Acc: 99.96% | Test Acc: 97.79%\n",
      "Epoch 814 | Loss: 0.0320 | Train Acc: 99.96% | Test Acc: 97.78%\n",
      "Epoch 815 | Loss: 0.0326 | Train Acc: 99.96% | Test Acc: 97.77%\n",
      "Epoch 816 | Loss: 0.0310 | Train Acc: 99.96% | Test Acc: 97.75%\n",
      "Epoch 817 | Loss: 0.0314 | Train Acc: 99.96% | Test Acc: 97.76%\n",
      "Epoch 818 | Loss: 0.0326 | Train Acc: 99.96% | Test Acc: 97.75%\n",
      "Epoch 819 | Loss: 0.0321 | Train Acc: 99.96% | Test Acc: 97.76%\n",
      "Epoch 820 | Loss: 0.0314 | Train Acc: 99.96% | Test Acc: 97.77%\n",
      "Epoch 821 | Loss: 0.0317 | Train Acc: 99.97% | Test Acc: 97.74%\n",
      "Epoch 822 | Loss: 0.0319 | Train Acc: 99.97% | Test Acc: 97.75%\n",
      "Epoch 823 | Loss: 0.0317 | Train Acc: 99.97% | Test Acc: 97.78%\n",
      "Epoch 824 | Loss: 0.0315 | Train Acc: 99.97% | Test Acc: 97.82%\n",
      "Epoch 825 | Loss: 0.0323 | Train Acc: 99.97% | Test Acc: 97.80%\n",
      "Epoch 826 | Loss: 0.0322 | Train Acc: 99.97% | Test Acc: 97.83%\n",
      "Epoch 827 | Loss: 0.0308 | Train Acc: 99.97% | Test Acc: 97.75%\n",
      "Epoch 828 | Loss: 0.0326 | Train Acc: 99.97% | Test Acc: 97.78%\n",
      "Epoch 829 | Loss: 0.0325 | Train Acc: 99.97% | Test Acc: 97.75%\n",
      "Epoch 830 | Loss: 0.0335 | Train Acc: 99.97% | Test Acc: 97.79%\n",
      "Epoch 831 | Loss: 0.0301 | Train Acc: 99.98% | Test Acc: 97.82%\n",
      "Epoch 832 | Loss: 0.0320 | Train Acc: 99.97% | Test Acc: 97.83%\n",
      "Epoch 833 | Loss: 0.0307 | Train Acc: 99.98% | Test Acc: 97.84%\n",
      "Epoch 834 | Loss: 0.0319 | Train Acc: 99.97% | Test Acc: 97.83%\n",
      "Epoch 835 | Loss: 0.0323 | Train Acc: 99.97% | Test Acc: 97.82%\n",
      "Epoch 836 | Loss: 0.0305 | Train Acc: 99.97% | Test Acc: 97.85%\n",
      "Epoch 837 | Loss: 0.0309 | Train Acc: 99.98% | Test Acc: 97.82%\n",
      "Epoch 838 | Loss: 0.0322 | Train Acc: 99.98% | Test Acc: 97.83%\n",
      "Epoch 839 | Loss: 0.0319 | Train Acc: 99.98% | Test Acc: 97.82%\n",
      "Epoch 840 | Loss: 0.0320 | Train Acc: 99.98% | Test Acc: 97.81%\n",
      "Epoch 841 | Loss: 0.0318 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 842 | Loss: 0.0298 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 843 | Loss: 0.0310 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 844 | Loss: 0.0305 | Train Acc: 99.98% | Test Acc: 97.83%\n",
      "Epoch 845 | Loss: 0.0317 | Train Acc: 99.98% | Test Acc: 97.84%\n",
      "Epoch 846 | Loss: 0.0306 | Train Acc: 99.98% | Test Acc: 97.85%\n",
      "Epoch 847 | Loss: 0.0300 | Train Acc: 99.97% | Test Acc: 97.85%\n",
      "Epoch 848 | Loss: 0.0321 | Train Acc: 99.98% | Test Acc: 97.83%\n",
      "Epoch 849 | Loss: 0.0305 | Train Acc: 99.98% | Test Acc: 97.81%\n",
      "Epoch 850 | Loss: 0.0308 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 851 | Loss: 0.0302 | Train Acc: 99.98% | Test Acc: 97.77%\n",
      "Epoch 852 | Loss: 0.0317 | Train Acc: 99.98% | Test Acc: 97.79%\n",
      "Epoch 853 | Loss: 0.0315 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 854 | Loss: 0.0305 | Train Acc: 99.98% | Test Acc: 97.76%\n",
      "Epoch 855 | Loss: 0.0308 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 856 | Loss: 0.0297 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 857 | Loss: 0.0306 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 858 | Loss: 0.0279 | Train Acc: 99.98% | Test Acc: 97.79%\n",
      "Epoch 859 | Loss: 0.0317 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 860 | Loss: 0.0300 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 861 | Loss: 0.0300 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 862 | Loss: 0.0293 | Train Acc: 99.98% | Test Acc: 97.76%\n",
      "Epoch 863 | Loss: 0.0295 | Train Acc: 99.98% | Test Acc: 97.79%\n",
      "Epoch 864 | Loss: 0.0312 | Train Acc: 99.98% | Test Acc: 97.79%\n",
      "Epoch 865 | Loss: 0.0307 | Train Acc: 99.98% | Test Acc: 97.79%\n",
      "Epoch 866 | Loss: 0.0298 | Train Acc: 99.98% | Test Acc: 97.79%\n",
      "Epoch 867 | Loss: 0.0306 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 868 | Loss: 0.0303 | Train Acc: 99.98% | Test Acc: 97.76%\n",
      "Epoch 869 | Loss: 0.0310 | Train Acc: 99.98% | Test Acc: 97.77%\n",
      "Epoch 870 | Loss: 0.0305 | Train Acc: 99.98% | Test Acc: 97.77%\n",
      "Epoch 871 | Loss: 0.0297 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 872 | Loss: 0.0295 | Train Acc: 99.99% | Test Acc: 97.79%\n",
      "Epoch 873 | Loss: 0.0302 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 874 | Loss: 0.0307 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 875 | Loss: 0.0288 | Train Acc: 99.99% | Test Acc: 97.79%\n",
      "Epoch 876 | Loss: 0.0309 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 877 | Loss: 0.0293 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 878 | Loss: 0.0278 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 879 | Loss: 0.0298 | Train Acc: 99.98% | Test Acc: 97.83%\n",
      "Epoch 880 | Loss: 0.0287 | Train Acc: 99.98% | Test Acc: 97.86%\n",
      "Epoch 881 | Loss: 0.0293 | Train Acc: 99.98% | Test Acc: 97.87%\n",
      "Epoch 882 | Loss: 0.0284 | Train Acc: 99.98% | Test Acc: 97.85%\n",
      "Epoch 883 | Loss: 0.0284 | Train Acc: 99.98% | Test Acc: 97.90%\n",
      "Epoch 884 | Loss: 0.0307 | Train Acc: 99.98% | Test Acc: 97.90%\n",
      "Epoch 885 | Loss: 0.0303 | Train Acc: 99.98% | Test Acc: 97.89%\n",
      "Epoch 886 | Loss: 0.0294 | Train Acc: 99.98% | Test Acc: 97.84%\n",
      "Epoch 887 | Loss: 0.0287 | Train Acc: 99.98% | Test Acc: 97.84%\n",
      "Epoch 888 | Loss: 0.0301 | Train Acc: 99.98% | Test Acc: 97.83%\n",
      "Epoch 889 | Loss: 0.0269 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 890 | Loss: 0.0300 | Train Acc: 99.98% | Test Acc: 97.77%\n",
      "Epoch 891 | Loss: 0.0274 | Train Acc: 99.98% | Test Acc: 97.76%\n",
      "Epoch 892 | Loss: 0.0296 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 893 | Loss: 0.0293 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 894 | Loss: 0.0287 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 895 | Loss: 0.0281 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 896 | Loss: 0.0288 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 897 | Loss: 0.0285 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 898 | Loss: 0.0288 | Train Acc: 99.98% | Test Acc: 97.75%\n",
      "Epoch 899 | Loss: 0.0278 | Train Acc: 99.98% | Test Acc: 97.75%\n",
      "Epoch 900 | Loss: 0.0280 | Train Acc: 99.99% | Test Acc: 97.78%\n",
      "Epoch 901 | Loss: 0.0305 | Train Acc: 99.99% | Test Acc: 97.79%\n",
      "Epoch 902 | Loss: 0.0276 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 903 | Loss: 0.0290 | Train Acc: 99.99% | Test Acc: 97.79%\n",
      "Epoch 904 | Loss: 0.0292 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 905 | Loss: 0.0282 | Train Acc: 99.99% | Test Acc: 97.84%\n",
      "Epoch 906 | Loss: 0.0281 | Train Acc: 99.99% | Test Acc: 97.86%\n",
      "Epoch 907 | Loss: 0.0285 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 908 | Loss: 0.0293 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 909 | Loss: 0.0287 | Train Acc: 99.99% | Test Acc: 97.79%\n",
      "Epoch 910 | Loss: 0.0279 | Train Acc: 99.98% | Test Acc: 97.82%\n",
      "Epoch 911 | Loss: 0.0282 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 912 | Loss: 0.0270 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 913 | Loss: 0.0282 | Train Acc: 99.98% | Test Acc: 97.82%\n",
      "Epoch 914 | Loss: 0.0289 | Train Acc: 99.98% | Test Acc: 97.83%\n",
      "Epoch 915 | Loss: 0.0265 | Train Acc: 99.98% | Test Acc: 97.85%\n",
      "Epoch 916 | Loss: 0.0270 | Train Acc: 99.98% | Test Acc: 97.90%\n",
      "Epoch 917 | Loss: 0.0270 | Train Acc: 99.99% | Test Acc: 97.90%\n",
      "Epoch 918 | Loss: 0.0291 | Train Acc: 99.99% | Test Acc: 97.87%\n",
      "Epoch 919 | Loss: 0.0280 | Train Acc: 99.99% | Test Acc: 97.86%\n",
      "Epoch 920 | Loss: 0.0274 | Train Acc: 99.99% | Test Acc: 97.86%\n",
      "Epoch 921 | Loss: 0.0275 | Train Acc: 99.99% | Test Acc: 97.84%\n",
      "Epoch 922 | Loss: 0.0284 | Train Acc: 99.99% | Test Acc: 97.84%\n",
      "Epoch 923 | Loss: 0.0278 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 924 | Loss: 0.0266 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 925 | Loss: 0.0279 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 926 | Loss: 0.0283 | Train Acc: 99.98% | Test Acc: 97.78%\n",
      "Epoch 927 | Loss: 0.0259 | Train Acc: 99.98% | Test Acc: 97.77%\n",
      "Epoch 928 | Loss: 0.0278 | Train Acc: 99.98% | Test Acc: 97.80%\n",
      "Epoch 929 | Loss: 0.0264 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 930 | Loss: 0.0261 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 931 | Loss: 0.0284 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 932 | Loss: 0.0284 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 933 | Loss: 0.0281 | Train Acc: 99.98% | Test Acc: 97.83%\n",
      "Epoch 934 | Loss: 0.0285 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 935 | Loss: 0.0266 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 936 | Loss: 0.0269 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 937 | Loss: 0.0259 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 938 | Loss: 0.0265 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 939 | Loss: 0.0262 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 940 | Loss: 0.0277 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 941 | Loss: 0.0265 | Train Acc: 99.98% | Test Acc: 97.81%\n",
      "Epoch 942 | Loss: 0.0278 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 943 | Loss: 0.0274 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 944 | Loss: 0.0268 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 945 | Loss: 0.0277 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 946 | Loss: 0.0249 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 947 | Loss: 0.0283 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 948 | Loss: 0.0268 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 949 | Loss: 0.0266 | Train Acc: 99.99% | Test Acc: 97.84%\n",
      "Epoch 950 | Loss: 0.0274 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 951 | Loss: 0.0270 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 952 | Loss: 0.0278 | Train Acc: 99.99% | Test Acc: 97.90%\n",
      "Epoch 953 | Loss: 0.0271 | Train Acc: 99.99% | Test Acc: 97.87%\n",
      "Epoch 954 | Loss: 0.0262 | Train Acc: 99.99% | Test Acc: 97.87%\n",
      "Epoch 955 | Loss: 0.0269 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 956 | Loss: 0.0260 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 957 | Loss: 0.0272 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 958 | Loss: 0.0268 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 959 | Loss: 0.0265 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 960 | Loss: 0.0271 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 961 | Loss: 0.0271 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 962 | Loss: 0.0263 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 963 | Loss: 0.0282 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 964 | Loss: 0.0270 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 965 | Loss: 0.0264 | Train Acc: 99.99% | Test Acc: 97.86%\n",
      "Epoch 966 | Loss: 0.0274 | Train Acc: 99.99% | Test Acc: 97.87%\n",
      "Epoch 967 | Loss: 0.0262 | Train Acc: 99.99% | Test Acc: 97.89%\n",
      "Epoch 968 | Loss: 0.0257 | Train Acc: 99.99% | Test Acc: 97.90%\n",
      "Epoch 969 | Loss: 0.0253 | Train Acc: 99.99% | Test Acc: 97.88%\n",
      "Epoch 970 | Loss: 0.0261 | Train Acc: 99.99% | Test Acc: 97.87%\n",
      "Epoch 971 | Loss: 0.0265 | Train Acc: 99.99% | Test Acc: 97.86%\n",
      "Epoch 972 | Loss: 0.0255 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 973 | Loss: 0.0253 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 974 | Loss: 0.0265 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 975 | Loss: 0.0262 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 976 | Loss: 0.0268 | Train Acc: 99.99% | Test Acc: 97.84%\n",
      "Epoch 977 | Loss: 0.0258 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 978 | Loss: 0.0257 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 979 | Loss: 0.0267 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 980 | Loss: 0.0257 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 981 | Loss: 0.0249 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 982 | Loss: 0.0260 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 983 | Loss: 0.0266 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 984 | Loss: 0.0282 | Train Acc: 99.99% | Test Acc: 97.85%\n",
      "Epoch 985 | Loss: 0.0272 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 986 | Loss: 0.0245 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 987 | Loss: 0.0252 | Train Acc: 99.99% | Test Acc: 97.80%\n",
      "Epoch 988 | Loss: 0.0255 | Train Acc: 99.99% | Test Acc: 97.81%\n",
      "Epoch 989 | Loss: 0.0264 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 990 | Loss: 0.0267 | Train Acc: 99.99% | Test Acc: 97.84%\n",
      "Epoch 991 | Loss: 0.0267 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 992 | Loss: 0.0255 | Train Acc: 99.99% | Test Acc: 97.84%\n",
      "Epoch 993 | Loss: 0.0249 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 994 | Loss: 0.0257 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 995 | Loss: 0.0253 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 996 | Loss: 0.0264 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 997 | Loss: 0.0258 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 998 | Loss: 0.0264 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "Epoch 999 | Loss: 0.0252 | Train Acc: 99.99% | Test Acc: 97.82%\n",
      "Epoch 1000 | Loss: 0.0255 | Train Acc: 99.99% | Test Acc: 97.83%\n",
      "\n",
      "Training Complete. Final Accuracy: 100.00%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T13:56:22.979902Z",
     "start_time": "2025-12-16T13:56:22.970862Z"
    }
   },
   "cell_type": "code",
   "source": "z @ z.T",
   "id": "5f557362accb8ccd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.03724074,  1.01862037,  0.        , -0.02037037, -0.04074074],\n",
       "       [ 1.01862037,  0.50931019,  0.        , -0.01018519, -0.02037037],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.02037037, -0.01018519,  0.        ,  0.50931019,  1.01862037],\n",
       "       [-0.04074074, -0.02037037,  0.        ,  1.01862037,  2.03724074]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T15:04:38.907690Z",
     "start_time": "2025-12-16T15:04:38.899370Z"
    }
   },
   "cell_type": "code",
   "source": "(z == z.max(axis=0, keepdims=True)).sum()/z.shape[1]",
   "id": "587c1333bd60bbc4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T15:03:39.907628Z",
     "start_time": "2025-12-16T15:03:39.899252Z"
    }
   },
   "cell_type": "code",
   "source": "z",
   "id": "7c18b29a979eeb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  7.77777778e-01,  5.55555556e-01,\n",
       "         3.33333333e-01,  1.11111111e-01, -1.11111111e-03,\n",
       "        -3.33333333e-03, -5.55555556e-03, -7.77777778e-03,\n",
       "        -1.00000000e-02],\n",
       "       [ 5.00000000e-01,  3.88888889e-01,  2.77777778e-01,\n",
       "         1.66666667e-01,  5.55555556e-02, -5.55555556e-04,\n",
       "        -1.66666667e-03, -2.77777778e-03, -3.88888889e-03,\n",
       "        -5.00000000e-03],\n",
       "       [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-5.00000000e-03, -3.88888889e-03, -2.77777778e-03,\n",
       "        -1.66666667e-03, -5.55555556e-04,  5.55555556e-02,\n",
       "         1.66666667e-01,  2.77777778e-01,  3.88888889e-01,\n",
       "         5.00000000e-01],\n",
       "       [-1.00000000e-02, -7.77777778e-03, -5.55555556e-03,\n",
       "        -3.33333333e-03, -1.11111111e-03,  1.11111111e-01,\n",
       "         3.33333333e-01,  5.55555556e-01,  7.77777778e-01,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T13:27:39.952573Z",
     "start_time": "2025-12-16T13:27:39.941062Z"
    }
   },
   "cell_type": "code",
   "source": "numpy.exp(z)",
   "id": "8d95bc333ec8e60f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.71828183, 2.17662993, 1.742909  , 1.39561243, 1.11751907,\n",
       "        0.99888951, 0.99667222, 0.99445985, 0.99225239, 0.99004983],\n",
       "       [1.64872127, 1.47534062, 1.32019279, 1.18136041, 1.05712774,\n",
       "        0.9994446 , 0.99833472, 0.99722608, 0.99611866, 0.99501248],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.99501248, 0.99611866, 0.99722608, 0.99833472, 0.9994446 ,\n",
       "        1.05712774, 1.18136041, 1.32019279, 1.47534062, 1.64872127],\n",
       "       [0.99004983, 0.99225239, 0.99445985, 0.99667222, 0.99888951,\n",
       "        1.11751907, 1.39561243, 1.742909  , 2.17662993, 2.71828183]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "276ec9050291ceec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
